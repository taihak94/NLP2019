{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the solution to HW3, written by Yaniv Bin and Tair Hakman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first would like to import all the required modules in order for our code to run properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk import nonterminals, Nonterminal, Production, induce_pcfg\n",
    "from nltk.parse import generate\n",
    "from nltk.grammar import Nonterminal, Production, toy_pcfg2\n",
    "from nltk.probability import DictionaryProbDist\n",
    "from nltk import Tree, CFG, PCFG, Nonterminal\n",
    "from nltk.treetransforms import chomsky_normal_form\n",
    "from numpy import log\n",
    "from nltk.corpus import LazyCorpusLoader, BracketParseCorpusReader\n",
    "from collections import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after doing so we can go ahead and solve the tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the assignment we will discuss designing CFG for NLP task.\n",
    "<br> We were given the following code which read CFGs from string representation, and parse sentences given a CFG using a  a bottom-up parsing algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = \"\"\"\n",
    "S -> NP VP\n",
    "VP -> IV | TV NP\n",
    "NP -> 'John' | \"bread\"\n",
    "IV -> 'left'\n",
    "TV -> 'eats'\n",
    "\"\"\"\n",
    "g = CFG.fromstring(sg)\n",
    "\n",
    "# Bottom-up  parser\n",
    "sr_parser = nltk.ShiftReduceParser(g, trace=2)\n",
    "\n",
    "# Parse sentences and observe the behavior of the parser\n",
    "def parse_sentence(sent):\n",
    "    tokens = sent.split()\n",
    "    trees = sr_parser.parse(tokens)\n",
    "    for tree in trees:\n",
    "        print(tree)\n",
    "\n",
    "parse_sentence(\"John left\")\n",
    "parse_sentence(\"John eats bread\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, an important note about our parser, the Shift-Reduce parser. This parser does not support ambiguity at all. If multiple reductions are available for a single word, it will simply choose the first reduction listed (and if it fails, it will not go back to try the second reduction). This means we can't have the same NT on the rhs of two different rules.\n",
    "\n",
    "We're required to support a new list of sentences. In order to explain the process of creating the CFG, we would like to split the sentence list into three different lists (slightly altering the original sentence order). The first list:\n",
    "\n",
    "John left\n",
    "John eats bread\n",
    "John loves Mary\n",
    "She loves John\n",
    "She loves them \n",
    "Everybody loves John\n",
    "A boy loves Mary\n",
    "\n",
    "And here's the grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = \"\"\"\n",
    "S -> DET NP VP | NP VP | PR_NOM VP\n",
    "NP -> 'John' | 'bread' | 'Mary' | 'boy'\n",
    "IV -> 'left' \n",
    "VP -> IV | TV NP | TV PR_ACC\n",
    "TV -> 'eats' | 'loves'\n",
    "PR_NOM -> 'She' | 'Everybody'\n",
    "PR_ACC -> 'them'\n",
    "DET -> 'A'\n",
    "\"\"\"\n",
    "\n",
    "g = CFG.fromstring(sg)\n",
    "\n",
    "# Bottom-up  parser\n",
    "sr_parser = nltk.ShiftReduceParser(g, trace=2)\n",
    "\n",
    "sentences = \"\"\"John left\n",
    "John eats bread\n",
    "John loves Mary\n",
    "She loves John\n",
    "She loves them \n",
    "Everybody loves John\n",
    "A boy loves Mary\n",
    "\"\"\"\n",
    "\n",
    "sentences = sentences.split(\"\\n\")\n",
    "\n",
    "for i in range(2,7):\n",
    "    parse_sentence(sentences[i])\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This bit was relatively easy to deal with. We've introduced pronouns, who are split (so far) into two categories by their case: Nominative (He, she etc) and accusative (her, them) etc. Nominative pronouns appear in the beginning of sentences, accusative ones in the end. So we've added pronouns as an alternative to NPs in the appropriate places. We've also introduced the category of determiners (only \"A\" so far), which can start a sentence before a noun (\"A book\") - note that a determiner can't precede a pronoun (\"A she\" is not legal).\n",
    "\n",
    "Our next sentences are a bit different:\n",
    "\n",
    "They love Mary \n",
    "They love her\n",
    "\n",
    "The difference is the use of 'love' instead of 'loves'. This is a different kind of verb that follows different speakers - \"They love\" and \"He loves\" are legal, but \"They loves\" and \"He love\" aren't. So we'll split our grammar into two categories - sentences of type 1 (without s), and type 2.\n",
    "\n",
    "In English, the category 1 is I/we/you/they (in this case, they will all be followed by 'love'), and category 2 is he/she/it (all followed by 'loves'). This separation is somewhat similar to singular/plural, however it is not the same - note that \"I\" (singluar) is in the same category as \"we\" and \"they\" (plural), while \"you\" can refer to both singluar and plural. So for a lack of better name, we will refer to those categories as categories 1 and 2.\n",
    "\n",
    "So here's our new grammar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 = I, we, you, they\n",
    "#2 = He, she, it\n",
    "\n",
    "sg = \"\"\"\n",
    "S -> S1 | S2\n",
    "S1 -> PR_NOM1 VP1\n",
    "VP1 -> TV1 NP | TV1 PR_ACC\n",
    "TV1 -> 'love'\n",
    "PR_NOM1 -> 'They'\n",
    "\n",
    "S2 -> DET NP VP2 | NP VP2 | PR_NOM2 VP2\n",
    "VP2 -> IV | TV2 NP | TV2 PR_ACC\n",
    "TV2 -> 'eats' | 'loves'\n",
    "PR_NOM2 -> 'She' | 'Everybody'\n",
    "\n",
    "NP -> 'John' | 'bread' | 'Mary' | 'boy'\n",
    "IV -> 'left' \n",
    "PR_ACC -> 'them' | 'her'\n",
    "DET -> 'A'\n",
    "\"\"\"\n",
    "\n",
    "g = CFG.fromstring(sg)\n",
    "\n",
    "# Bottom-up  parser\n",
    "sr_parser = nltk.ShiftReduceParser(g, trace=2)\n",
    "\n",
    "sentences = \"\"\"John left\n",
    "John eats bread\n",
    "John loves Mary\n",
    "She loves John\n",
    "She loves them \n",
    "Everybody loves John\n",
    "A boy loves Mary\n",
    "They love Mary \n",
    "They love her\n",
    "\"\"\"\n",
    "\n",
    "sentences = sentences.split(\"\\n\")\n",
    "\n",
    "for i in range(7,9):\n",
    "    parse_sentence(sentences[i])\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we split most rules into types 1 and 2. Note that accusative pronounes aren't changed - the word \"her\" will be the same in both \"I love her\" and \"she loves her\".\n",
    "Our last group of sentences is:\n",
    "\n",
    "John gave Mary a heavy book\n",
    "John gave it to Mary\n",
    "\n",
    "This is when we encounter the limitations of the SRP. Looking at the first sentence, \"John gave Mary\" is already a legal sentence in our grammar (assuming the verb \"gave\" is added). So once we parse \"John gave Mary\", before the parser continues to the next word, those words will be reduced to \"S\". The best solution would be to use a better, less limited parser that supports ambiguity. But sticking with the SR parser, our solution will be a bit untidy - we will simply add a rule that adds the rest of the words after S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sg = \"\"\"\n",
    "S -> S1 | S2 | S DET AD_NP | S PREP_NP\n",
    "S1 -> PR_NOM1 VP1\n",
    "VP1 -> TV1 NP | TV1 PR_ACC\n",
    "TV1 -> 'love'\n",
    "PR_NOM1 -> 'They'\n",
    "\n",
    "S2 -> DET NP VP2 | NP VP2 | PR_NOM2 VP2 | \n",
    "VP2 -> IV2 | TV2 NP | TV2 PR_ACC\n",
    "TV2 -> 'eats' | 'loves' | 'gave'\n",
    "PR_NOM2 -> 'She' | 'Everybody'\n",
    "IV2 -> 'left'\n",
    "\n",
    "NP -> 'John' | 'bread' | 'Mary' | 'boy' | 'book'\n",
    "AD_NP -> ADJ NP\n",
    "PREP_NP -> PREP NP\n",
    "PR_ACC -> 'them' | 'her' | 'it'\n",
    "PREP -> \"to\"\n",
    "DET -> 'A' | 'a'\n",
    "ADJ -> 'heavy'\n",
    "\"\"\"\n",
    "\n",
    "g = CFG.fromstring(sg)\n",
    "\n",
    "# Bottom-up  parser\n",
    "sr_parser = nltk.ShiftReduceParser(g, trace=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the use of infinite recursion causes NLTK to throw some warnings - however they are false, as all sentences are parsed (and clearly all roles are used).\n",
    "\n",
    "Let's verify all 11 sentences are parsed correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"\"\"John left\n",
    "John eats bread\n",
    "John loves Mary\n",
    "She loves John\n",
    "She loves them \n",
    "Everybody loves John\n",
    "A boy loves Mary\n",
    "They love Mary \n",
    "They love her\n",
    "John gave Mary a heavy book\n",
    "John gave it to Mary\"\"\"\n",
    "\n",
    "'''\n",
    "Number: singular / plural (e.g., he/they)\n",
    "Gender: masculine / feminine / neutral (e.g., he/she/it)\n",
    "Case: nominative / accusative (e.g., he/him)\n",
    "'''\n",
    "\n",
    "sentences = sentences.split(\"\\n\")\n",
    "\n",
    "for i in range(11):\n",
    "    parse_sentence(sentences[i])\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for gender: we didn't need to encode it, because gender doesn't make a grammatical difference in English. Most pronouns (I/we/you/they) are netural to gender. And he/she, the only pronouns who are specific to a gender, behave the same gramatically (\"he loves her\", \"she loves her\"). The same also goes for nouns who have a clear gender (\"John loves her\", \"Mary loves her\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our grammar isn't perfect, and it does overgenerate in certain cases. One weakness is the lack of separation between different nouns, even though certain nouns can't logically do certain actions. This allows us to parse illogical sentences, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_sentence(\"bread gave book a heavy Mary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another weakness is the workaround we've added for the last sentences, which basically allows us to add certain combinations (such as DET ADJ NP) at the end of every sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_sentence(\"John loves Mary a heavy book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And since the role is recursive, we can add it as many times as we like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_sentence(\"John loves Mary a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we're expected to add support to these sentences:\n",
    "\n",
    "John saw a man with a telescope\n",
    "John saw a man on the hill with a telescope\n",
    "\n",
    "Mary knows men and women\n",
    "Mary knows men, children and women\n",
    "\n",
    "John and Mary eat bread\n",
    "John and Mary eat bread with cheese\n",
    "\n",
    "We can keep using the same trick from the end of last question - a recursive role to allow us to add certain suffixes at the end of a legal sentence (potentially endlessley). For example: if \"John saw a man\" is a legal sentence in our language, we can add two possible suffixes after it: \"on the hill\" and \"with a telescope\". This will make all combinations legal - many of them makes sense (\"John saw a man with a telescope\" \"...man on the hill\" \"...man on the hill with a telescope\" \"...man with a telescope on the hill\"), but also, as we've seen in the last question, we're exposed to infinite loops.\n",
    "\n",
    "Here is the new grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = \"\"\"\n",
    "S -> S1 | S2 | S DET AD_NP | S PREP_NP | S PREP DET_NP | S CONJ_NP\n",
    "S1 -> PR_NOM1 VP1\n",
    "VP1 -> TV1 NP | TV1 PR_ACC\n",
    "TV1 -> 'love' | 'eat'\n",
    "PR_NOM1 -> 'They' | NP CONJ NP\n",
    "\n",
    "S2 -> DET NP VP2 | NP VP2 | PR_NOM2 VP2 | \n",
    "VP2 -> IV2 | TV2 NP | TV2 PR_ACC | TV2 DET_NP\n",
    "TV2 -> 'eats' | 'loves' | 'gave' | 'saw' | 'knows'\n",
    "PR_NOM2 -> 'She' | 'Everybody'\n",
    "IV2 -> 'left'\n",
    "\n",
    "NP -> 'John' | 'bread' | 'Mary' | 'boy' | 'book' | 'man' | 'telescope' | 'hill' | 'men' | 'women' | 'children' | 'cheese'\n",
    "AD_NP -> ADJ NP\n",
    "PREP_NP -> PREP NP\n",
    "DET_NP -> DET NP\n",
    "CONJ_NP -> CONJ NP\n",
    "PR_ACC -> 'them' | 'her' | 'it'\n",
    "PREP -> \"to\" | 'with' | 'on'\n",
    "DET -> 'A' | 'a' | 'the'\n",
    "CONJ -> 'and' | ','\n",
    "ADJ -> 'heavy'\n",
    "\"\"\"\n",
    "\n",
    "g = CFG.fromstring(sg)\n",
    "\n",
    "# Bottom-up  parser\n",
    "sr_parser = nltk.ShiftReduceParser(g, trace=2)\n",
    "\n",
    "# Parse sentences and observe the behavior of the parser\n",
    "def parse_sentence(sent):\n",
    "    tokens = re.findall(r\"[\\w']+|[.,!?;]\", sent)\n",
    "    trees = sr_parser.parse(tokens)\n",
    "    for tree in trees:\n",
    "        print(tree)\n",
    "\n",
    "sentences = \"\"\"John saw a man with a telescope\n",
    "John saw a man on the hill with a telescope\n",
    "Mary knows men and women\n",
    "Mary knows men, children and women\n",
    "John and Mary eat bread\n",
    "John and Mary eat bread with cheese\"\"\"\n",
    "\n",
    "\n",
    "sentences = sentences.split(\"\\n\")\n",
    "\n",
    "for i in range(6):\n",
    "    parse_sentence(sentences[i])\n",
    "    print (\"---------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few specific notes about implementation:\n",
    "\n",
    "-We've added the category of conjuction, such as 'and'.\n",
    "-The 4th sentence includes a comma, which also has a grammatical role. However, the split function previously used does not split punctuation (it created the word 'men,' rather than 'men' and ','), so we've changed the split method to a method that will split the coma (using regular expressions). The comma is treated as a conjuction word, similar to 'and' - again, the endless recursion helps us to add two suffixes (\", children\" \"and women\") on top of \"Mary knows\".\n",
    "-The expression \"John and Mary\" is equivalent to \"they\", so it's treated as a nominative pronoun.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the given examples and the number of coordination:\n",
    "\"John and Mary\" - 2 people\n",
    "\"John or Mary\" - 1 people\n",
    "\"John or the children\" - either one or an unspecified (bigger than 1) number of people.\n",
    "\n",
    "The first example was already dealt with in the last question - by treating \"NP and NP\" as equivalent to \"They\".\n",
    "The case of 'or' is a bit more complicated, because it can refer to either singular or plural, as the last two examples show. One of the weaknesses of our grammar (as we demonstrate in the next question), is not separating singluar and plural nouns. So a possible solution to deal with the problem is to separate the category NP into NP_PL and NP_SIN (plural and singular). After we split it, we can safely treat \"NP_PL or NP_PL\" as plural (similar to \"they\"), and \"NP_SIN or NP_SIN\" as singluar (similar to he/she). \n",
    "\n",
    "The mixed case, \"NP_PL or NP_SIN\" is still ambiguous - but this is not a problem with our grammar, but with the English language. When we say 'John or the children', we don't know whether we're talking about singluar or plural. The correct grammar in those cases is the singluar grammar - \"John or the children love him\" (rather than \"loves\"), so we can treat the mix case as similar to 'They'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither of the issues we pointed out on the previous question are now fixed. We're still prone to illogical nouns use (\"telescope eats a children with boy on a John\" parses ok) and still prone to endlessley stacking sentences on top of each other. \n",
    "Some of our new added NPs (\"women\", \"men\") are plural - meaning swapping them with a singluar NP is problematic. For example, 'Mary knows women' makes sense, but \"Mary knows hill\" is gramatically wrong - we would have needed a determiner (\"Mary knows **a** hill\") to create a correct sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_sentence (\"Mary knows hill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new problem is introduced with conjuction - our conjuction role treats the comma sign and the word 'and' in the same way. Which means we can create a list with any combination of those. Of course, English grammar demands each item on the list to be comma separated, while \"and\" only appears before the last item (\"a, b, c and d\"). This example breaks both roles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_sentence (\"Mary knows men and children and book and telescope and hill, boy, John\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has a model called generate which is able to generate sentences given a CFG grammer. \n",
    "Our goal in this part is to create a generator for a PCFG instead of a CFG.\n",
    "\n",
    "So lets start by writing our generator function, which, given a PCFG, return a tree representing the generation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generates a tree from a PCFG grammer \n",
    "def pcfg_generate(grammar):\n",
    "    start = grammar.start()\n",
    "    return(pcfg_generate_root(grammar, start))\n",
    "    \n",
    "# generates a tree from a given root based on the PCFG grammer    \n",
    "def pcfg_generate_root(grammar, root):\n",
    "    #if it's not a terminal it means we have to generate from the probabilities\n",
    "    if isinstance(root, Nonterminal):\n",
    "        item_productions = grammar.productions(lhs=root)\n",
    "        dict = {}\n",
    "        for pr in item_productions: dict[pr.rhs()] = pr.prob()\n",
    "        item_probDist = DictionaryProbDist(dict)\n",
    "        prod = item_probDist.generate()\n",
    "        if (len(prod) == 2):\n",
    "            lh = prod[0]\n",
    "            rh = prod[1]\n",
    "            lh_tree = pcfg_generate_root(grammar, lh)\n",
    "            rh_tree = pcfg_generate_root(grammar, rh)\n",
    "            return Tree(root, [lh_tree, rh_tree])\n",
    "        else:\n",
    "            lh = prod[0]\n",
    "            return Tree(root,[pcfg_generate_root(grammar, lh)])\n",
    "           \n",
    "    #if it's a terminal we can just return it\n",
    "    else:\n",
    "        return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test our function with the *toy_pcfg2* grammer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tree = pcfg_generate(toy_pcfg2)\n",
    "Tree.fromstring(example_tree.pformat()).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, not all the resulting sentences makes sense... This might be due to the fact the PCFG is based on distribution, and therefore it will always lean towards certain phrases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now continue on to do some validations in the next subsections - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we are going to do is to generate 1000 random trees given the *toy_pcfg2* grammer, and save all resulting trees into a file called \"toy_pcfg2.gen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_trees_file(n=1000):\n",
    "    for i in range(n):\n",
    "        current_tree = pcfg_generate(toy_pcfg2)\n",
    "        with open(\"toy_pcfg2.gen\", \"a+\") as f:\n",
    "            current_tree.pprint(stream=f)\n",
    "            f.write(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_trees_file(n=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now once we created such file we can use it to conduct some experiments on the resulting trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is to read our trees from our file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read_file = open(\"toy_pcfg2.gen\", \"r\")\n",
    "buffer = \"\"\n",
    "for line in read_file :\n",
    "    buffer += line\n",
    "trees = buffer.split(\"*\")\n",
    "trees = trees[:len(trees) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our trees, we would like to compute the frequency distribution of each non-terminal and pre-terminal in the generated corpus - to do so the first thing we're going to need is the tree's productions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toy_sample_productions = []\n",
    "for tree in trees:\n",
    "    toy_sample_productions += Tree.fromstring(tree).productions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have those we can calculate the distribution of each non-terminal and pre-terminal, because it's the same as the productions distributions.\n",
    "\n",
    "When using NLTK we found there are two ways of doing so, one is using native functions in the NLTK library, and the other is to implement it by ourselves. We will demonstrate both and use that fact to evaluate our own function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way, using NLTK native function, is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One way of achieving our goal\n",
    "pcfg_induced = induce_pcfg(Nonterminal('S'), toy_sample_productions).productions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second way is to use ConditionalFreqDist for every left-hand and right-hand side of the production, and then create a ConditionalProbDist using MLE (like we did in the previous assignemnts), this will also result in the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_productions_probabilities(productions):\n",
    "    #And another way\n",
    "    cfd = nltk.ConditionalFreqDist((production.lhs(), production.rhs()) for production in productions)\n",
    "    cpd_mle = nltk.ConditionalProbDist(cfd, nltk.MLEProbDist)\n",
    "    return cpd_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now if we compare the distributions from both methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toy_sample_probability = calc_productions_probabilities(toy_sample_productions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First Method:\")\n",
    "for nt in pcfg_induced:\n",
    "    print(nt)\n",
    "print(\"\\nSecond Method:\")\n",
    "for cond in toy_sample_probability:\n",
    "    for sample in toy_sample_probability[cond].samples():\n",
    "        print(\"{} -> {} {}\".format(cond, sample, toy_sample_probability[cond].prob(sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as we can see the results are the same (give or take rounding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we calculated the distribution for our toy sample, we can calculate them for the entire **toy_pcfg2** grammar, we will use the results in the next section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toy_pcfg_probability = calc_productions_probabilities(toy_pcfg2.productions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we would like to calculate next is a measure called **KL Divergence**.\n",
    "<br>The *Kullbackâ€“Leibler divergence*(also called relative entropy) is a measure for how much one probability distribution differ from another.\n",
    "<br> In order to do so we will consider two distributions,\n",
    "<br> $P = (x_1: p_1, x_2: p_2, \\dots , x_n: p_n)$ and $Q = (y_1: q_1, y_2: q_2, \\dots, y_n: q_n)$ such that $\\Sigma{p_i} = 1$ and $\\Sigma{q_i} = 1$\n",
    "<br>The KL Divergence is then defined as:\n",
    "<br>$KL(P,Q) = \\Sigma_{i=1, \\dots, n} {p_i * \\log{\\frac{p_i}{q_i}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can notice this calculation is somewhat problematic because it assumes some things that doesn't always hold in real world:\n",
    "- Both $P$ and $Q$ are defined over the same outcomes $x_i$ \n",
    "- We never face $q_i = 0$ for some $i$ or $p_i = 0$ for some $i$\n",
    "\n",
    "One way of facing that is to decide $0 * \\log{0} = 0$ but this only help when $p_i = 0$ or both $p_i = 0$ and $q_i = 0$, so in the case when $p_i \\neq 0$ and $q_i = 0$ we define the difference as infinite. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method of dealing with those issue might work in some cases and calculations, but in our case $P$ and $Q$ are derived from observations and sample counting - that is, $P$ and $Q$ are probability distributions derived from frequency distributions. We have to therfore take into account some things that might happen:\n",
    "- there is a big chance for unseen events - meaning samples that appear in one probability but not the other.\n",
    "- Because of that we can't define the difference as infinite.\n",
    "\n",
    "In order to deal with that issue, we have to implement a *smoothing* technique on the measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick way of applying a smoothing over the measure is to use a method called *absolute discounting*.\n",
    "<br> Absolute discounting would work in our case as follows:\n",
    "- We define a small cconstant $\\epsilon$ (for example $\\epsilon = 0.0001$)\n",
    "- Define $SP = {a, b, c}$ the samples observed in $P$\n",
    "- Define $CP = |SP| = 3$, the number of samples observed in $P$\n",
    "- Similarly, $SQ = {a, b, d}$ and $CQ = 3$\n",
    "- Define $SU = SP U SQ = {a, b, c, d}$ - all the observed samples and $CU = |SU| = 4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we define all of this terms, we can them modify our probabilities (smooth) as follows:\n",
    "- let $pc = \\epsilon * \\frac{|SU-SP|}{|SP|}$ and $qc = \\epsilon * \\frac{|SU-SQ|}{|SQ|}$\n",
    "- for each $p_i$ :\n",
    " - let $p'_i = p_i - pc$ if $i\\in{SP}$\n",
    " - let $p'_i = \\epsilon$ otherwise\n",
    "- similarly for $q'_i$ with $qc$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we do so, we never get a 0 probability because each probability is defined by it's relative part in the sample(smoothing). and so now we can calculate $KL(P', Q')$ without getting infinite difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the smoothing technique we can go ahead and implement the function in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_kl_divergence(p, q, eps=0.0001):\n",
    "    sp = list(p.samples())\n",
    "    cp = len(sp)\n",
    "    sq = list(q.samples())\n",
    "    cq = len(sq)\n",
    "    su = set(sp + sq)\n",
    "    cu = len(su)\n",
    "    pc = eps * ((cq) / float(cp))\n",
    "    qc = eps * ((cp) / float(cq))\n",
    "    kl = 0.0\n",
    "    for i in su:\n",
    "        if i in sp:\n",
    "             p_i = p.prob(i) - pc\n",
    "        else:\n",
    "            p_i = eps\n",
    "        if i in sq:\n",
    "            q_i = q.prob(i) - qc\n",
    "        else:\n",
    "            q_i = eps\n",
    "            \n",
    "        kl +=  p_i * log(p_i / float(q_i))\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we would like to apply it to our two probability distributions (one is our toy sample and one is the original toy test grammar):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divergence = {}\n",
    "for condition in toy_sample_probability.conditions():\n",
    "    kl = compute_kl_divergence(toy_sample_probability[condition], toy_pcfg_probability[condition])\n",
    "    divergence[condition] = kl\n",
    "print(\"Divergence:\", divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyze the results in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've just seen, the results are similar on some of the tags (for example S and PP), and divert on others. \n",
    "The similar rules (with 0 diversion) are similar due to the fact that in the original toy grammar, this rules have only one derivation rule and it is derived with a 1.0 probabilty, and so our samples will all use that rule, the diversion happens once we look at rules with more \"options\", then in our samples such rules won't exist at all (because our sampler is based on the probabilities and is limited in the size it generates - just 1000 trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<TODO> add more here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we'd like to do now is to explore how we can induce a PCFG from a given treebank.\n",
    "<br>To do so we would use the Penn Treebank given by NLTK. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we'd like to do is to induce the PCFG from the original treebank without transforming it into a CNF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Penn Treebank comes with some additional information for the tags, for example some NP tags has additional information like NP-something(change this) , we don't need this information in our induction(it actually might mess up the results because each NP tag then will be considered seperately and this will mess the probabilities). \n",
    "<br>Another thing it holds is some NONE tags, used for \"missing\" words, for example a sentence like:\n",
    "\n",
    "We don't want to take NONE tags into account in our induced PCFG, so a suggested method is to ignore all NONE tags (just remove their branch from the tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets implement a function that simplifies a tag then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simplify_functional_tag(tag):\n",
    "    if '-' in tag:\n",
    "        tag = tag.split('-')[0]\n",
    "    # -NONE- tag\n",
    "    if \"\" == tag:\n",
    "        return \" \"\n",
    "    return tag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after we did so we can use this function to create all productions given a tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tag(tree):\n",
    "    if isinstance(tree, Tree):\n",
    "        simplified_tag = simplify_functional_tag(tree.label())\n",
    "        # -NONE- tag\n",
    "        if simplified_tag == \" \":\n",
    "            return \" \"\n",
    "        return Nonterminal(simplify_functional_tag(tree.label()))\n",
    "    else:\n",
    "        return tree\n",
    "\n",
    "def tree_to_production(tree):\n",
    "    # if the root is NONE we don't need to parse it more\n",
    "    if(\" \" == get_tag(tree)):\n",
    "        return\n",
    "    else:\n",
    "        return Production(get_tag(tree), [get_tag(child) for child in tree])\n",
    "\n",
    "def tree_to_productions(tree):\n",
    "    yield tree_to_production(tree)\n",
    "    for child in tree:\n",
    "        if isinstance(child, Tree):\n",
    "            for prod in tree_to_productions(child):\n",
    "                if prod:\n",
    "                    yield prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we get all the productions of a given tree, all we have to do is to induce a PCFG from those productions, to so do we will use NLTK function called **induce_pcfg**, what this function does is-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each production **A -> B C** in a list of productions the probability of it in a PCFG is\n",
    "<br>$P(B, C|A) = \\frac{count(A -> B C)}{count(A -> *)}$ so it's the relation between how many times we've seen *A -> B C* compared to all the possible rules for *A*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So implementing it in python we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pcfg_learn(treebank, n):\n",
    "    trees = treebank.parsed_sents()[:n]\n",
    "    trees_productions = []\n",
    "    for tree in trees:\n",
    "        productions = tree_to_productions(tree)\n",
    "        trees_productions += [prod for prod in productions]\n",
    "    pcfg_induced = induce_pcfg(Nonterminal('S'), trees_productions)\n",
    "    return pcfg_induced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "treebank = LazyCorpusLoader('treebank/combined', BracketParseCorpusReader, r'wsj_.*\\.mrg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we'd like to do is to examine the tree bank a little bit, we'd like to see how many internal nodes are in it, and how many productions are in it (we shall do so for 200 trees and not the entire treebank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_productions(treebank, n):\n",
    "    trees = treebank.parsed_sents()[:n]\n",
    "    trees_productions = []\n",
    "    for tree in trees:\n",
    "        productions = tree_to_productions(tree)\n",
    "        trees_productions += [prod for prod in productions]\n",
    "    return len([prod for prod in trees_productions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_tree_nodes(tree):\n",
    "    if isinstance(tree, Tree):\n",
    "        child_nodes = 0.0\n",
    "        for child in tree:\n",
    "            child_nodes += count_tree_nodes(child)\n",
    "        return(1 + child_nodes)\n",
    "    else:\n",
    "        # leave\n",
    "        return 0\n",
    "\n",
    "def count_internal_nodes(treebank, n):\n",
    "    trees = treebank.parsed_sents()[:n]\n",
    "    trees_internals = 0.0\n",
    "    for tree in trees:\n",
    "        trees_internals += count_tree_nodes(tree)\n",
    "    return trees_internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of productions:\", count_productions(treebank, 200))\n",
    "print(\"Number of internals:\", count_internal_nodes(treebank, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there's a slight difference, that might be due to the fact we haven't removed the NONE tags from the internal node counts, so lets see how many NONE tags are in there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_nones_in_tree(tree):\n",
    "    if isinstance(tree, Tree):\n",
    "        tree_tag = tree.label()\n",
    "        child_nodes = 0.0\n",
    "        if(\"NONE\" in tree_tag):\n",
    "            child_nodes = 1        \n",
    "        for child in tree:\n",
    "            child_nodes += count_nones_in_tree(child)\n",
    "        return(child_nodes)\n",
    "    else:\n",
    "        # leave\n",
    "        return 0\n",
    "\n",
    "def count_none(treebank, n):\n",
    "    trees = treebank.parsed_sents()[:n]\n",
    "    none_trees_count = 0.0\n",
    "    for tree in trees:\n",
    "        none_trees_count += count_nones_in_tree(tree)\n",
    "    return none_trees_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of nones:\", count_none(treebank, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as expected all the \"missing\" productions are for the \"NONE\" elements. \n",
    "<br> this result fits what we learned in class - the number of internal nodes is equal to the number of productions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing what we did in the previous section , we don't want to build a pcfg now, we just to look at the frequencies, so what we will do is build a freqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pcfg_freq(treebank, n):\n",
    "    trees = treebank.parsed_sents()[:n]\n",
    "    trees_productions = []\n",
    "    for tree in trees:\n",
    "        productions = tree_to_productions(tree)\n",
    "        trees_productions += [prod for prod in productions]\n",
    "    pcfg_freq = Counter(trees_productions)\n",
    "    return pcfg_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to examine the frequencies on 200 trees from the treebank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "two_hundred_trees_freq = pcfg_freq(treebank, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the distribution of productions according to their frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_distributions(tree_freq, fignum):\n",
    "    _, counts = zip(*tree_freq.items())\n",
    "    counts = (list(counts))\n",
    "    counts.sort()\n",
    "\n",
    "    labels, values = zip(*Counter(counts).items())\n",
    "    indexes = np.arange(len(labels))\n",
    "\n",
    "\n",
    "    plt.figure(num=fignum, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.bar(indexes, values, 1.5)\n",
    "    plt.xticks(indexes, labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions(two_hundred_trees_freq, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, most productions appear only once, it is very rare for a production to appear over 10 times for example. This makes sense considering Penn Treebank is based on paper articles which has a very diverse English, so the productions would be unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "two_hundred_trees_pcfg = pcfg_learn(treebank, 200)\n",
    "four_hunred_trees_pcfg = pcfg_learn(treebank, 400)\n",
    "\n",
    "#TODO - compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of calculating the PCFG of a simple treebank, we'd like to calculate it for a CNF treebank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The firat thing we need to do before transforming a tree into a CNF tree, is to simplify the tree - meaning get rid of NONE tags and simplify the tags.\n",
    "<br>We treat a NONE brunch as a brunch we can remove from the tree because it leads no where.\n",
    "<br>The code below simplifies a tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simplify_functional_tag(tag):\n",
    "    if '-' in tag:\n",
    "        tag = tag.split('-')[0]\n",
    "    # -NONE- tag\n",
    "    if \"\" == tag:\n",
    "        return \" \"\n",
    "    return tag\n",
    "\n",
    "def get_tag_clean(tree):\n",
    "    if isinstance(tree, Tree):\n",
    "        simplified_tag = simplify_functional_tag(tree.label())\n",
    "        return simplified_tag\n",
    "\n",
    "def build_simplified_tree(tree):\n",
    "    if isinstance(tree, Tree):\n",
    "        root_tag = get_tag_clean(tree)\n",
    "        if(\" \" == root_tag):\n",
    "            return \" \"\n",
    "        else:\n",
    "            simplified_child_nodes = [build_simplified_tree(child) for child in tree]\n",
    "            # remove all NONE children\n",
    "            simplified_children = [child for child in  simplified_child_nodes if child != \" \"]\n",
    "            if(len(simplified_children) >= 1):\n",
    "                return(Tree(root_tag, simplified_children))\n",
    "            else:\n",
    "                # This means the only child is NONE so we can remove it from it's parent as well\n",
    "                return \" \"\n",
    "    else:\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we do so we just have to turn a tree into production (notice we don't have to handle the NONE cases anymore as our tree is clear of them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tag_or_value(tree):\n",
    "    if isinstance(tree, Tree):\n",
    "        return Nonterminal(tree.label())\n",
    "    else:\n",
    "        return tree\n",
    "        \n",
    "def cnf_tree_to_production(tree):\n",
    "    return Production(get_tag_or_value(tree),[get_tag_or_value(child) for child in tree])\n",
    "\n",
    "def cnf_tree_to_productions(tree):\n",
    "    yield cnf_tree_to_production(tree)\n",
    "    for child in tree:\n",
    "        if isinstance(child, Tree):\n",
    "            for prod in cnf_tree_to_productions(child):\n",
    "                if prod:\n",
    "                    yield prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after setting up the functions to all stages, letse set it all together to induce a PCFG from a CFG treebank - \n",
    "The stages are, for each tree in the treebank:\n",
    "- Simplify the tree and rid of NONE tags.\n",
    "- Transform the tree into a CNF tree.\n",
    "- Induce the productions from the CNF tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pcfg_cnf_learn(treebank, n):\n",
    "    trees = treebank.parsed_sents()[:n]\n",
    "    trees_productions = []\n",
    "    for tree in trees:\n",
    "        new_tree = build_simplified_tree(tree)\n",
    "        chomsky_normal_form(new_tree, factor='right', horzMarkov=1, vertMarkov=1, childChar='|', parentChar='^')\n",
    "        productions = cnf_tree_to_productions(new_tree)\n",
    "        trees_productions += [prod for prod in productions]\n",
    "    pcfg_induced = induce_pcfg(Nonterminal('S'), trees_productions)\n",
    "    return pcfg_induced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank = LazyCorpusLoader('treebank/combined', BracketParseCorpusReader, r'wsj_.*\\.mrg')\n",
    "pcfg_cnf = two_hundred_trees_pcfg = pcfg_cnf_learn(treebank, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to show the number of productions in the CNF treebank - so we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of productions:\", len(pcfg_cnf.productions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen earlier that in the original treebank we had 8692 internal nodes, now we have 2797 productions, but these are unique productions, to find out exactly how many productions we have (not only unique ones) lets count the amount of internal nodes in the CNF treebank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pcfg_cnf_internals(treebank, n):\n",
    "    trees = treebank.parsed_sents()[:n]\n",
    "    trees_internals = 0.0\n",
    "    for tree in trees:\n",
    "        new_tree = build_simplified_tree(tree)\n",
    "        chomsky_normal_form(new_tree, factor='right', horzMarkov=1, vertMarkov=1, childChar='|', parentChar='^')\n",
    "        trees_internals += count_tree_nodes(new_tree)\n",
    "    return trees_internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Internal nodes/ number of productions:\", pcfg_cnf_internals(treebank, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see turning it into a CNF added more productions(and internal nodes) over all. This makes sense considering a CNF tree is usally bigger than the non-CNF tree.\n",
    "\n",
    "#TODO - add conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we'd like to do now is to explore the CFG hypothesis that a node expansion is independent from its location within a tree.\n",
    "<br>We will do so by exploring the expansion of the NP category in thr CNF trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we'd like to do is show how the NP's RHS and LHS distributed , meaning for each LHS show the distribution of it's RHS. to do so we will look at each NP tag in the treebank and see it's production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_np_productions(treebank, n):\n",
    "    trees = treebank.parsed_sents()[:n]\n",
    "    np_productions = []\n",
    "    for tree in trees:\n",
    "        productions = tree_to_productions(tree)\n",
    "        for prod in productions:\n",
    "            if(prod.lhs() == Nonterminal('NP')):\n",
    "                np_productions += [(prod.rhs())]\n",
    "    return np_productions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_productions = get_np_productions(treebank, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the productions we's like to look at an histogram of them, to do so we will use the counter object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = Counter(np_productions)\n",
    "counts = counter.most_common()\n",
    "\n",
    "ind = [count for key, count in counts]\n",
    "bins = range(0, len(counts))\n",
    "bins = [bin * 5 for bin in bins]\n",
    "\n",
    "keys = [key for key, count in counts]\n",
    "\n",
    "plt.figure(num=6, figsize=(18, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.bar(bins, ind , align='edge', width=1)\n",
    "plt.xticks(bins, keys, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'd like to to see what is the distribution of NP that comes after S, to so we need to get all NP productions after an S:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tree_to_np_under_x_productions(x, tree, is_child):\n",
    "    productions = []\n",
    "    if isinstance(tree, Tree):\n",
    "        if(get_tag(tree) == Nonterminal(x)):\n",
    "            for child in tree:\n",
    "                productions += tree_to_np_under_x_productions(x, child, True)\n",
    "            return productions\n",
    "        elif(get_tag(tree) == Nonterminal('NP') and is_child):\n",
    "            productions += [Production(get_tag(tree), [get_tag(child) for child in tree])]  \n",
    "        for child in tree:\n",
    "            productions += tree_to_np_under_x_productions(x, child, False) \n",
    "    return productions\n",
    "    \n",
    "    \n",
    "def get_np_s_production(treebank, n):\n",
    "    trees = treebank.parsed_sents()[:n]\n",
    "    np_s_productions = []\n",
    "    for tree in trees:\n",
    "        np_s_productions += tree_to_np_under_x_productions('S', tree, False)\n",
    "    np_s_productions = [prod.rhs() for prod in np_s_productions]\n",
    "    return np_s_productions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_s_productions = get_np_s_production(treebank, 200)\n",
    "counter_s_np = Counter(np_s_productions)\n",
    "counts_s_np = counter_s_np.most_common()\n",
    "\n",
    "ind_s_np = [count for key, count in counts_s_np]\n",
    "bins_s_np = range(0, len(counts_s_np))\n",
    "bins_s_np = [bin * 10 for bin in bins_s_np]\n",
    "keys_s_np = [key for key, count in counts_s_np]\n",
    "\n",
    "plt.figure(num=7, figsize=(20, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.bar(bins_s_np, ind_s_np , align='edge', width=1)\n",
    "plt.xticks(bins_s_np, keys_s_np, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now lets do the same for 'VP' tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_np_vp_production(treebank, n):\n",
    "    trees = treebank.parsed_sents()[:n]\n",
    "    np_vp_productions = []\n",
    "    for tree in trees:\n",
    "        np_vp_productions += tree_to_np_under_x_productions('VP', tree, False)\n",
    "    np_vp_productions = [prod.rhs() for prod in np_vp_productions]\n",
    "    return np_vp_productions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_vp_productions = get_np_vp_production(treebank, 200)\n",
    "counter_vp_np = Counter(np_vp_productions)\n",
    "counts_vp_np = counter_vp_np.most_common()\n",
    "\n",
    "ind_vp_np = [count for key, count in counts_vp_np]\n",
    "bins_vp_np = range(0, len(counts_vp_np))\n",
    "bins_vp_np = [bin * 10 for bin in bins_vp_np]\n",
    "keys_vp_np = [key for key, count in counts_vp_np]\n",
    "\n",
    "plt.figure(num=7, figsize=(20, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.bar(bins_s_np, ind_s_np , align='edge', width=1)\n",
    "plt.xticks(bins_vp_np, keys_vp_np, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what we'd like to do is to compare those distributions, so lets first turn all of the frequency plots into proper distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_freq = nltk.ConditionalFreqDist((Nonterminal('NP'), prod) for prod in np_productions)\n",
    "np_s_freq = nltk.ConditionalFreqDist((Nonterminal('NP'), prob) for prob in np_s_productions)\n",
    "np_vp_freq = nltk.ConditionalFreqDist((Nonterminal('NP'), prob) for prob in np_vp_productions)\n",
    "\n",
    "np_probs = nltk.ConditionalProbDist(np_freq, nltk.MLEProbDist)\n",
    "np_s_probs = nltk.ConditionalProbDist(np_s_freq, nltk.MLEProbDist)\n",
    "np_vp_probs = nltk.ConditionalProbDist(np_vp_freq, nltk.MLEProbDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we'd like to compare each pair using KL-Divergence we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_divergence_np(prob_a, prob_b):\n",
    "    divergence_np = {}\n",
    "    for condition in prob_a.conditions():\n",
    "        kl_np_s = compute_kl_divergence(prob_a[condition], prob_b[condition])\n",
    "        divergence_np[condition] = kl\n",
    "    return(divergence_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by comparing the entire treebank to the ones starting with s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_divergence_np(np_probs, np_s_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_divergence_np(np_probs, np_vp_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_divergence_np(np_s_probs, np_vp_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO - conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment we will construct a viterbi parser for the previously induced PCFG and then evaluate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1 - building the parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go ahead and build a parser, we need to set up our training data and our test data, to do so we will split the Penn Treebank into two parts - the training corpus will contain 80% of the trees, and the testing corpus will contain the other 20%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do so, lets explore the treebank so we know the length each set should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank = LazyCorpusLoader('treebank/combined', BracketParseCorpusReader, r'wsj_.*\\.mrg')\n",
    "trees = treebank.parsed_sents()\n",
    "total = len(trees)\n",
    "eighty_pr = int(total * 0.8)\n",
    "twenty_pr = int(total * 0.2)\n",
    "print(\"80%: \", eighty_pr)\n",
    "print(\"20%: \", twenty_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the split fits the division from the question - about 3200 train trees and about 800 test trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we know the exact size each should have, and we can write functions to get each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_set(start, end):\n",
    "    treebank = LazyCorpusLoader('treebank/combined', BracketParseCorpusReader, r'wsj_.*\\.mrg')\n",
    "    trees = treebank.parsed_sents()[start : end - 1]\n",
    "    for tree in trees:\n",
    "        yield tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = get_set(0, 3131)\n",
    "test_set = get_set(3131, 3131 + 782)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we'd like to do is to learn a PCFG over the Chomsky Normal Form version of the treebank. \n",
    "To do so we will use the functions from part 2 and the training set size as the *n* input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank = LazyCorpusLoader('treebank/combined', BracketParseCorpusReader, r'wsj_.*\\.mrg')\n",
    "pcfg_training = pcfg_cnf_learn(treebank, 3131)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now once we have a PCFG we can finally construct a **Viterbi Parser** for our treebank. \n",
    "To do so we will use NLTK native function to build such parser, and we will feed it with our PCFG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_parser = nltk.ViterbiParser(pcfg_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test our parser on a few example sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'any country is fine , as long as you know how to drive'.split()\n",
    "p = v_parser.parse(sent)\n",
    "for t in p:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And another few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'we saw a big hill and we climbed on top of it'.split()\n",
    "p = v_parser.parse(sent)\n",
    "for t in p:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"it does not take much to be happy , you just need to try\".split()\n",
    "p = v_parser.parse(sent)\n",
    "for t in p:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And sometimes it fails (because it lacks knowledge about the words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'big cat'.split()\n",
    "p = v_parser.parse(sent)\n",
    "try:\n",
    "    for t in p:\n",
    "        print(t)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the sentences I gave are not that likely - this makes sense considering this treebank is composed out of journalisem articles, just for fun lets try it over a more journalistic sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"many banks are turning away from strict price competition\".split()\n",
    "p = v_parser.parse(sent)\n",
    "for t in p:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as we can see it is actually more likely than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After writing the Viterbi parser, the next thing we'd like to do is to determine how well it preforms, to do so we will use the trusty old measure we defined in the previous assignment - Precision, Recall and Fscore, but a modified version of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we'd like to is to evaluate them in two methods:\n",
    "- unlabeled: The method we previously used (in HW2).\n",
    "- labeled: match regular precision and recall, but also use the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mentioned index refers to the index which is part of the constituents. \n",
    "<br>A constituent is a triplet of the form *(interior node, index of the first word in the sentence covered by it, index of the last word in the sentence covered by it)*. When matching constituents in labeled precision recall what we do is - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO add more info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do then is given a tree - we want to build it's constituents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(l): return flatten(l[0]) + (flatten(l[1:]) if len(l) > 1 else []) if type(l) is list else [l]\n",
    "\n",
    "def parse_inner_labels(tree, final_tags, list_of_consts):\n",
    "    if(isinstance(tree, Tree)):\n",
    "        if(tree.label() in final_tags):\n",
    "            return final_tags.index(tree.label())\n",
    "        else:\n",
    "            child_indices = flatten([parse_inner_labels(child, final_tags, list_of_consts) for child in tree])\n",
    "            list_of_consts.append((tree.label(), child_indices[0], child_indices[len(child_indices) - 1] + 1))\n",
    "            return child_indices\n",
    "\n",
    "def build_constituents(tree):\n",
    "    tagged_words = tree.pos()\n",
    "    final_tags = [b for (a, b) in tagged_words]\n",
    "    leaves = tree.leaves()\n",
    "    list_or_consts = []\n",
    "    parse_inner_labels(tree, final_tags, list_or_consts)\n",
    "    return list_or_consts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will test our function using the examples in the assignment on parsing evaluation by *Scott Farrar*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_tree = Tree(Nonterminal('S') ,[Tree(Nonterminal('NP'),[Tree(Nonterminal('A'), [\"a\"])]), Tree(Nonterminal('VP'), [Tree(Nonterminal('B') ,[\"b\"]), Tree(Nonterminal('PP'), [Tree(Nonterminal('C'), [\"c\"])])])])\n",
    "consts = build_constituents(parse_tree)\n",
    "print(consts)\n",
    "second_parse_tree = Tree(Nonterminal('S'), [Tree(Nonterminal('NP'), [Tree(Nonterminal('A'), [\"a\"])]), Tree(Nonterminal('VP'), [Tree(Nonterminal('B') ,[\"b\"])] ), Tree(Nonterminal('PP') ,[Tree(Nonterminal('C'), [\"c\"])])])\n",
    "consts_two = build_constituents(second_parse_tree)\n",
    "print(consts_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (NNP Moody) (POS 's))\n",
      "  (VP\n",
      "    (VBD said)\n",
      "    (SBAR\n",
      "      (S\n",
      "        (NP (DT those) (NNS returns))\n",
      "        (VP\n",
      "          (VBP compare)\n",
      "          (PP\n",
      "            (IN with)\n",
      "            (NP\n",
      "              (NP\n",
      "                (DT a)\n",
      "                (ADJP (CD 3.8) (NN %))\n",
      "                (JJ total)\n",
      "                (NN return))\n",
      "              (PP\n",
      "                (IN for)\n",
      "                (NP\n",
      "                  (JJ longer-term)\n",
      "                  (NNP Treasury)\n",
      "                  (NNS notes)\n",
      "                  (CC and)\n",
      "                  (NNS bonds)))))))))\n",
      "  (. .))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"'compare'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f962f110336d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimplified\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mviterbi_parsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimplified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mviterbi_parsed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_constituents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/parse/viterbi.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# The most likely constituent table.  This table specifies the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    682\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             raise ValueError(\n\u001b[0;32m--> 684\u001b[0;31m                 \u001b[0;34m\"Grammar does not cover some of the \"\u001b[0m \u001b[0;34m\"input words: %r.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m             )\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'compare'\"."
     ]
    }
   ],
   "source": [
    "for tree in test_set:\n",
    "    simplified = build_simplified_tree(tree)\n",
    "    print(simplified)\n",
    "    viterbi_parsed = v_parser.parse(simplified.leaves())\n",
    "    for t in viterbi_parsed:\n",
    "        print(t)\n",
    "        print(build_constituents(t))\n",
    "    break\n",
    "#     parsed = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
