{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the solution to HW3, written by Yaniv Bin and Tair Hakman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first would like to import all the required modules in order for our code to run properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk import nonterminals, Nonterminal, Production, induce_pcfg\n",
    "from nltk.parse import generate\n",
    "from nltk.grammar import Nonterminal, Production, toy_pcfg2\n",
    "from nltk.probability import DictionaryProbDist\n",
    "from nltk import Tree, CFG, PCFG, Nonterminal\n",
    "from numpy import log\n",
    "from nltk.corpus import LazyCorpusLoader, BracketParseCorpusReader\n",
    "from collections import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after doing so we can go ahead and solve the tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the assignment we will discuss designing CFG for NLP task.\n",
    "<br> We were given the following code which read CFGs from string representation, and parse sentences given a CFG using a  a bottom-up parsing algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 'John left'\n",
      "    [ * John left]\n",
      "  S [ 'John' * left]\n",
      "  R [ NP * left]\n",
      "  S [ NP 'left' * ]\n",
      "  R [ NP IV * ]\n",
      "  R [ NP VP * ]\n",
      "  R [ S * ]\n",
      "(S (NP John) (VP (IV left)))\n",
      "Parsing 'John eats bread'\n",
      "    [ * John eats bread]\n",
      "  S [ 'John' * eats bread]\n",
      "  R [ NP * eats bread]\n",
      "  S [ NP 'eats' * bread]\n",
      "  R [ NP TV * bread]\n",
      "  S [ NP TV 'bread' * ]\n",
      "  R [ NP TV NP * ]\n",
      "  R [ NP VP * ]\n",
      "  R [ S * ]\n",
      "(S (NP John) (VP (TV eats) (NP bread)))\n"
     ]
    }
   ],
   "source": [
    "sg = \"\"\"\n",
    "S -> NP VP\n",
    "VP -> IV | TV NP\n",
    "NP -> 'John' | \"bread\"\n",
    "IV -> 'left'\n",
    "TV -> 'eats'\n",
    "\"\"\"\n",
    "g = CFG.fromstring(sg)\n",
    "\n",
    "# Bottom-up  parser\n",
    "sr_parser = nltk.ShiftReduceParser(g, trace=2)\n",
    "\n",
    "# Parse sentences and observe the behavior of the parser\n",
    "def parse_sentence(sent):\n",
    "    tokens = sent.split()\n",
    "    trees = sr_parser.parse(tokens)\n",
    "    for tree in trees:\n",
    "        print(tree)\n",
    "\n",
    "parse_sentence(\"John left\")\n",
    "parse_sentence(\"John eats bread\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, an important note about our parser, the Shift-Reduce parser. This parser does not support ambiguity at all. If multiple reductions are available for a single word, it will simply choose the first reduction listed (and if it fails, it will not go back to try the second reduction). This means we can't have the same NT on the rhs of two different rules.\n",
    "\n",
    "We're required to support a new list of sentences. In order to explain the process of creating the CFG, we would like to split the sentence list into three different lists (slightly altering the original sentence order). The first list:\n",
    "\n",
    "John left\n",
    "John eats bread\n",
    "John loves Mary\n",
    "She loves John\n",
    "She loves them \n",
    "Everybody loves John\n",
    "A boy loves Mary\n",
    "\n",
    "And here's the grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 'John loves Mary'\n",
      "    [ * John loves Mary]\n",
      "  S [ 'John' * loves Mary]\n",
      "  R [ NP * loves Mary]\n",
      "  S [ NP 'loves' * Mary]\n",
      "  R [ NP TV * Mary]\n",
      "  S [ NP TV 'Mary' * ]\n",
      "  R [ NP TV NP * ]\n",
      "  R [ NP VP * ]\n",
      "  R [ S * ]\n",
      "(S (NP John) (VP (TV loves) (NP Mary)))\n",
      "---------\n",
      "Parsing 'She loves John'\n",
      "    [ * She loves John]\n",
      "  S [ 'She' * loves John]\n",
      "  R [ PR_NOM * loves John]\n",
      "  S [ PR_NOM 'loves' * John]\n",
      "  R [ PR_NOM TV * John]\n",
      "  S [ PR_NOM TV 'John' * ]\n",
      "  R [ PR_NOM TV NP * ]\n",
      "  R [ PR_NOM VP * ]\n",
      "  R [ S * ]\n",
      "(S (PR_NOM She) (VP (TV loves) (NP John)))\n",
      "---------\n",
      "Parsing 'She loves them'\n",
      "    [ * She loves them]\n",
      "  S [ 'She' * loves them]\n",
      "  R [ PR_NOM * loves them]\n",
      "  S [ PR_NOM 'loves' * them]\n",
      "  R [ PR_NOM TV * them]\n",
      "  S [ PR_NOM TV 'them' * ]\n",
      "  R [ PR_NOM TV PR_ACC * ]\n",
      "  R [ PR_NOM VP * ]\n",
      "  R [ S * ]\n",
      "(S (PR_NOM She) (VP (TV loves) (PR_ACC them)))\n",
      "---------\n",
      "Parsing 'Everybody loves John'\n",
      "    [ * Everybody loves John]\n",
      "  S [ 'Everybody' * loves John]\n",
      "  R [ PR_NOM * loves John]\n",
      "  S [ PR_NOM 'loves' * John]\n",
      "  R [ PR_NOM TV * John]\n",
      "  S [ PR_NOM TV 'John' * ]\n",
      "  R [ PR_NOM TV NP * ]\n",
      "  R [ PR_NOM VP * ]\n",
      "  R [ S * ]\n",
      "(S (PR_NOM Everybody) (VP (TV loves) (NP John)))\n",
      "---------\n",
      "Parsing 'A boy loves Mary'\n",
      "    [ * A boy loves Mary]\n",
      "  S [ 'A' * boy loves Mary]\n",
      "  R [ DET * boy loves Mary]\n",
      "  S [ DET 'boy' * loves Mary]\n",
      "  R [ DET NP * loves Mary]\n",
      "  S [ DET NP 'loves' * Mary]\n",
      "  R [ DET NP TV * Mary]\n",
      "  S [ DET NP TV 'Mary' * ]\n",
      "  R [ DET NP TV NP * ]\n",
      "  R [ DET NP VP * ]\n",
      "  R [ S * ]\n",
      "(S (DET A) (NP boy) (VP (TV loves) (NP Mary)))\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "sg = \"\"\"\n",
    "S -> DET NP VP | NP VP | PR_NOM VP\n",
    "NP -> 'John' | 'bread' | 'Mary' | 'boy'\n",
    "IV -> 'left' \n",
    "VP -> IV | TV NP | TV PR_ACC\n",
    "TV -> 'eats' | 'loves'\n",
    "PR_NOM -> 'She' | 'Everybody'\n",
    "PR_ACC -> 'them'\n",
    "DET -> 'A'\n",
    "\"\"\"\n",
    "\n",
    "g = CFG.fromstring(sg)\n",
    "\n",
    "# Bottom-up  parser\n",
    "sr_parser = nltk.ShiftReduceParser(g, trace=2)\n",
    "\n",
    "sentences = \"\"\"John left\n",
    "John eats bread\n",
    "John loves Mary\n",
    "She loves John\n",
    "She loves them \n",
    "Everybody loves John\n",
    "A boy loves Mary\n",
    "\"\"\"\n",
    "\n",
    "sentences = sentences.split(\"\\n\")\n",
    "\n",
    "for i in range(2,7):\n",
    "    parse_sentence(sentences[i])\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This bit was relatively easy to deal with. We've introduced pronouns, who are split (so far) into two categories by their case: Nominative (He, she etc) and accusative (her, them) etc. Nominative pronouns appear in the beginning of sentences, accusative ones in the end. So we've added pronouns as an alternative to NPs in the appropriate places. We've also introduced the category of determiners (only \"A\" so far), which can start a sentence before a noun (\"A book\") - note that a determiner can't precede a pronoun (\"A she\" is not legal).\n",
    "\n",
    "Our next sentences are a bit different:\n",
    "\n",
    "They love Mary \n",
    "They love her\n",
    "\n",
    "The difference is the use of 'love' instead of 'loves'. This is a different kind of verb that follows different speakers - \"They love\" and \"He loves\" are legal, but \"They loves\" and \"He love\" aren't. So we'll split our grammar into two categories - sentences of type 1 (without s), and type 2.\n",
    "\n",
    "In English, the category 1 is I/we/you/they (in this case, they will all be followed by 'love'), and category 2 is he/she/it (all followed by 'loves'). This separation is somewhat similar to singular/plural, however it is not the same - note that \"I\" (singluar) is in the same category as \"we\" and \"they\" (plural), while \"you\" can refer to both singluar and plural. So for a lack of better name, we will refer to those categories as categories 1 and 2.\n",
    "\n",
    "So here's our new grammar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 'They love Mary'\n",
      "    [ * They love Mary]\n",
      "  S [ 'They' * love Mary]\n",
      "  R [ PR_NOM1 * love Mary]\n",
      "  S [ PR_NOM1 'love' * Mary]\n",
      "  R [ PR_NOM1 TV1 * Mary]\n",
      "  S [ PR_NOM1 TV1 'Mary' * ]\n",
      "  R [ PR_NOM1 TV1 NP * ]\n",
      "  R [ PR_NOM1 VP1 * ]\n",
      "  R [ S1 * ]\n",
      "  R [ S * ]\n",
      "(S (S1 (PR_NOM1 They) (VP1 (TV1 love) (NP Mary))))\n",
      "---------\n",
      "Parsing 'They love her'\n",
      "    [ * They love her]\n",
      "  S [ 'They' * love her]\n",
      "  R [ PR_NOM1 * love her]\n",
      "  S [ PR_NOM1 'love' * her]\n",
      "  R [ PR_NOM1 TV1 * her]\n",
      "  S [ PR_NOM1 TV1 'her' * ]\n",
      "  R [ PR_NOM1 TV1 PR_ACC * ]\n",
      "  R [ PR_NOM1 VP1 * ]\n",
      "  R [ S1 * ]\n",
      "  R [ S * ]\n",
      "(S (S1 (PR_NOM1 They) (VP1 (TV1 love) (PR_ACC her))))\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "#1 = I, we, you, they\n",
    "#2 = He, she, it\n",
    "\n",
    "sg = \"\"\"\n",
    "S -> S1 | S2\n",
    "S1 -> PR_NOM1 VP1\n",
    "VP1 -> TV1 NP | TV1 PR_ACC\n",
    "TV1 -> 'love'\n",
    "PR_NOM1 -> 'They'\n",
    "\n",
    "S2 -> DET NP VP2 | NP VP2 | PR_NOM2 VP2\n",
    "VP2 -> IV | TV2 NP | TV2 PR_ACC\n",
    "TV2 -> 'eats' | 'loves'\n",
    "PR_NOM2 -> 'She' | 'Everybody'\n",
    "\n",
    "NP -> 'John' | 'bread' | 'Mary' | 'boy'\n",
    "IV -> 'left' \n",
    "PR_ACC -> 'them' | 'her'\n",
    "DET -> 'A'\n",
    "\"\"\"\n",
    "\n",
    "g = CFG.fromstring(sg)\n",
    "\n",
    "# Bottom-up  parser\n",
    "sr_parser = nltk.ShiftReduceParser(g, trace=2)\n",
    "\n",
    "sentences = \"\"\"John left\n",
    "John eats bread\n",
    "John loves Mary\n",
    "She loves John\n",
    "She loves them \n",
    "Everybody loves John\n",
    "A boy loves Mary\n",
    "They love Mary \n",
    "They love her\n",
    "\"\"\"\n",
    "\n",
    "sentences = sentences.split(\"\\n\")\n",
    "\n",
    "for i in range(7,9):\n",
    "    parse_sentence(sentences[i])\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we split most rules into types 1 and 2. Note that accusative pronounes aren't changed - the word \"her\" will be the same in both \"I love her\" and \"she loves her\".\n",
    "Our last group of sentences is:\n",
    "\n",
    "John gave Mary a heavy book\n",
    "John gave it to Mary\n",
    "\n",
    "This is when we encounter the limitations of the SRP. Looking at the first sentence, \"John gave Mary\" is already a legal sentence in our grammar (assuming the verb \"gave\" is added). So once we parse \"John gave Mary\", before the parser continues to the next word, those words will be reduced to \"S\". The best solution would be to use a better, less limited parser that supports ambiguity. But sticking with the SR parser, our solution will be a bit untidy - we will simply add a rule that adds the rest of the words after S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: S -> S1 will never be used\n",
      "Warning: S -> S2 will never be used\n",
      "Warning: S -> S DET AD_NP will never be used\n",
      "Warning: S -> S PR_NP will never be used\n",
      "Warning: S1 -> PR_NOM1 VP1 will never be used\n",
      "Warning: VP1 -> TV1 NP will never be used\n",
      "Warning: VP1 -> TV1 PR_ACC will never be used\n",
      "Warning: TV1 -> 'love' will never be used\n",
      "Warning: PR_NOM1 -> 'They' will never be used\n",
      "Warning: S2 -> DET NP VP2 will never be used\n",
      "Warning: S2 -> NP VP2 will never be used\n",
      "Warning: S2 -> PR_NOM2 VP2 will never be used\n"
     ]
    }
   ],
   "source": [
    "sg = \"\"\"\n",
    "S -> S1 | S2 | S DET AD_NP | S PREP_NP\n",
    "S1 -> PR_NOM1 VP1\n",
    "VP1 -> TV1 NP | TV1 PR_ACC\n",
    "TV1 -> 'love'\n",
    "PR_NOM1 -> 'They'\n",
    "\n",
    "S2 -> DET NP VP2 | NP VP2 | PR_NOM2 VP2 | \n",
    "VP2 -> IV2 | TV2 NP | TV2 PR_ACC\n",
    "TV2 -> 'eats' | 'loves' | 'gave'\n",
    "PR_NOM2 -> 'She' | 'Everybody'\n",
    "IV2 -> 'left'\n",
    "\n",
    "NP -> 'John' | 'bread' | 'Mary' | 'boy' | 'book'\n",
    "AD_NP -> ADJ NP\n",
    "PREP_NP -> PREP NP\n",
    "PR_ACC -> 'them' | 'her' | 'it'\n",
    "PREP -> \"to\"\n",
    "DET -> 'A' | 'a'\n",
    "ADJ -> 'heavy'\n",
    "\"\"\"\n",
    "\n",
    "g = CFG.fromstring(sg)\n",
    "\n",
    "# Bottom-up  parser\n",
    "sr_parser = nltk.ShiftReduceParser(g, trace=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the use of infinite recursion causes NLTK to throw some warnings - however they are false, as all sentences are parsed (and clearly all roles are used).\n",
    "\n",
    "Let's verify all 11 sentences are parsed correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 'John left'\n",
      "    [ * John left]\n",
      "  S [ 'John' * left]\n",
      "  R [ NP * left]\n",
      "  S [ NP 'left' * ]\n",
      "  R [ NP IV2 * ]\n",
      "  R [ NP VP2 * ]\n",
      "  R [ S2 * ]\n",
      "  R [ S * ]\n",
      "(S (S2 (NP John) (VP2 (IV2 left))))\n",
      "---------\n",
      "Parsing 'John eats bread'\n",
      "    [ * John eats bread]\n",
      "  S [ 'John' * eats bread]\n",
      "  R [ NP * eats bread]\n",
      "  S [ NP 'eats' * bread]\n",
      "  R [ NP TV2 * bread]\n",
      "  S [ NP TV2 'bread' * ]\n",
      "  R [ NP TV2 NP * ]\n",
      "  R [ NP VP2 * ]\n",
      "  R [ S2 * ]\n",
      "  R [ S * ]\n",
      "(S (S2 (NP John) (VP2 (TV2 eats) (NP bread))))\n",
      "---------\n",
      "Parsing 'John loves Mary'\n",
      "    [ * John loves Mary]\n",
      "  S [ 'John' * loves Mary]\n",
      "  R [ NP * loves Mary]\n",
      "  S [ NP 'loves' * Mary]\n",
      "  R [ NP TV2 * Mary]\n",
      "  S [ NP TV2 'Mary' * ]\n",
      "  R [ NP TV2 NP * ]\n",
      "  R [ NP VP2 * ]\n",
      "  R [ S2 * ]\n",
      "  R [ S * ]\n",
      "(S (S2 (NP John) (VP2 (TV2 loves) (NP Mary))))\n",
      "---------\n",
      "Parsing 'She loves John'\n",
      "    [ * She loves John]\n",
      "  S [ 'She' * loves John]\n",
      "  R [ PR_NOM2 * loves John]\n",
      "  S [ PR_NOM2 'loves' * John]\n",
      "  R [ PR_NOM2 TV2 * John]\n",
      "  S [ PR_NOM2 TV2 'John' * ]\n",
      "  R [ PR_NOM2 TV2 NP * ]\n",
      "  R [ PR_NOM2 VP2 * ]\n",
      "  R [ S2 * ]\n",
      "  R [ S * ]\n",
      "(S (S2 (PR_NOM2 She) (VP2 (TV2 loves) (NP John))))\n",
      "---------\n",
      "Parsing 'She loves them'\n",
      "    [ * She loves them]\n",
      "  S [ 'She' * loves them]\n",
      "  R [ PR_NOM2 * loves them]\n",
      "  S [ PR_NOM2 'loves' * them]\n",
      "  R [ PR_NOM2 TV2 * them]\n",
      "  S [ PR_NOM2 TV2 'them' * ]\n",
      "  R [ PR_NOM2 TV2 PR_ACC * ]\n",
      "  R [ PR_NOM2 VP2 * ]\n",
      "  R [ S2 * ]\n",
      "  R [ S * ]\n",
      "(S (S2 (PR_NOM2 She) (VP2 (TV2 loves) (PR_ACC them))))\n",
      "---------\n",
      "Parsing 'Everybody loves John'\n",
      "    [ * Everybody loves John]\n",
      "  S [ 'Everybody' * loves John]\n",
      "  R [ PR_NOM2 * loves John]\n",
      "  S [ PR_NOM2 'loves' * John]\n",
      "  R [ PR_NOM2 TV2 * John]\n",
      "  S [ PR_NOM2 TV2 'John' * ]\n",
      "  R [ PR_NOM2 TV2 NP * ]\n",
      "  R [ PR_NOM2 VP2 * ]\n",
      "  R [ S2 * ]\n",
      "  R [ S * ]\n",
      "(S (S2 (PR_NOM2 Everybody) (VP2 (TV2 loves) (NP John))))\n",
      "---------\n",
      "Parsing 'A boy loves Mary'\n",
      "    [ * A boy loves Mary]\n",
      "  S [ 'A' * boy loves Mary]\n",
      "  R [ DET * boy loves Mary]\n",
      "  S [ DET 'boy' * loves Mary]\n",
      "  R [ DET NP * loves Mary]\n",
      "  S [ DET NP 'loves' * Mary]\n",
      "  R [ DET NP TV2 * Mary]\n",
      "  S [ DET NP TV2 'Mary' * ]\n",
      "  R [ DET NP TV2 NP * ]\n",
      "  R [ DET NP VP2 * ]\n",
      "  R [ S2 * ]\n",
      "  R [ S * ]\n",
      "(S (S2 (DET A) (NP boy) (VP2 (TV2 loves) (NP Mary))))\n",
      "---------\n",
      "Parsing 'They love Mary'\n",
      "    [ * They love Mary]\n",
      "  S [ 'They' * love Mary]\n",
      "  R [ PR_NOM1 * love Mary]\n",
      "  S [ PR_NOM1 'love' * Mary]\n",
      "  R [ PR_NOM1 TV1 * Mary]\n",
      "  S [ PR_NOM1 TV1 'Mary' * ]\n",
      "  R [ PR_NOM1 TV1 NP * ]\n",
      "  R [ PR_NOM1 VP1 * ]\n",
      "  R [ S1 * ]\n",
      "  R [ S * ]\n",
      "(S (S1 (PR_NOM1 They) (VP1 (TV1 love) (NP Mary))))\n",
      "---------\n",
      "Parsing 'They love her'\n",
      "    [ * They love her]\n",
      "  S [ 'They' * love her]\n",
      "  R [ PR_NOM1 * love her]\n",
      "  S [ PR_NOM1 'love' * her]\n",
      "  R [ PR_NOM1 TV1 * her]\n",
      "  S [ PR_NOM1 TV1 'her' * ]\n",
      "  R [ PR_NOM1 TV1 PR_ACC * ]\n",
      "  R [ PR_NOM1 VP1 * ]\n",
      "  R [ S1 * ]\n",
      "  R [ S * ]\n",
      "(S (S1 (PR_NOM1 They) (VP1 (TV1 love) (PR_ACC her))))\n",
      "---------\n",
      "Parsing 'John gave Mary a heavy book'\n",
      "    [ * John gave Mary a heavy book]\n",
      "  S [ 'John' * gave Mary a heavy book]\n",
      "  R [ NP * gave Mary a heavy book]\n",
      "  S [ NP 'gave' * Mary a heavy book]\n",
      "  R [ NP TV2 * Mary a heavy book]\n",
      "  S [ NP TV2 'Mary' * a heavy book]\n",
      "  R [ NP TV2 NP * a heavy book]\n",
      "  R [ NP VP2 * a heavy book]\n",
      "  R [ S2 * a heavy book]\n",
      "  R [ S * a heavy book]\n",
      "  S [ S 'a' * heavy book]\n",
      "  R [ S DET * heavy book]\n",
      "  S [ S DET 'heavy' * book]\n",
      "  R [ S DET ADJ * book]\n",
      "  S [ S DET ADJ 'book' * ]\n",
      "  R [ S DET ADJ NP * ]\n",
      "  R [ S DET AD_NP * ]\n",
      "  R [ S * ]\n",
      "(S\n",
      "  (S (S2 (NP John) (VP2 (TV2 gave) (NP Mary))))\n",
      "  (DET a)\n",
      "  (AD_NP (ADJ heavy) (NP book)))\n",
      "---------\n",
      "Parsing 'John gave it to Mary'\n",
      "    [ * John gave it to Mary]\n",
      "  S [ 'John' * gave it to Mary]\n",
      "  R [ NP * gave it to Mary]\n",
      "  S [ NP 'gave' * it to Mary]\n",
      "  R [ NP TV2 * it to Mary]\n",
      "  S [ NP TV2 'it' * to Mary]\n",
      "  R [ NP TV2 PR_ACC * to Mary]\n",
      "  R [ NP VP2 * to Mary]\n",
      "  R [ S2 * to Mary]\n",
      "  R [ S * to Mary]\n",
      "  S [ S 'to' * Mary]\n",
      "  R [ S PREP * Mary]\n",
      "  S [ S PREP 'Mary' * ]\n",
      "  R [ S PREP NP * ]\n",
      "  R [ S PR_NP * ]\n",
      "  R [ S * ]\n",
      "(S\n",
      "  (S (S2 (NP John) (VP2 (TV2 gave) (PR_ACC it))))\n",
      "  (PR_NP (PREP to) (NP Mary)))\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "sentences = \"\"\"John left\n",
    "John eats bread\n",
    "John loves Mary\n",
    "She loves John\n",
    "She loves them \n",
    "Everybody loves John\n",
    "A boy loves Mary\n",
    "They love Mary \n",
    "They love her\n",
    "John gave Mary a heavy book\n",
    "John gave it to Mary\"\"\"\n",
    "\n",
    "'''\n",
    "Number: singular / plural (e.g., he/they)\n",
    "Gender: masculine / feminine / neutral (e.g., he/she/it)\n",
    "Case: nominative / accusative (e.g., he/him)\n",
    "'''\n",
    "\n",
    "sentences = sentences.split(\"\\n\")\n",
    "\n",
    "for i in range(11):\n",
    "    parse_sentence(sentences[i])\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for gender: we didn't need to encode it, because gender doesn't make a grammatical difference in English. Most pronouns (I/we/you/they) are netural to gender. And he/she, the only pronouns who are specific to a gender, behave the same gramatically (\"he loves her\", \"she loves her\"). The same also goes for nouns who have a clear gender (\"John loves her\", \"Mary loves her\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our grammar isn't perfect, and it does overgenerate in certain cases. One weakness is the lack of separation between different nouns, even though certain nouns can't logically do certain actions. This allows us to parse illogical sentences, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 'bread gave book a heavy Mary'\n",
      "    [ * bread gave book a heavy Mary]\n",
      "  S [ 'bread' * gave book a heavy Mary]\n",
      "  R [ NP * gave book a heavy Mary]\n",
      "  S [ NP 'gave' * book a heavy Mary]\n",
      "  R [ NP TV2 * book a heavy Mary]\n",
      "  S [ NP TV2 'book' * a heavy Mary]\n",
      "  R [ NP TV2 NP * a heavy Mary]\n",
      "  R [ NP VP2 * a heavy Mary]\n",
      "  R [ S2 * a heavy Mary]\n",
      "  R [ S * a heavy Mary]\n",
      "  S [ S 'a' * heavy Mary]\n",
      "  R [ S DET * heavy Mary]\n",
      "  S [ S DET 'heavy' * Mary]\n",
      "  R [ S DET ADJ * Mary]\n",
      "  S [ S DET ADJ 'Mary' * ]\n",
      "  R [ S DET ADJ NP * ]\n",
      "  R [ S DET AD_NP * ]\n",
      "  R [ S * ]\n",
      "(S\n",
      "  (S (S2 (NP bread) (VP2 (TV2 gave) (NP book))))\n",
      "  (DET a)\n",
      "  (AD_NP (ADJ heavy) (NP Mary)))\n"
     ]
    }
   ],
   "source": [
    "parse_sentence(\"bread gave book a heavy Mary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another weakness is the workaround we've added for the last sentences, which basically allows us to add certain combinations (such as DET ADJ NP) at the end of every sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 'John loves Mary a heavy book'\n",
      "    [ * John loves Mary a heavy book]\n",
      "  S [ 'John' * loves Mary a heavy book]\n",
      "  R [ NP * loves Mary a heavy book]\n",
      "  S [ NP 'loves' * Mary a heavy book]\n",
      "  R [ NP TV2 * Mary a heavy book]\n",
      "  S [ NP TV2 'Mary' * a heavy book]\n",
      "  R [ NP TV2 NP * a heavy book]\n",
      "  R [ NP VP2 * a heavy book]\n",
      "  R [ S2 * a heavy book]\n",
      "  R [ S * a heavy book]\n",
      "  S [ S 'a' * heavy book]\n",
      "  R [ S DET * heavy book]\n",
      "  S [ S DET 'heavy' * book]\n",
      "  R [ S DET ADJ * book]\n",
      "  S [ S DET ADJ 'book' * ]\n",
      "  R [ S DET ADJ NP * ]\n",
      "  R [ S DET AD_NP * ]\n",
      "  R [ S * ]\n",
      "(S\n",
      "  (S (S2 (NP John) (VP2 (TV2 loves) (NP Mary))))\n",
      "  (DET a)\n",
      "  (AD_NP (ADJ heavy) (NP book)))\n"
     ]
    }
   ],
   "source": [
    "parse_sentence(\"John loves Mary a heavy book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And since the role is recursive, we can add it as many times as we like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 'John loves Mary a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book'\n",
      "    [ * John loves Mary a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  S [ 'John' * loves Mary a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ NP * loves Mary a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  S [ NP 'loves' * Mary a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ NP TV2 * Mary a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  S [ NP TV2 'Mary' * a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ NP TV2 NP * a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ NP VP2 * a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ S2 * a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ S * a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  S [ S 'a' * heavy book a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ S DET * heavy book a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  S [ S DET 'heavy' * book a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ S DET ADJ * book a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  S [ S DET ADJ 'book' * a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ S DET ADJ NP * a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ S DET AD_NP * a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ S * a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  S [ S 'a' * heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ S DET * heavy book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  S [ S DET 'heavy' * book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ S DET ADJ * book a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  S [ S DET ADJ 'book' * a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ S DET ADJ NP * a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ S DET AD_NP * a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ S * a heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  S [ S 'a' * heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ S DET * heavy book a heavy book a heavy book a heavy book a heavy book]\n",
      "  S [ S DET 'heavy' * book a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ S DET ADJ * book a heavy book a heavy book a heavy book a heavy book]\n",
      "  S [ S DET ADJ 'book' * a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ S DET ADJ NP * a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ S DET AD_NP * a heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ S * a heavy book a heavy book a heavy book a heavy book]\n",
      "  S [ S 'a' * heavy book a heavy book a heavy book a heavy book]\n",
      "  R [ S DET * heavy book a heavy book a heavy book a heavy book]\n",
      "  S [ S DET 'heavy' * book a heavy book a heavy book a heavy book]\n",
      "  R [ S DET ADJ * book a heavy book a heavy book a heavy book]\n",
      "  S [ S DET ADJ 'book' * a heavy book a heavy book a heavy book]\n",
      "  R [ S DET ADJ NP * a heavy book a heavy book a heavy book]\n",
      "  R [ S DET AD_NP * a heavy book a heavy book a heavy book]\n",
      "  R [ S * a heavy book a heavy book a heavy book]\n",
      "  S [ S 'a' * heavy book a heavy book a heavy book]\n",
      "  R [ S DET * heavy book a heavy book a heavy book]\n",
      "  S [ S DET 'heavy' * book a heavy book a heavy book]\n",
      "  R [ S DET ADJ * book a heavy book a heavy book]\n",
      "  S [ S DET ADJ 'book' * a heavy book a heavy book]\n",
      "  R [ S DET ADJ NP * a heavy book a heavy book]\n",
      "  R [ S DET AD_NP * a heavy book a heavy book]\n",
      "  R [ S * a heavy book a heavy book]\n",
      "  S [ S 'a' * heavy book a heavy book]\n",
      "  R [ S DET * heavy book a heavy book]\n",
      "  S [ S DET 'heavy' * book a heavy book]\n",
      "  R [ S DET ADJ * book a heavy book]\n",
      "  S [ S DET ADJ 'book' * a heavy book]\n",
      "  R [ S DET ADJ NP * a heavy book]\n",
      "  R [ S DET AD_NP * a heavy book]\n",
      "  R [ S * a heavy book]\n",
      "  S [ S 'a' * heavy book]\n",
      "  R [ S DET * heavy book]\n",
      "  S [ S DET 'heavy' * book]\n",
      "  R [ S DET ADJ * book]\n",
      "  S [ S DET ADJ 'book' * ]\n",
      "  R [ S DET ADJ NP * ]\n",
      "  R [ S DET AD_NP * ]\n",
      "  R [ S * ]\n",
      "(S\n",
      "  (S\n",
      "    (S\n",
      "      (S\n",
      "        (S\n",
      "          (S\n",
      "            (S\n",
      "              (S (S2 (NP John) (VP2 (TV2 loves) (NP Mary))))\n",
      "              (DET a)\n",
      "              (AD_NP (ADJ heavy) (NP book)))\n",
      "            (DET a)\n",
      "            (AD_NP (ADJ heavy) (NP book)))\n",
      "          (DET a)\n",
      "          (AD_NP (ADJ heavy) (NP book)))\n",
      "        (DET a)\n",
      "        (AD_NP (ADJ heavy) (NP book)))\n",
      "      (DET a)\n",
      "      (AD_NP (ADJ heavy) (NP book)))\n",
      "    (DET a)\n",
      "    (AD_NP (ADJ heavy) (NP book)))\n",
      "  (DET a)\n",
      "  (AD_NP (ADJ heavy) (NP book)))\n"
     ]
    }
   ],
   "source": [
    "parse_sentence(\"John loves Mary a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book a heavy book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we're expected to add support to these sentences:\n",
    "\n",
    "John saw a man with a telescope\n",
    "John saw a man on the hill with a telescope\n",
    "\n",
    "Mary knows men and women\n",
    "Mary knows men, children and women\n",
    "\n",
    "John and Mary eat bread\n",
    "John and Mary eat bread with cheese\n",
    "\n",
    "We can keep using the same trick from the end of last question - a recursive role to allow us to add certain suffixes at the end of a legal sentence (potentially endlessley). For example: if \"John saw a man\" is a legal sentence in our language, we can add two possible suffixes after it: \"on the hill\" and \"with a telescope\". This will make all combinations legal - many of them makes sense (\"John saw a man with a telescope\" \"...man on the hill\" \"...man on the hill with a telescope\" \"...man with a telescope on the hill\"), but also, as we've seen in the last question, we're exposed to infinite loops.\n",
    "\n",
    "Here is the new grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: S -> S1 will never be used\n",
      "Warning: S -> S2 will never be used\n",
      "Warning: S -> S DET AD_NP will never be used\n",
      "Warning: S -> S PREP_NP will never be used\n",
      "Warning: S -> S PREP DET_NP will never be used\n",
      "Warning: S -> S CONJ_NP will never be used\n",
      "Warning: S1 -> PR_NOM1 VP1 will never be used\n",
      "Warning: VP1 -> TV1 NP will never be used\n",
      "Warning: VP1 -> TV1 PR_ACC will never be used\n",
      "Warning: TV1 -> 'love' will never be used\n",
      "Warning: TV1 -> 'eat' will never be used\n",
      "Warning: PR_NOM1 -> 'They' will never be used\n",
      "Warning: PR_NOM1 -> NP CONJ NP will never be used\n",
      "Warning: S2 -> DET NP VP2 will never be used\n",
      "Warning: S2 -> DET NP VP2 will never be used\n",
      "Warning: S2 -> NP VP2 will never be used\n",
      "Warning: S2 -> PR_NOM2 VP2 will never be used\n",
      "Parsing 'John saw a man with a telescope'\n",
      "    [ * John saw a man with a telescope]\n",
      "  S [ 'John' * saw a man with a telescope]\n",
      "  R [ NP * saw a man with a telescope]\n",
      "  S [ NP 'saw' * a man with a telescope]\n",
      "  R [ NP TV2 * a man with a telescope]\n",
      "  S [ NP TV2 'a' * man with a telescope]\n",
      "  R [ NP TV2 DET * man with a telescope]\n",
      "  S [ NP TV2 DET 'man' * with a telescope]\n",
      "  R [ NP TV2 DET NP * with a telescope]\n",
      "  R [ NP TV2 DET_NP * with a telescope]\n",
      "  R [ NP VP2 * with a telescope]\n",
      "  R [ S2 * with a telescope]\n",
      "  R [ S * with a telescope]\n",
      "  S [ S 'with' * a telescope]\n",
      "  R [ S PREP * a telescope]\n",
      "  S [ S PREP 'a' * telescope]\n",
      "  R [ S PREP DET * telescope]\n",
      "  S [ S PREP DET 'telescope' * ]\n",
      "  R [ S PREP DET NP * ]\n",
      "  R [ S PREP DET_NP * ]\n",
      "  R [ S * ]\n",
      "(S\n",
      "  (S (S2 (NP John) (VP2 (TV2 saw) (DET_NP (DET a) (NP man)))))\n",
      "  (PREP with)\n",
      "  (DET_NP (DET a) (NP telescope)))\n",
      "---------\n",
      "Parsing 'John saw a man on the hill with a telescope'\n",
      "    [ * John saw a man on the hill with a telescope]\n",
      "  S [ 'John' * saw a man on the hill with a telescope]\n",
      "  R [ NP * saw a man on the hill with a telescope]\n",
      "  S [ NP 'saw' * a man on the hill with a telescope]\n",
      "  R [ NP TV2 * a man on the hill with a telescope]\n",
      "  S [ NP TV2 'a' * man on the hill with a telescope]\n",
      "  R [ NP TV2 DET * man on the hill with a telescope]\n",
      "  S [ NP TV2 DET 'man' * on the hill with a telescope]\n",
      "  R [ NP TV2 DET NP * on the hill with a telescope]\n",
      "  R [ NP TV2 DET_NP * on the hill with a telescope]\n",
      "  R [ NP VP2 * on the hill with a telescope]\n",
      "  R [ S2 * on the hill with a telescope]\n",
      "  R [ S * on the hill with a telescope]\n",
      "  S [ S 'on' * the hill with a telescope]\n",
      "  R [ S PREP * the hill with a telescope]\n",
      "  S [ S PREP 'the' * hill with a telescope]\n",
      "  R [ S PREP DET * hill with a telescope]\n",
      "  S [ S PREP DET 'hill' * with a telescope]\n",
      "  R [ S PREP DET NP * with a telescope]\n",
      "  R [ S PREP DET_NP * with a telescope]\n",
      "  R [ S * with a telescope]\n",
      "  S [ S 'with' * a telescope]\n",
      "  R [ S PREP * a telescope]\n",
      "  S [ S PREP 'a' * telescope]\n",
      "  R [ S PREP DET * telescope]\n",
      "  S [ S PREP DET 'telescope' * ]\n",
      "  R [ S PREP DET NP * ]\n",
      "  R [ S PREP DET_NP * ]\n",
      "  R [ S * ]\n",
      "(S\n",
      "  (S\n",
      "    (S (S2 (NP John) (VP2 (TV2 saw) (DET_NP (DET a) (NP man)))))\n",
      "    (PREP on)\n",
      "    (DET_NP (DET the) (NP hill)))\n",
      "  (PREP with)\n",
      "  (DET_NP (DET a) (NP telescope)))\n",
      "---------\n",
      "Parsing 'Mary knows men and women'\n",
      "    [ * Mary knows men and women]\n",
      "  S [ 'Mary' * knows men and women]\n",
      "  R [ NP * knows men and women]\n",
      "  S [ NP 'knows' * men and women]\n",
      "  R [ NP TV2 * men and women]\n",
      "  S [ NP TV2 'men' * and women]\n",
      "  R [ NP TV2 NP * and women]\n",
      "  R [ NP VP2 * and women]\n",
      "  R [ S2 * and women]\n",
      "  R [ S * and women]\n",
      "  S [ S 'and' * women]\n",
      "  R [ S CONJ * women]\n",
      "  S [ S CONJ 'women' * ]\n",
      "  R [ S CONJ NP * ]\n",
      "  R [ S CONJ_NP * ]\n",
      "  R [ S * ]\n",
      "(S\n",
      "  (S (S2 (NP Mary) (VP2 (TV2 knows) (NP men))))\n",
      "  (CONJ_NP (CONJ and) (NP women)))\n",
      "---------\n",
      "Parsing 'Mary knows men , children and women'\n",
      "    [ * Mary knows men , children and women]\n",
      "  S [ 'Mary' * knows men , children and women]\n",
      "  R [ NP * knows men , children and women]\n",
      "  S [ NP 'knows' * men , children and women]\n",
      "  R [ NP TV2 * men , children and women]\n",
      "  S [ NP TV2 'men' * , children and women]\n",
      "  R [ NP TV2 NP * , children and women]\n",
      "  R [ NP VP2 * , children and women]\n",
      "  R [ S2 * , children and women]\n",
      "  R [ S * , children and women]\n",
      "  S [ S ',' * children and women]\n",
      "  R [ S CONJ * children and women]\n",
      "  S [ S CONJ 'children' * and women]\n",
      "  R [ S CONJ NP * and women]\n",
      "  R [ S CONJ_NP * and women]\n",
      "  R [ S * and women]\n",
      "  S [ S 'and' * women]\n",
      "  R [ S CONJ * women]\n",
      "  S [ S CONJ 'women' * ]\n",
      "  R [ S CONJ NP * ]\n",
      "  R [ S CONJ_NP * ]\n",
      "  R [ S * ]\n",
      "(S\n",
      "  (S\n",
      "    (S (S2 (NP Mary) (VP2 (TV2 knows) (NP men))))\n",
      "    (CONJ_NP (CONJ ,) (NP children)))\n",
      "  (CONJ_NP (CONJ and) (NP women)))\n",
      "---------\n",
      "Parsing 'John and Mary eat bread'\n",
      "    [ * John and Mary eat bread]\n",
      "  S [ 'John' * and Mary eat bread]\n",
      "  R [ NP * and Mary eat bread]\n",
      "  S [ NP 'and' * Mary eat bread]\n",
      "  R [ NP CONJ * Mary eat bread]\n",
      "  S [ NP CONJ 'Mary' * eat bread]\n",
      "  R [ NP CONJ NP * eat bread]\n",
      "  R [ PR_NOM1 * eat bread]\n",
      "  S [ PR_NOM1 'eat' * bread]\n",
      "  R [ PR_NOM1 TV1 * bread]\n",
      "  S [ PR_NOM1 TV1 'bread' * ]\n",
      "  R [ PR_NOM1 TV1 NP * ]\n",
      "  R [ PR_NOM1 VP1 * ]\n",
      "  R [ S1 * ]\n",
      "  R [ S * ]\n",
      "(S\n",
      "  (S1\n",
      "    (PR_NOM1 (NP John) (CONJ and) (NP Mary))\n",
      "    (VP1 (TV1 eat) (NP bread))))\n",
      "---------\n",
      "Parsing 'John and Mary eat bread with cheese'\n",
      "    [ * John and Mary eat bread with cheese]\n",
      "  S [ 'John' * and Mary eat bread with cheese]\n",
      "  R [ NP * and Mary eat bread with cheese]\n",
      "  S [ NP 'and' * Mary eat bread with cheese]\n",
      "  R [ NP CONJ * Mary eat bread with cheese]\n",
      "  S [ NP CONJ 'Mary' * eat bread with cheese]\n",
      "  R [ NP CONJ NP * eat bread with cheese]\n",
      "  R [ PR_NOM1 * eat bread with cheese]\n",
      "  S [ PR_NOM1 'eat' * bread with cheese]\n",
      "  R [ PR_NOM1 TV1 * bread with cheese]\n",
      "  S [ PR_NOM1 TV1 'bread' * with cheese]\n",
      "  R [ PR_NOM1 TV1 NP * with cheese]\n",
      "  R [ PR_NOM1 VP1 * with cheese]\n",
      "  R [ S1 * with cheese]\n",
      "  R [ S * with cheese]\n",
      "  S [ S 'with' * cheese]\n",
      "  R [ S PREP * cheese]\n",
      "  S [ S PREP 'cheese' * ]\n",
      "  R [ S PREP NP * ]\n",
      "  R [ S PREP_NP * ]\n",
      "  R [ S * ]\n",
      "(S\n",
      "  (S\n",
      "    (S1\n",
      "      (PR_NOM1 (NP John) (CONJ and) (NP Mary))\n",
      "      (VP1 (TV1 eat) (NP bread))))\n",
      "  (PREP_NP (PREP with) (NP cheese)))\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "sg = \"\"\"\n",
    "S -> S1 | S2 | S DET AD_NP | S PREP_NP | S PREP DET_NP | S CONJ_NP\n",
    "S1 -> PR_NOM1 VP1\n",
    "VP1 -> TV1 NP | TV1 PR_ACC\n",
    "TV1 -> 'love' | 'eat'\n",
    "PR_NOM1 -> 'They' | NP CONJ NP\n",
    "\n",
    "S2 -> DET NP VP2 | NP VP2 | PR_NOM2 VP2 | \n",
    "VP2 -> IV2 | TV2 NP | TV2 PR_ACC | TV2 DET_NP\n",
    "TV2 -> 'eats' | 'loves' | 'gave' | 'saw' | 'knows'\n",
    "PR_NOM2 -> 'She' | 'Everybody'\n",
    "IV2 -> 'left'\n",
    "\n",
    "NP -> 'John' | 'bread' | 'Mary' | 'boy' | 'book' | 'man' | 'telescope' | 'hill' | 'men' | 'women' | 'children' | 'cheese'\n",
    "AD_NP -> ADJ NP\n",
    "PREP_NP -> PREP NP\n",
    "DET_NP -> DET NP\n",
    "CONJ_NP -> CONJ NP\n",
    "PR_ACC -> 'them' | 'her' | 'it'\n",
    "PREP -> \"to\" | 'with' | 'on'\n",
    "DET -> 'A' | 'a' | 'the'\n",
    "CONJ -> 'and' | ','\n",
    "ADJ -> 'heavy'\n",
    "\"\"\"\n",
    "\n",
    "g = CFG.fromstring(sg)\n",
    "\n",
    "# Bottom-up  parser\n",
    "sr_parser = nltk.ShiftReduceParser(g, trace=2)\n",
    "\n",
    "# Parse sentences and observe the behavior of the parser\n",
    "def parse_sentence(sent):\n",
    "    tokens = re.findall(r\"[\\w']+|[.,!?;]\", sent)\n",
    "    trees = sr_parser.parse(tokens)\n",
    "    for tree in trees:\n",
    "        print(tree)\n",
    "\n",
    "sentences = \"\"\"John saw a man with a telescope\n",
    "John saw a man on the hill with a telescope\n",
    "Mary knows men and women\n",
    "Mary knows men, children and women\n",
    "John and Mary eat bread\n",
    "John and Mary eat bread with cheese\"\"\"\n",
    "\n",
    "\n",
    "sentences = sentences.split(\"\\n\")\n",
    "\n",
    "for i in range(6):\n",
    "    parse_sentence(sentences[i])\n",
    "    print (\"---------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few specific notes about implementation:\n",
    "\n",
    "-We've added the category of conjuction, such as 'and'.\n",
    "-The 4th sentence includes a comma, which also has a grammatical role. However, the split function previously used does not split punctuation (it created the word 'men,' rather than 'men' and ','), so we've changed the split method to a method that will split the coma (using regular expressions). The comma is treated as a conjuction word, similar to 'and' - again, the endless recursion helps us to add two suffixes (\", children\" \"and women\") on top of \"Mary knows\".\n",
    "-The expression \"John and Mary\" is equivalent to \"they\", so it's treated as a nominative pronoun.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the given examples and the number of coordination:\n",
    "\"John and Mary\" - 2 people\n",
    "\"John or Mary\" - 1 people\n",
    "\"John or the children\" - either one or an unspecified (bigger than 1) number of people.\n",
    "\n",
    "The first example was already dealt with in the last question - by treating \"NP and NP\" as equivalent to \"They\".\n",
    "The case of 'or' is a bit more complicated, because it can refer to either singular or plural, as the last two examples show. One of the weaknesses of our grammar (as we demonstrate in the next question), is not separating singluar and plural nouns. So a possible solution to deal with the problem is to separate the category NP into NP_PL and NP_SIN (plural and singular). After we split it, we can safely treat \"NP_PL or NP_PL\" as plural (similar to \"they\"), and \"NP_SIN or NP_SIN\" as singluar (similar to he/she). \n",
    "\n",
    "The mixed case, \"NP_PL or NP_SIN\" is still ambiguous - but this is not a problem with our grammar, but with the English language. When we say 'John or the children', we don't know whether we're talking about singluar or plural. The correct grammar in those cases is the singluar grammar - \"John or the children love him\" (rather than \"loves\"), so we can treat the mix case as similar to 'They'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither of the issues we pointed out on the previous question are now fixed. We're still prone to illogical nouns use (\"telescope eats a children with boy on a John\" parses ok) and still prone to endlessley stacking sentences on top of each other. \n",
    "Some of our new added NPs (\"women\", \"men\") are plural - meaning swapping them with a singluar NP is problematic. For example, 'Mary knows women' makes sense, but \"Mary knows hill\" is gramatically wrong - we would have needed a determiner (\"Mary knows **a** hill\") to create a correct sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 'Mary knows hill'\n",
      "    [ * Mary knows hill]\n",
      "  S [ 'Mary' * knows hill]\n",
      "  R [ NP * knows hill]\n",
      "  S [ NP 'knows' * hill]\n",
      "  R [ NP TV2 * hill]\n",
      "  S [ NP TV2 'hill' * ]\n",
      "  R [ NP TV2 NP * ]\n",
      "  R [ NP VP2 * ]\n",
      "  R [ S2 * ]\n",
      "  R [ S * ]\n",
      "(S (S2 (NP Mary) (VP2 (TV2 knows) (NP hill))))\n"
     ]
    }
   ],
   "source": [
    "parse_sentence (\"Mary knows hill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new problem is introduced with conjuction - our conjuction role treats the comma sign and the word 'and' in the same way. Which means we can create a list with any combination of those. Of course, English grammar demands each item on the list to be comma separated, while \"and\" only appears before the last item (\"a, b, c and d\"). This example breaks both roles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 'Mary knows men and children and book and telescope and hill , boy , John'\n",
      "    [ * Mary knows men and children and book and telescope and hill , boy , John]\n",
      "  S [ 'Mary' * knows men and children and book and telescope and hill , boy , John]\n",
      "  R [ NP * knows men and children and book and telescope and hill , boy , John]\n",
      "  S [ NP 'knows' * men and children and book and telescope and hill , boy , John]\n",
      "  R [ NP TV2 * men and children and book and telescope and hill , boy , John]\n",
      "  S [ NP TV2 'men' * and children and book and telescope and hill , boy , John]\n",
      "  R [ NP TV2 NP * and children and book and telescope and hill , boy , John]\n",
      "  R [ NP VP2 * and children and book and telescope and hill , boy , John]\n",
      "  R [ S2 * and children and book and telescope and hill , boy , John]\n",
      "  R [ S * and children and book and telescope and hill , boy , John]\n",
      "  S [ S 'and' * children and book and telescope and hill , boy , John]\n",
      "  R [ S CONJ * children and book and telescope and hill , boy , John]\n",
      "  S [ S CONJ 'children' * and book and telescope and hill , boy , John]\n",
      "  R [ S CONJ NP * and book and telescope and hill , boy , John]\n",
      "  R [ S CONJ_NP * and book and telescope and hill , boy , John]\n",
      "  R [ S * and book and telescope and hill , boy , John]\n",
      "  S [ S 'and' * book and telescope and hill , boy , John]\n",
      "  R [ S CONJ * book and telescope and hill , boy , John]\n",
      "  S [ S CONJ 'book' * and telescope and hill , boy , John]\n",
      "  R [ S CONJ NP * and telescope and hill , boy , John]\n",
      "  R [ S CONJ_NP * and telescope and hill , boy , John]\n",
      "  R [ S * and telescope and hill , boy , John]\n",
      "  S [ S 'and' * telescope and hill , boy , John]\n",
      "  R [ S CONJ * telescope and hill , boy , John]\n",
      "  S [ S CONJ 'telescope' * and hill , boy , John]\n",
      "  R [ S CONJ NP * and hill , boy , John]\n",
      "  R [ S CONJ_NP * and hill , boy , John]\n",
      "  R [ S * and hill , boy , John]\n",
      "  S [ S 'and' * hill , boy , John]\n",
      "  R [ S CONJ * hill , boy , John]\n",
      "  S [ S CONJ 'hill' * , boy , John]\n",
      "  R [ S CONJ NP * , boy , John]\n",
      "  R [ S CONJ_NP * , boy , John]\n",
      "  R [ S * , boy , John]\n",
      "  S [ S ',' * boy , John]\n",
      "  R [ S CONJ * boy , John]\n",
      "  S [ S CONJ 'boy' * , John]\n",
      "  R [ S CONJ NP * , John]\n",
      "  R [ S CONJ_NP * , John]\n",
      "  R [ S * , John]\n",
      "  S [ S ',' * John]\n",
      "  R [ S CONJ * John]\n",
      "  S [ S CONJ 'John' * ]\n",
      "  R [ S CONJ NP * ]\n",
      "  R [ S CONJ_NP * ]\n",
      "  R [ S * ]\n",
      "(S\n",
      "  (S\n",
      "    (S\n",
      "      (S\n",
      "        (S\n",
      "          (S\n",
      "            (S (S2 (NP Mary) (VP2 (TV2 knows) (NP men))))\n",
      "            (CONJ_NP (CONJ and) (NP children)))\n",
      "          (CONJ_NP (CONJ and) (NP book)))\n",
      "        (CONJ_NP (CONJ and) (NP telescope)))\n",
      "      (CONJ_NP (CONJ and) (NP hill)))\n",
      "    (CONJ_NP (CONJ ,) (NP boy)))\n",
      "  (CONJ_NP (CONJ ,) (NP John)))\n"
     ]
    }
   ],
   "source": [
    "parse_sentence (\"Mary knows men and children and book and telescope and hill, boy, John\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has a model called generate which is able to generate sentences given a CFG grammer. \n",
    "Our goal in this part is to create a generator for a PCFG instead of a CFG.\n",
    "\n",
    "So lets start by writing our generator function, which, given a PCFG, return a tree representing the generation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generates a tree from a PCFG grammer \n",
    "def pcfg_generate(grammar):\n",
    "    start = grammar.start()\n",
    "    return(pcfg_generate_root(grammar, start))\n",
    "    \n",
    "# generates a tree from a given root based on the PCFG grammer    \n",
    "def pcfg_generate_root(grammar, root):\n",
    "    #if it's not a terminal it means we have to generate from the probabilities\n",
    "    if isinstance(root, Nonterminal):\n",
    "        item_productions = grammar.productions(lhs=root)\n",
    "        dict = {}\n",
    "        for pr in item_productions: dict[pr.rhs()] = pr.prob()\n",
    "        item_probDist = DictionaryProbDist(dict)\n",
    "        prod = item_probDist.generate()\n",
    "        if (len(prod) == 2):\n",
    "            lh = prod[0]\n",
    "            rh = prod[1]\n",
    "            lh_tree = pcfg_generate_root(grammar, lh)\n",
    "            rh_tree = pcfg_generate_root(grammar, rh)\n",
    "            return Tree(root, [lh_tree, rh_tree])\n",
    "        else:\n",
    "            lh = prod[0]\n",
    "            return Tree(root,[pcfg_generate_root(grammar, lh)])\n",
    "           \n",
    "    #if it's a terminal we can just return it\n",
    "    else:\n",
    "        return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test our function with the *toy_pcfg2* grammer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         S      \n",
      "      ___|____   \n",
      "     NP       VP\n",
      "  ___|___     |  \n",
      "Det      N    V \n",
      " |       |    |  \n",
      " a      hill ate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_tree = pcfg_generate(toy_pcfg2)\n",
    "Tree.fromstring(example_tree.pformat()).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, not all the resulting sentences makes sense... This might be due to the fact the PCFG is based on distribution, and therefore it will always lean towards certain phrases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now continue on to do some validations in the next subsections - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we are going to do is to generate 1000 random trees given the *toy_pcfg2* grammer, and save all resulting trees into a file called \"toy_pcfg2.gen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_trees_file(n=1000):\n",
    "    for i in range(n):\n",
    "        current_tree = pcfg_generate(toy_pcfg2)\n",
    "        with open(\"toy_pcfg2.gen\", \"a+\") as f:\n",
    "            current_tree.pprint(stream=f)\n",
    "            f.write(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_trees_file(n=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now once we created such file we can use it to conduct some experiments on the resulting trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is to read our trees from our file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read_file = open(\"toy_pcfg2.gen\", \"r\")\n",
    "buffer = \"\"\n",
    "for line in read_file :\n",
    "    buffer += line\n",
    "trees = buffer.split(\"*\")\n",
    "trees = trees[:len(trees) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our trees, we would like to compute the frequency distribution of each non-terminal and pre-terminal in the generated corpus - to do so the first thing we're going to need is the tree's productions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toy_sample_productions = []\n",
    "for tree in trees:\n",
    "    toy_sample_productions += Tree.fromstring(tree).productions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have those we can calculate the distribution of each non-terminal and pre-terminal, because it's the same as the productions distributions.\n",
    "\n",
    "When using NLTK we found there are two ways of doing so, one is using native functions in the NLTK library, and the other is to implement it by ourselves. We will demonstrate both and use that fact to evaluate our own function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way, using NLTK native function, is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One way of achieving our goal\n",
    "pcfg_induced = induce_pcfg(Nonterminal('S'), toy_sample_productions).productions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second way is to use ConditionalFreqDist for every left-hand and right-hand side of the production, and then create a ConditionalProbDist using MLE (like we did in the previous assignemnts), this will also result in the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_productions_probabilities(productions):\n",
    "    #And another way\n",
    "    cfd = nltk.ConditionalFreqDist((production.lhs(), production.rhs()) for production in productions)\n",
    "    cpd_mle = nltk.ConditionalProbDist(cfd, nltk.MLEProbDist)\n",
    "    return cpd_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now if we compare the distributions from both methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toy_sample_probability = calc_productions_probabilities(toy_sample_productions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Method:\n",
      "S -> NP VP [1.0]\n",
      "NP -> NP PP [0.306984]\n",
      "NP -> Name [0.274248]\n",
      "Name -> 'Bob' [0.458886]\n",
      "PP -> P NP [1.0]\n",
      "P -> 'under' [0.397013]\n",
      "NP -> Det N [0.418768]\n",
      "Det -> 'the' [0.40938]\n",
      "N -> 'hill' [0.49971]\n",
      "VP -> V NP [0.582505]\n",
      "V -> 'ran' [0.275]\n",
      "Det -> 'a' [0.313839]\n",
      "N -> 'table' [0.115808]\n",
      "P -> 'with' [0.602987]\n",
      "Name -> 'Jack' [0.541114]\n",
      "Det -> 'my' [0.276781]\n",
      "VP -> V [0.411531]\n",
      "V -> 'ate' [0.509]\n",
      "N -> 'boy' [0.112913]\n",
      "N -> 'telescope' [0.143023]\n",
      "N -> 'cookie' [0.128547]\n",
      "V -> 'saw' [0.216]\n",
      "VP -> VP PP [0.00596421]\n",
      "\n",
      "Second Method:\n",
      "S -> (NP, VP) 1.0\n",
      "NP -> (NP, PP) 0.3069835111542192\n",
      "NP -> (Name,) 0.27424830261881666\n",
      "NP -> (Det, N) 0.4187681862269641\n",
      "Name -> ('Bob',) 0.4588859416445623\n",
      "Name -> ('Jack',) 0.5411140583554377\n",
      "PP -> (P, NP) 1.0\n",
      "P -> ('under',) 0.3970125786163522\n",
      "P -> ('with',) 0.6029874213836478\n",
      "Det -> ('the',) 0.40938042848870876\n",
      "Det -> ('a',) 0.3138390272148234\n",
      "Det -> ('my',) 0.27678054429646787\n",
      "N -> ('hill',) 0.49971048060220036\n",
      "N -> ('table',) 0.11580775911986103\n",
      "N -> ('boy',) 0.1129125651418645\n",
      "N -> ('telescope',) 0.14302258251302838\n",
      "N -> ('cookie',) 0.12854661262304573\n",
      "VP -> (V, NP) 0.5825049701789264\n",
      "VP -> (V,) 0.4115308151093439\n",
      "VP -> (VP, PP) 0.005964214711729622\n",
      "V -> ('ran',) 0.275\n",
      "V -> ('ate',) 0.509\n",
      "V -> ('saw',) 0.216\n"
     ]
    }
   ],
   "source": [
    "print(\"First Method:\")\n",
    "for nt in pcfg_induced:\n",
    "    print(nt)\n",
    "print(\"\\nSecond Method:\")\n",
    "for cond in toy_sample_probability:\n",
    "    for sample in toy_sample_probability[cond].samples():\n",
    "        print(\"{} -> {} {}\".format(cond, sample, toy_sample_probability[cond].prob(sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as we can see the results are the same (give or take rounding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we calculated the distribution for our toy sample, we can calculate them for the entire **toy_pcfg2** grammar, we will use the results in the next section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_pcfg_probability = calc_productions_probabilities(toy_pcfg2.productions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we would like to calculate next is a measure called **KL Divergence**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_kl_divergence(p, q, eps=0.0001):\n",
    "    sp = list(p.samples())\n",
    "    cp = len(sp)\n",
    "    sq = list(q.samples())\n",
    "    cq = len(sq)\n",
    "    su = set(sp + sq)\n",
    "    cu = len(su)\n",
    "    pc = eps * ((cq) / float(cp))\n",
    "    qc = eps * ((cp) / float(cq))\n",
    "    kl = 0.0\n",
    "    for i in su:\n",
    "        if i in sp:\n",
    "             p_i = p.prob(i) - pc\n",
    "        else:\n",
    "            p_i = eps\n",
    "        if i in sq:\n",
    "            q_i = q.prob(i) - qc\n",
    "        else:\n",
    "            q_i = eps\n",
    "            \n",
    "        kl +=  p_i * log( p_i / float(q_i))\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{S: 0.0, NP: 0.016769055318143367, Name: 0.0033852302910155826, PP: 0.0, P: 0.021369753421888488, Det: 0.013761199547802194, N: 0.22508191881226158, VP: 0.3882076278950367, V: 0.06886553732824867}\n"
     ]
    }
   ],
   "source": [
    "divergence = {}\n",
    "for condition in toy_sample_probability.conditions():\n",
    "    kl = compute_kl_divergence(toy_sample_probability[condition], toy_pcfg_probability[condition])\n",
    "    divergence[condition] = kl\n",
    "print(divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've just seen, the results are similar on some of the tags (for example S and PP), and divert on others. \n",
    "The similar rules (with 0 diversion) are similar due to the fact that in the original toy grammar, this rules have only one derivation rule and it is derived with a 1.0 probabilty, and so our samples will all use that rule, the diversion happens once we look at rules with more \"options\", then in our samples such rules won't exist at all (because our sampler is based on the probabilities and is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simplify_functional_tag(tag):\n",
    "    if '-' in tag:\n",
    "        tag = tag.split('-')[0]\n",
    "    # -NONE- tag\n",
    "    if \"\" == tag:\n",
    "        return \" \"\n",
    "    return tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tag(tree):\n",
    "    if isinstance(tree, Tree):\n",
    "        simplified_tag = simplify_functional_tag(tree.label())\n",
    "        # -NONE- tag\n",
    "        if simplified_tag == \" \":\n",
    "            return \" \"\n",
    "        return Nonterminal(simplify_functional_tag(tree.label()))\n",
    "    else:\n",
    "        return tree\n",
    "\n",
    "def tree_to_production(tree):\n",
    "    # if the root is NONE we don't need to parse it more\n",
    "    if(\" \" == get_tag(tree)):\n",
    "        return\n",
    "    else:\n",
    "        return Production(get_tag(tree), [get_tag(child) for child in tree])\n",
    "\n",
    "def tree_to_productions(tree):\n",
    "    yield tree_to_production(tree)\n",
    "    for child in tree:\n",
    "        if isinstance(child, Tree):\n",
    "            for prod in tree_to_productions(child):\n",
    "                if prod:\n",
    "                    yield prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pcfg_learn(treebank, n):\n",
    "    trees = treebank.parsed_sents()[:n]\n",
    "    trees_productions = []\n",
    "    for tree in trees:\n",
    "        productions = tree_to_productions(tree)\n",
    "        trees_productions += [prod for prod in productions]\n",
    "    pcfg_induced = induce_pcfg(Nonterminal('S'), trees_productions)\n",
    "    return pcfg_induced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "treebank = LazyCorpusLoader('treebank/combined', BracketParseCorpusReader, r'wsj_.*\\.mrg')\n",
    "two_hundred_trees_pcfg = pcfg_learn(treebank, 200)\n",
    "four_hunred_trees_pcfg = pcfg_learn(treebank, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO - answer part 1 questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing what we did in the previous section , we don't want to build a pcfg now, just to watch the frequencies, so what we will do is build a freqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pcfg_freq(treebank, n):\n",
    "    trees = treebank.parsed_sents()[:n]\n",
    "    trees_productions = []\n",
    "    for tree in trees:\n",
    "        productions = tree_to_productions(tree)\n",
    "        trees_productions += [prod for prod in productions]\n",
    "    pcfg_freq = Counter(trees_productions)\n",
    "    return pcfg_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "two_hundred_trees_freq = pcfg_freq(treebank, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAGPCAYAAACH93DDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8v8AjyTj6JWhgp8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb86c988cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_count = two_hundred_trees_freq.most_common()\n",
    "_, counts = zip(*two_hundred_trees_freq.items())\n",
    "counts = (list(counts))\n",
    "counts.sort()\n",
    "\n",
    "labels, values = zip(*Counter(counts).items())\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "\n",
    "plt.figure(num=3, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.bar(indexes, values, 1.5)\n",
    "plt.xticks(indexes, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_functional_tag(tag):\n",
    "    if '-' in tag:\n",
    "        tag = tag.split('-')[0]\n",
    "    # -NONE- tag\n",
    "    if \"\" == tag:\n",
    "        return \" \"\n",
    "    return tag\n",
    "\n",
    "def get_tag_clean(tree):\n",
    "    if isinstance(tree, Tree):\n",
    "        simplified_tag = simplify_functional_tag(tree.label())\n",
    "        return simplified_tag\n",
    "\n",
    "def build_simplified_tree(tree):\n",
    "    if isinstance(tree, Tree):\n",
    "        root_tag = get_tag_clean(tree)\n",
    "        if(\" \" == root_tag):\n",
    "            return \" \"\n",
    "        else:\n",
    "            simplified_child_nodes = [build_simplified_tree(child) for child in tree]\n",
    "            # remove all NONE children\n",
    "            simplified_children = [child for child in  simplified_child_nodes if child != \" \"]\n",
    "            if(len(simplified_children) >= 1):\n",
    "                return(Tree(root_tag, simplified_children))\n",
    "            else:\n",
    "                # This means the only child is NONE so we can remove it from it's parent as well\n",
    "                return \" \"\n",
    "    else:\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tag_or_value(tree):\n",
    "    if isinstance(tree, Tree):\n",
    "        return Nonterminal(tree.label())\n",
    "    else:\n",
    "        tree\n",
    "        \n",
    "def tree_to_production(tree):\n",
    "    return Production(get_tag_or_value(tree),[get_tag_or_value(child) for child in tree])\n",
    "\n",
    "def tree_to_productions(tree):\n",
    "    yield tree_to_production(tree)\n",
    "    for child in tree:\n",
    "        if isinstance(child, Tree):\n",
    "            for prod in tree_to_productions(child):\n",
    "                if prod:\n",
    "                    yield prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pcfg_cnf_learn(treebank, n):\n",
    "    trees = treebank.parsed_sents()[:n]\n",
    "    trees_productions = []\n",
    "    for tree in trees:\n",
    "        new_tree = build_simplified_tree(tree)\n",
    "        chomsky_normal_form(tree, factor='right', horzMarkov=1, vertMarkov=1, childChar='|', parentChar='^')\n",
    "        chomsky_normal_form(new_tree, factor='right', horzMarkov=1, vertMarkov=1, childChar='|', parentChar='^')\n",
    "        productions = tree_to_productions(new_tree)\n",
    "        trees_productions += [prod for prod in productions]\n",
    "    pcfg_induced = induce_pcfg(Nonterminal('S'), trees_productions)\n",
    "    return pcfg_induced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank = LazyCorpusLoader('treebank/combined', BracketParseCorpusReader, r'wsj_.*\\.mrg')\n",
    "pcfg_cnf = two_hundred_trees_pcfg = pcfg_cnf_learn(treebank, 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
