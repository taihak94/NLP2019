{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I'd like to start with importing all the modules i'm going to use in this assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re, pprint, collections\n",
    "from urllib import request\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "from os.path import abspath, dirname, join\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from math import log\n",
    "import random\n",
    "from collections import *\n",
    "import zipfile\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following code in order to tokenize the data based on the ptb method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_special_char(x):\n",
    "    return ((x=='N') or ('\\n' in x))\n",
    "\n",
    "def replace_numbers(tokens):\n",
    "    return [x if not x.isdigit() else 'N' for x in tokens]\n",
    "\n",
    "def find_most_common(tokens, top):\n",
    "    r = re.compile(\"^[a-zA-Z]+$\")\n",
    "    word_tokens = list(filter(r.match, tokens))\n",
    "    counter = collections.Counter(word_tokens)\n",
    "    most_common = counter.most_common(top)\n",
    "    return [a for a, b in most_common]\n",
    "\n",
    "def replace_noncommon_tokens(tokens, most_common):\n",
    "    return [x if ((x in most_common) or (is_special_char(x))) else '<unk>' for x in tokens]\n",
    "\n",
    "def ptb_preprocess(filenames, top=10000):\n",
    "    for single_file in filenames:\n",
    "    path = nltk.data.find(single_file)\n",
    "    raw = open(path, 'r').read()\n",
    "    \n",
    "    # clean punctuations\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$|\\#|\\@|\\%|\\&|\\*|\\^|\\~|\\<|\\>|\\=|\\+|\\n')\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # to lowercase\n",
    "    words = [w.lower() for w in tokens]\n",
    "\n",
    "    # filter numbers\n",
    "    words_no_numbers = replace_numbers(words)\n",
    "\n",
    "    # get most common words and replace all other words with unk\n",
    "    common_tokens = find_most_common(words, top)\n",
    "    words = replace_noncommon_tokens(words_no_numbers, common_tokens)\n",
    "\n",
    "    # write out the new data into a file \n",
    "    new_filename = single_file + \".out\"\n",
    "    with open(new_filename, 'w') as f:\n",
    "        for word in words:\n",
    "            if ('\\n' in word):\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "                f.write(\"%s \" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to test the above code on a few example files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_first_file = abspath(join(dirname(\"__file__\"), \"segmentationExample.txt\"))\n",
    "path_to_second_file = abspath(join(dirname(\"__file__\"), \"SplitAndPuncExample.txt\"))\n",
    "path_to_third_file = abspath(join(dirname(\"__file__\"), \"numbersExample.txt\"))\n",
    "\n",
    "ptb_preprocess([path_to_first_file, path_to_second_file, path_to_third_file], 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example the file \"segmentationExample.text\" which contains the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_first_file) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will result in the file \"segmentationExample.text.out\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_first_result = abspath(join(dirname(\"__file__\"), \"segmentationExample.txt.out\"))\n",
    "with open(path_to_first_result) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file \"SplitAndPuncExample.txt\" :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_second_file) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will result in the files \"SplitAndPuncExample.txt.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_second_result = abspath(join(dirname(\"__file__\"), \"SplitAndPuncExample.txt.out\"))\n",
    "with open(path_to_second_result) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file \"numbersExample.txt\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_third_file) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will result in the files \"numbersExample.txt.out\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_third_result = abspath(join(dirname(\"__file__\"), \"numbersExample.txt.out\"))\n",
    "with open(path_to_third_result) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### part 1.1.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will plot a graph of the results of the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following is an implementation of the ngram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramModel(object):\n",
    "    def __init__(self, n, train, estimator=None):\n",
    "        self._n = n\n",
    "        self.is_unigram_model = (n == 1)\n",
    "        cfd = nltk.ConditionalFreqDist((\" \".join(train[i : i + n - 1]), \"\".join(train[i + n - 1])) for i in range(len(train) - n + 1))\n",
    "        self._probdist = nltk.ConditionalProbDist(cfd, estimator)\n",
    "        \n",
    "        self._ngramsData = ngrams(train, n)\n",
    "        self._ngrams = set()\n",
    "        for ngram in self._ngramsData:\n",
    "            self._ngrams.add(ngram)\n",
    "        \n",
    "        if not self.is_unigram_model:\n",
    "            self._backoff = NgramModel(n - 1, train, estimator)\n",
    "            self._lambda = 1\n",
    "    \n",
    "    def prob(self, word, context):\n",
    "        if (tuple(context.split()) + (word, ) in self._ngrams) or (self.is_unigram_model):\n",
    "            return self._probdist[context].prob(word)\n",
    "        else:\n",
    "            new_context = \" \".join(context.split()[1:])\n",
    "            backoff = self._backoff.prob(word, new_context)\n",
    "            return self._lambda * backoff\n",
    "        \n",
    "    def logprob(self, word, context):\n",
    "        return -(log(self.prob(word, context), 2))\n",
    "    \n",
    "    def get_seed(self):\n",
    "        return random.choice(self._probdist.conditions())\n",
    "    \n",
    "    def generate(self, seed, length):\n",
    "#         seed = seed.split()\n",
    "#         res = list(seed)\n",
    "#         for i in range(length):\n",
    "#             res.append(self.generate_one(res))\n",
    "#         return res\n",
    "        out = []\n",
    "        curr = seed\n",
    "        end = self._probdist.conditions()[-1]\n",
    "        i = 0\n",
    "        while (i <= length and (not curr == end)):\n",
    "          i += 1\n",
    "          word = self._probdist[curr].generate()\n",
    "          curr = \" \".join((curr.split())[1:] + [word])\n",
    "          out.append(word)\n",
    "        return out\n",
    "    \n",
    "#     def generate_one(self, context):\n",
    "#         keys = set(self._probdist.conditions())\n",
    "#         if \" \".join(context) in keys:\n",
    "#             return self._probdist[context].generate()\n",
    "#         elif self._n > 1:\n",
    "#             return self._backoff.generate_one(context[:1])\n",
    "#         else:\n",
    "#             return \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_word_lm(dataset, n=2):\n",
    "    model = NgramModel(n, dataset, estimator=nltk.MLEProbDist)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the data structure used to build the model is a dictionary of size at most , but one can note we also had to save the backoff models - that is in order to do smoothing and backoff over the perplexity calculations. \n",
    "\n",
    "As we can see in the given class implementation - the model should export methods for evaluating itself, for generating random text, and for calculating the probabilty and entropy of a word given a context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to measure how well our model is doing, we can do so by using a measure called perplexity - a model perplexity can be evaluated as followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_entropy(model, text, n=2):\n",
    "    H = 0.0\n",
    "    for i in range(n - 1, len(text)):\n",
    "        context, word = tuple(text[i - n + 1:i]), text[i]\n",
    "        context = \" \".join(context)\n",
    "        H += model.logprob(word, context)\n",
    "    return H / float(len(text) - (n - 1))\n",
    "\n",
    "def calc_preplexity(model, text, n=2):\n",
    "    text_entropy = model_entropy(model, text, n)\n",
    "    return 2 ** (text_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When testing the above implemented model on the ptb training and validation data we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptb_train_path = abspath(join(dirname(\"__file__\"), \"ptb.train.txt\"))\n",
    "ptb_test_path = abspath(join(dirname(\"__file__\"), \"ptb.test.txt\"))\n",
    "ptb_train_tokenized = (open(ptb_train_path, 'r').read()).split()\n",
    "ptb_test_tokenized = (open(ptb_test_path, 'r').read()).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the ngram model withn = 4\n",
    "n = 3\n",
    "lm_MLE = train_word_lm(ptb_train_tokenized, n)\n",
    "\n",
    "print(calc_preplexity(lm_MLE, ptb_test_tokenized, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, we can always use a different estimator in order to change our model perplexity, for example, the following model is using the Lidstone estimator with a gamma instead of the MLE one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_word_lm_lidstone(dataset, n=2, gamma=0.01):\n",
    "    lidstone_estimator = lambda fd: nltk.LidstoneProbDist(fd, gamma, fd.B() + 1)\n",
    "    model = NgramModel(n, dataset, estimator=lidstone_estimator)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get the following perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "gamma = 0.01\n",
    "lm_LIDSTONE = train_word_lm_lidstone(ptb_train_tokenized, n, gamma)\n",
    "print(calc_preplexity(lm_LIDSTONE, ptb_test_tokenized, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following graph shows how differnt gamma values in such model change the results of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = np.linspace(0, 1, 20)\n",
    "n = 3\n",
    "perplexities_l = list(range(20))\n",
    "i = 0\n",
    "for gamma in gammas:\n",
    "    lm_LIDSTONE = train_word_lm_lidstone(ptb_train_tokenized, n, gamma)\n",
    "    perplexities_l[i] = calc_preplexity(lm_LIDSTONE, ptb_test_tokenized, n)\n",
    "    print(perplexities_l[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gammas, perplexities_l)\n",
    "plt.axis([0, 1, 100, 120])\n",
    "plt.ylabel(\"perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another way is to change the value of the n-grams , and that results in different perplexity - an example of the difference can be seen in the following graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities_m = list(range(2, 20))\n",
    "gamma = 0.01\n",
    "\n",
    "for n in range(2, 20):\n",
    "    lm_LIDSTONE = train_word_lm_lidstone(ptb_train_tokenized, n, gamma)    \n",
    "    perp = calc_preplexity(lm_LIDSTONE, ptb_test_tokenized, n)\n",
    "    print(perp)\n",
    "    perplexities_m[n - 2] = perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vals = list(range(2, 20))\n",
    "\n",
    "plt.plot(n_vals, perplexities_m)\n",
    "plt.axis([2, 20, 1, 20])\n",
    "plt.ylabel(\"perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see starting with a low value of n we get really high perplexity(bad) and as we increase the value the perplexity gets better. One can also note that at around n=7 the perplexity is 1, that case isn't perfect because it means the probabilities are always 0. \n",
    "So the ideal size for n is at about n=6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the two models above and the two graphs, we can now compose the \"ideal\" model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107.40517847274688\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "gamma = 0.01\n",
    "lm_IDEAL = train_word_lm_lidstone(ptb_train_tokenized, n, gamma)\n",
    "print(calc_preplexity(lm_IDEAL, ptb_test_tokenized, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compared to the results from <TODO> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 1.3.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of using the model is by generating text using it, the following method generates text given a model and a seed(a starting prefix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(model, seed):\n",
    "    out = model.generate(seed, 100)\n",
    "    out = seed + \" \" + \" \".join(out)\n",
    "\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can note this method is not ideal if the seed length is smaller than the ngram size we used to train the model, a way to avoid such problem is to find a matching ngram starting with the seed and running the method from there (did not implemnet it) or just making sure the seed is big enough. Another way is the issue of halting. if the model run into an unknown history this code will break, so one must make sure no unknown history will occure - to do so we have the condition in the while loop. I made this generator limited to 100 iterations if possible because I didn't want to make it generate to big of a file given a large model, but one can easily change that number to anything.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few different examples of using the previously trained model with different seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: granted a\n",
      "granted a major crop in europe that gives futures traders said the tendered units will continue making its <unk> support conservatives should take a top post at american <unk> owns five stations including several cbs inc. and oakes <unk> & <unk> a fashion magazine <unk> publications inc. owns random house four years running i hate to lose ground against other major japanese retail stores primarily in discount <unk> everything 's a lot more done if he agrees mr. kemp broke the cease-fire if the <unk> baskets too i care very much said mr. mcalpine could be one reason i began looking at just\n"
     ]
    }
   ],
   "source": [
    "seed = lm_IDEAL.get_seed()\n",
    "print(\"SEED:\", seed)\n",
    "generate(lm_IDEAL, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: calls the\n",
      "calls the <unk> funds currently offer yields almost N million offer mr. <unk> denied that the court now he says is as important as his own projections contributed to volatility he says i tried the nagging memory of one of england was a light N million of the area but has left holders of telerate inc. common stock if it uses its own <unk> harris <unk> and trust departments but <unk> are unconstitutional president bush have suggested ways of washington is as usual i 'm not sure what will come from independent producers which unlike utilities are a guide to general levels but\n"
     ]
    }
   ],
   "source": [
    "seed = lm_IDEAL.get_seed()\n",
    "print(\"SEED:\", seed)\n",
    "generate(lm_IDEAL, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: play mr.\n",
      "play mr. short 's ninth move a key member of the same time spending on the big board thursday under <unk> from about N passengers would expand a president would have veto rights to the profitable u.s. market analysts said the visit was unrelated to jaguar gm chairman roger stone said that the committee chairman henry gonzalez d. texas the lead underwriter to the farmer can say world-wide name said <unk> robert norton some fruit <unk> and <unk> <unk> drugs posed to the <unk> <unk> and <unk> <unk> has made a $ N million common shares the company faces significant challenges in some\n"
     ]
    }
   ],
   "source": [
    "seed = lm_IDEAL.get_seed()\n",
    "print(\"SEED:\", seed)\n",
    "generate(lm_IDEAL, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: smoking patterns\n",
      "smoking patterns a higher return and was unable to fend off further advances and growth allows users to believe that the latest period twice as many as one million deaths the vaccine its effectiveness before considering mandatory pro bono work we do n't think that washington 's decisions do not recoup from the spring <unk> using the new holding company the battery plant which makes hair and skin care concern at continental have n't seen the profit <unk> u s west 's u s west 's leaders in the region fighting illegal drugs in the third quarter time on the sale of the\n"
     ]
    }
   ],
   "source": [
    "seed = lm_IDEAL.get_seed()\n",
    "print(\"SEED:\", seed)\n",
    "generate(lm_IDEAL, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = random.choice(lm_IDEAL.conditions())\n",
    "print(\"SEED:\", seed)\n",
    "generate(lm_IDEAL, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, this is a deterministic method that will always result in the same (or very similar in case of an equal probabilty) results. In order to create a more random generator the Temprature parameter was introduced to the generator. The idea behind temprature sampling is that the temprature allows you to give different \"weights\" to the probabilty - in higher temprature a more <TODO> while in a lower probabilty we get towards <TODO> .\n",
    "In the generator code one can see how the equations for temprature sampling are enconded into the \"sample_word\" function. Firstly - we can note that 'lp/self.temp' is  exactly <TODO>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a summary of the first article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<TODO>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of the second article:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<TODO>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to use Yoav Goldberg's n-gram model code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_char_lm(fname, order=4):\n",
    "    data = open(fname, 'r').read()\n",
    "    lm = defaultdict(Counter)\n",
    "    pad = \"~\" * order\n",
    "    data = pad + data\n",
    "    for i in range(len(data)-order):\n",
    "        history, char = data[i:i+order], data[i+order]\n",
    "        lm[history][char]+=1\n",
    "    def normalize(counter):\n",
    "        s = float(sum(counter.values()))\n",
    "        return [(c,cnt/s) for c,cnt in counter.items()]\n",
    "    outlm = {hist:normalize(chars) for hist, chars in lm.items()}\n",
    "    return outlm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train it on the cooking recipes.\n",
    "First we need to gather all the recipes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://www.ffts.com/recipes/lg/lg32965.zip\n",
    "\n",
    "with zipfile.ZipFile(\"lg32965.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"recipes\")\n",
    "file_list = os.listdir(\"recipes\")\n",
    "recipes = \"\"\n",
    "enc = 'iso-8859-15'\n",
    "for file in file_list:\n",
    "    recipes_from_file = open(\"recipes/\" + file, 'r', encoding=enc).read()\n",
    "    recipes_from_file.translate(string.punctuation)\n",
    "    recipes += recipes_from_file\n",
    "\n",
    "print(\"Number of Recipes:\", recipes.count('MMMMM----- Recipe via Meal-Master (tm) v8.05'))\n",
    "tokenized_recipes = word_tokenize(recipes)\n",
    "num_of_tokens = len(tokenized_recipes)\n",
    "print(\"Number of Tokens:\", num_of_tokens)\n",
    "vocab = sorted(set(tokenized_recipes))\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "\n",
    "print(\"Number of Chars:\")\n",
    "\n",
    "print(\"Distribution of the size of recipes in words and in chars:\")\n",
    "\n",
    "print(\"Distribution of length of words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then split the data into a training set, dev set and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eighty_per = round(0.8 * num_of_tokens)\n",
    "ten_per = round(num_of_tokens * 0.1)\n",
    "with open(\"recipes/training.txt\",'w') as f:\n",
    "    f.write(\" \".join(tokenized_recipes[:eighty_per]))\n",
    "    f.close()\n",
    "with open(\"recipes/dev.txt\",'w') as f:\n",
    "    f.write(\" \".join(tokenized_recipes[eighty_per: eighty_per + ten_per]))\n",
    "    f.close()\n",
    "    \n",
    "with open(\"recipes/testing.txt\",'w') as f:\n",
    "    f.write(\" \".join(tokenized_recipes[eighty_per + ten_per: ]))\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can use Yoav Goldberg's n-gram model code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order = 4\n",
    "recipe_model = train_char_lm(\"recipes/training.txt\", order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to modify our perplexity method to fit a character model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def char_model_entropy(model, text, n=2):\n",
    "    H = 0.0\n",
    "    processed_ws = 0 \n",
    "    for i in range(n - 1, len(text)):\n",
    "        context, word = tuple(text[i - n:i]), text[i]\n",
    "        context = \"\".join(context)\n",
    "        score = 0\n",
    "        if(context in model):\n",
    "            for c,v in model[context]:\n",
    "                if c == word:\n",
    "                    score = v\n",
    "        print(score)\n",
    "        if(not(score == 0 or score == 0.0)) :\n",
    "            processed_ws += 1\n",
    "            H += np.log2(score)\n",
    "    return - (H / float(len(text) - n))\n",
    "\n",
    "def calc_preplexity_char(model, text, n=2):\n",
    "    text_entropy = char_model_entropy(model, text, n)\n",
    "    return 2 ** (text_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the perplexity of such model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = open(\"recipes/testing.txt\", 'r').read()\n",
    "test_data.translate(string.punctuation)\n",
    "pad = \"~\" * order\n",
    "test_data = pad + test_data\n",
    "tokenized_test = list(test_data)\n",
    "# print(tokenized_test[0])\n",
    "print(calc_preplexity_char(recipe_model, tokenized_test, order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to sample a few recipes from that model, so we will use Yoav's samling methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_letter(lm, history, order):\n",
    "        history = history[-order:]\n",
    "        dist = lm[history]\n",
    "        x = random.random()\n",
    "        for c,v in dist:\n",
    "            x = x - v\n",
    "            if x <= 0: return c\n",
    "            \n",
    "def generate_text(lm, order, nletters=1000):\n",
    "    history = \"~\" * order\n",
    "    out = []\n",
    "    for i in range(nletters):\n",
    "        c = generate_letter(lm, history, order)\n",
    "        history = history[-order:] + c\n",
    "        out.append(c)\n",
    "    return \"\".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(recipe_model, order, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(recipe_model, order, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(recipe_model, order, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(recipe_model, order, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(recipe_model, order, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
