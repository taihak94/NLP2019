{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submitted by :\n",
    "Tair Hakman & Yaniv Bin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First We'd like to start with importing all the modules we're going to use in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re, pprint, collections\n",
    "from urllib import request\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "from os.path import abspath, dirname, join\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import math\n",
    "from math import log\n",
    "import random\n",
    "from collections import *\n",
    "import zipfile, tarfile\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and preparing the ptb set given in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "\n",
    "tar = tarfile.open(\"simple-examples.tgz\",\"r\") \n",
    "for item in tar:\n",
    "    tar.extract(item, \"ptb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following code in order to tokenize the data based on the ptb method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_numbers(tokens):\n",
    "    return [x if not x.isdigit() else 'N' for x in tokens]\n",
    "\n",
    "def find_most_common(tokens, top):\n",
    "    counter = collections.Counter(tokens)\n",
    "    most_common = counter.most_common(top)\n",
    "    return [a for a, b in most_common]\n",
    "\n",
    "def replace_noncommon_tokens(sentences, most_common):\n",
    "    res = []\n",
    "    for sentence in sentences:\n",
    "        res.append([x if (x in most_common) else '<unk>' for x in sentence])\n",
    "    return res\n",
    "\n",
    "def ptb_preprocess(filenames, top=10000):\n",
    "    for single_file in filenames:\n",
    "        path = nltk.data.find(single_file)\n",
    "        raw = open(path, 'r').read()\n",
    "        segments = raw.split(\"\\n\")\n",
    "        sentences = []\n",
    "        for segment in segments:\n",
    "            tokens = word_tokenize(segment)\n",
    "            # remove punctuation\n",
    "            tokens = [x for x in tokens if x not in string.punctuation]\n",
    "            # to lowercase\n",
    "            words = [w.lower() for w in tokens]\n",
    "            # filter numbers\n",
    "            sentence = replace_numbers(words)\n",
    "            \n",
    "            sentences.append(sentence)\n",
    "       \n",
    "        # get most common words and replace all other words with unk\n",
    "        common_tokens = find_most_common([word for sentence in sentences for word in sentence], top)\n",
    "        sentences = replace_noncommon_tokens(sentences, common_tokens)\n",
    "        \n",
    "        # write out the new data into a file \n",
    "        new_filename = single_file + \".out\"\n",
    "        with open(new_filename, 'w') as f:\n",
    "            for sentence in sentences:\n",
    "                for word in sentence:\n",
    "                    f.write(\"%s \" % word)\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to test the above code on a few example files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_first_file = abspath(join(dirname(\"__file__\"), \"segmentationExample.txt\"))\n",
    "path_to_second_file = abspath(join(dirname(\"__file__\"), \"SplitAndPuncExample.txt\"))\n",
    "path_to_third_file = abspath(join(dirname(\"__file__\"), \"numbersExample.txt\"))\n",
    "path_to_file_four = abspath(join(dirname(\"__file__\"), \"uncommonExample.txt\"))\n",
    "\n",
    "ptb_preprocess([path_to_first_file, path_to_second_file, path_to_third_file], 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example the file \"segmentationExample.text\" which contains the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(path_to_first_file) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will result in the file \"segmentationExample.text.out\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_first_result = abspath(join(dirname(\"__file__\"), \"segmentationExample.txt.out\"))\n",
    "with open(path_to_first_result) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file \"SplitAndPuncExample.txt\" :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(path_to_second_file) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will result in the files \"SplitAndPuncExample.txt.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_second_result = abspath(join(dirname(\"__file__\"), \"SplitAndPuncExample.txt.out\"))\n",
    "with open(path_to_second_result) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file \"numbersExample.txt\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(path_to_third_file) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will result in the files \"numbersExample.txt.out\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_third_result = abspath(join(dirname(\"__file__\"), \"numbersExample.txt.out\"))\n",
    "with open(path_to_third_result) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to show uncommon words filtering we will reduce the size of filtering (indtead of a 10000) to 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptb_preprocess([path_to_file_four], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we test it with the file \"uncommonExample.txt\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(path_to_file_four) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_file_four_result = abspath(join(dirname(\"__file__\"), \"uncommonExample.txt.out\"))\n",
    "with open(path_to_file_four_result) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the results for running our tokenizer on shakespears work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_shakespear = abspath(join(dirname(\"__file__\"), \"shakespear.txt\"))\n",
    "ptb_preprocess([path_to_shakespear], 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can be found under a file called \"shakespear.txt.out\" attached to the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### part 1.1.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will gather the statistics on the dev file given in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following is an implementation of the ngram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NgramModel(object):\n",
    "    def __init__(self, n, train, smoothing=False, estimator=None):\n",
    "        self._n = n\n",
    "        self.is_unigram_model = (n == 1)\n",
    "        self.is_smooth = smoothing\n",
    "        \n",
    "        cfd = nltk.ConditionalFreqDist((\" \".join(train[i : i + n - 1]), \"\".join(train[i + n - 1])) for i in range(len(train) - n + 1))\n",
    "        self._probdist = nltk.ConditionalProbDist(cfd, estimator)\n",
    "        \n",
    "        # if we are not using smoothing we should implement a backoff model and keep all the seen ngrams\n",
    "        if not self.is_smooth:\n",
    "            self._ngramsData = ngrams(train, n)\n",
    "            self._ngrams = set()\n",
    "            for ngram in self._ngramsData:\n",
    "                self._ngrams.add(ngram)\n",
    "        \n",
    "        if not self.is_unigram_model:\n",
    "            if not self.is_smooth:\n",
    "                self._backoff = NgramModel(n - 1, train, estimator=estimator)\n",
    "                self._lambda = 1\n",
    "    \n",
    "    def prob(self, word, context):\n",
    "        if (self.is_smooth and self._probdist[context].logprob(word) != 0):\n",
    "            return self._probdist[context].logprob(word)\n",
    "        \n",
    "        # if we are not using smoothing we need to use a different method for avoiding 0 probability \n",
    "        elif (tuple(context.split()) + (word, ) in self._ngrams) or (self.is_unigram_model):\n",
    "            return self._probdist[context].logprob(word)\n",
    "        else:\n",
    "            new_context = \" \".join(context.split()[1:])\n",
    "            backoff = self._backoff.prob(word, new_context)\n",
    "            return self._lambda * backoff\n",
    "        \n",
    "    def logprob(self, word, context):\n",
    "        return - self.prob(word, context)\n",
    "    \n",
    "    def get_seed(self):\n",
    "        return random.choice(self._probdist.conditions())\n",
    "    \n",
    "    def generate(self, seed, length):\n",
    "        out = []\n",
    "        curr = seed\n",
    "        end = self._probdist.conditions()[-1]\n",
    "        i = 0\n",
    "        while (i <= length and (not curr == end)):\n",
    "          i += 1\n",
    "          word = self._probdist[curr].generate()\n",
    "          curr = \" \".join((curr.split())[1:] + [word])\n",
    "          out.append(word)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_word_lm(dataset, n=2):\n",
    "    model = NgramModel(n, dataset, estimator=nltk.MLEProbDist)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data structure required to build the model is - a dictionary of size at most the number of N-grams multiplied by the size of the vocabulary - so we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%latex\n",
    "\\begin{align}\n",
    "{\\mathbf{dictSize}} & = \\frac{{\\mathbf{Vocabulary}}}{N} * \\mathbf{Vocabulary}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from that we also save an array of all possible ngrams, and the backoff in case we don't use a smoothing technique.\n",
    "To save the backoff - that is in order to use the backoff technique over the perplexity calculations - the data structure is similar and defined recursively.\n",
    "One can also note we did not implement a smoothing as a Katz backoff using alpha - but a more accturate results would have been preduced if we did so. \n",
    "\n",
    "As we can see in the given class implementation - the model should export methods for evaluating itself, for generating random text, and for calculating the probabilty and entropy of a word given a context. \n",
    "\n",
    "the memory required for holding the model(non smoothing) is therefore at most: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{align}\n",
       "{\\mathbf{dictSize_N}} & = \\frac{{\\mathbf{Vocabulary}}}{N} * \\mathbf{Vocabulary} \\\\\n",
       "{\\mathbf{Memory}} & = {\\mathbf{dictSize_N}} + {\\mathbf{dictSize_(N + 1)}} + \\dots + {\\mathbf{dictSize_1}} + {\\mathbf(size-of-ngrams)}\n",
       "\\end{align}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{align}\n",
    "{\\mathbf{dictSize_N}} & = \\frac{{\\mathbf{Vocabulary}}}{N} * \\mathbf{Vocabulary} \\\\\n",
    "{\\mathbf{Memory}} & = {\\mathbf{dictSize_N}} + {\\mathbf{dictSize_(N + 1)}} + \\dots + {\\mathbf{dictSize_1}} + {\\mathbf(size-of-ngrams)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a smoothing using model - we don't need to save anything other than the initial ngram dictionary (no need to build it using recursion or keeping all seen ngrams), in that case it takes a much smaller memory of at most: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{align}\n",
       "{\\mathbf{dictSize_N}} & = \\frac{{\\mathbf{Vocabulary}}}{N} * \\mathbf{Vocabulary} \\\\\n",
       "\\end{align}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{align}\n",
    "{\\mathbf{dictSize_N}} & = \\frac{{\\mathbf{Vocabulary}}}{N} * \\mathbf{Vocabulary} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to measure how well our model is doing, we can do so by using a measure called perplexity - a model perplexity can be evaluated as followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_entropy(model, text, n=2):\n",
    "    H = 0.0\n",
    "    for i in range(n - 1, len(text)):\n",
    "        context, word = tuple(text[i - n + 1:i]), text[i]\n",
    "        context = \" \".join(context)\n",
    "        H += model.logprob(word, context)\n",
    "    return H / float(len(text) - (n - 1))\n",
    "\n",
    "def calc_preplexity(model, text, n=2):\n",
    "    text_entropy = model_entropy(model, text, n)\n",
    "    return 2 ** (text_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the perplexity - the better the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When testing the above implemented model on the ptb training and validation data we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptb_train_path = abspath(join(dirname(\"__file__\"), \"ptb/simple-examples/data/ptb.train.txt\"))\n",
    "ptb_test_path = abspath(join(dirname(\"__file__\"), \"ptb/simple-examples/data/ptb.test.txt\"))\n",
    "ptb_train_tokenized = (open(ptb_train_path, 'r').read()).split()\n",
    "ptb_test_tokenized = (open(ptb_test_path, 'r').read()).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107.32412922857847\n"
     ]
    }
   ],
   "source": [
    "# Train the ngram model with n = 3\n",
    "n = 3\n",
    "lm_MLE = train_word_lm(ptb_train_tokenized, n)\n",
    "\n",
    "print(calc_preplexity(lm_MLE, ptb_test_tokenized, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, we can always use a different estimator in order to change our model perplexity, for example, the following model is using the Lidstone estimator with a gamma instead of the MLE one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_word_lm_lidstone(dataset, n=2, gamma=0.01):\n",
    "    lidstone_estimator = lambda fd: nltk.LidstoneProbDist(fd, gamma, fd.B() + 100)\n",
    "    model = NgramModel(n, dataset,smoothing=True, estimator=lidstone_estimator)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we don't need to use the backoff technique because the lidstone estimator provides a smoothing over the probabilities - instead of using regular MLE it's like creating \"bins\" and the depositing an initial amount into each bin, and then add to that all the actual probabilities. By doing so we will never reach a point where the probability is 0 (unless gamma is 0).\n",
    "The formula it uses to do so is described as (for an experiment with count c, B bins and N outcomes) - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%latex\n",
    "\\begin{align}\n",
    "\\frac{{\\mathbf{c + gamma}}}{\\mathbf{N + B * gamma}} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how selecting the size of B can change the probability - We chose this value after some trial and error with the values, intending to not create a too sparse result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the following perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131.03836275826552\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "gamma = 0.01\n",
    "lm_LIDSTONE = train_word_lm_lidstone(ptb_train_tokenized, n, gamma)\n",
    "print(calc_preplexity(lm_LIDSTONE, ptb_test_tokenized, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following graph shows how differnt gamma values in such model change the results of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gammas = np.linspace(0.01, 1, 20)\n",
    "n = 3\n",
    "perplexities_l = list(range(20))\n",
    "i = 0\n",
    "for gamma in gammas:\n",
    "    lm_LIDSTONE = train_word_lm_lidstone(ptb_train_tokenized, n, gamma)\n",
    "    perplexities_l[i] = calc_preplexity(lm_LIDSTONE, ptb_test_tokenized, n)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa33b299438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(gammas, perplexities_l)\n",
    "plt.axis([0, 1, 0, 150])\n",
    "plt.ylabel(\"perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another way is to change the value of the n-grams , and that results in different perplexity - an example of the difference can be seen in the following graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222.34568792531184\n",
      "131.03836275826552\n",
      "92.68189306321692\n",
      "86.8073309910055\n",
      "88.1396212035187\n",
      "90.23579833162498\n",
      "91.866887156757\n",
      "92.98319580473813\n",
      "93.75392937489524\n",
      "94.32844819478187\n",
      "94.75903430473937\n",
      "95.16790589088733\n",
      "95.44106148718808\n",
      "95.66883524570639\n",
      "95.82513499124981\n",
      "95.96614348993651\n",
      "96.1112564570859\n",
      "96.22072195944745\n"
     ]
    }
   ],
   "source": [
    "perplexities_m = list(range(2, 20))\n",
    "gamma = 0.01\n",
    "\n",
    "for n in range(2, 20):\n",
    "    lm_LIDSTONE = train_word_lm_lidstone(ptb_train_tokenized, n, gamma)    \n",
    "    perp = calc_preplexity(lm_LIDSTONE, ptb_test_tokenized, n)\n",
    "    perplexities_m[n - 2] = perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD8CAYAAACGsIhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCSSwkJERCIpLEREJJLCQkREIv1/DzRiG15HtRYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3309d1128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_vals = list(range(2, 20))\n",
    "\n",
    "plt.plot(n_vals, perplexities_m)\n",
    "plt.axis([2, 20, 1, 250])\n",
    "plt.ylabel(\"perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see starting with a low value of n we get really high perplexity(bad) and as we increase the value the perplexity gets better, that is until it reaches 7, that is because the ngrams are getting too large so most of them become unseen ngrams (a 7 words sentence is too long to be frequently repeated in the text) So the ideal size for n is at about n=6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the two models above and the two graphs, we can now compose the \"ideal\" model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.92074266919535\n"
     ]
    }
   ],
   "source": [
    "n = 6\n",
    "gamma = 0.2\n",
    "lm_IDEAL = train_word_lm_lidstone(ptb_train_tokenized, n, gamma)\n",
    "print(calc_preplexity(lm_IDEAL, ptb_test_tokenized, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compared to the results from <TODO> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 1.3.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of using the model is by generating text using it, the following method generates text given a model and a seed(a starting prefix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(model, seed):\n",
    "    out = model.generate(seed, 100)\n",
    "    out = seed + \" \" + \" \".join(out)\n",
    "\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can note this method is not ideal if the seed length is smaller than the ngram size we used to train the model, a way to avoid such problem is to find a matching ngram starting with the seed and running the method from there ) or just making sure the seed is big enough. Another way is the issue of halting - here I gave it a number of iteration or forced it to halt when it sees the last possible ngram. \n",
    "Another issue that might arise is if the model run into an unknown history, in my case the code will break, so one must make sure no unknown history will occure - to do so we have the condition in the while loop, another way to avoid it is similar to the backoff we can also reduce the ngram size down and go back a step with the probability (selecting next word based on that). \n",
    "I made this generator limited to 100 iterations if possible because I didn't want to make it generate to big of a file given a large model, but one can easily change that number to anything. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few different examples of using the previously trained model with different seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: government its people its newspapers\n",
      "government its people its newspapers does not have the capacity to fight this battle successfully all <unk> countries must jointly decide to combat and punish the consumers and distributors of drugs the u.s. as the major drug consumer should lead this joint effort reduction if not the total <unk> of drug consumption is the requirement for victory much is being done in colombia to fight the drug <unk> mafia <unk> homes and <unk> have been <unk> by the military authorities and sophisticated and powerful communications equipment have been seized more than N planes and <unk> have been <unk> at airports and a large number of vehicles\n"
     ]
    }
   ],
   "source": [
    "seed = lm_IDEAL.get_seed()\n",
    "print(\"SEED:\", seed)\n",
    "generate(lm_IDEAL, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: among other things would exempt\n",
      "among other things would exempt many <unk> executives from reporting trades in their own companies ' shares the proposed changes also would allow executives to report exercises of options later and less often many of the letters maintain that investor confidence has been so shaken by the N stock market crash head the list of reasons in addition competition has <unk> the market with both <unk> and coats driving prices down the <unk> movement has n't helped sales warm <unk> over the past two years he said the company expects to conclude negotiations with other creditors within N days color systems which <unk> black-and-white film to\n"
     ]
    }
   ],
   "source": [
    "seed = lm_IDEAL.get_seed()\n",
    "print(\"SEED:\", seed)\n",
    "generate(lm_IDEAL, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: claims adjustments for insurance companies\n",
      "claims adjustments for insurance companies also beginning trading today on the big board are el paso refinery limited partnership el paso texas <unk> and franklin <unk> trust san mateo calif. <unk> el paso owns and operates a petroleum refinery franklin is a closed-end management investment company on the nasdaq over-the-counter system allied capital corp. washington d.c. <unk> began trading last thursday allied capital is a closed-end management investment company on the nasdaq over-the-counter system allied capital corp. washington d.c. <unk> began trading last thursday allied capital is a closed-end management investment company that will operate as a business development concern the yale political union does n't\n"
     ]
    }
   ],
   "source": [
    "seed = lm_IDEAL.get_seed()\n",
    "print(\"SEED:\", seed)\n",
    "generate(lm_IDEAL, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: works several times faster than\n",
      "works several times faster than previously available chips hewlett-packard co. became the first company world-wide to announce a product based on the chip earlier this month but it wo n't start shipping the computers until early next year an olivetti spokesman said the company 's factories are already beginning to produce the machine and that it should be available in europe by december what this means is that europeans will have these machines in their offices before americans do the spokesman said the new chip is a very big step in computing and it is important that olivetti be one of the first out on the\n"
     ]
    }
   ],
   "source": [
    "seed = lm_IDEAL.get_seed()\n",
    "print(\"SEED:\", seed)\n",
    "generate(lm_IDEAL, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: senate convicted u.s. district judge\n",
      "senate convicted u.s. district judge <unk> hastings of florida of eight impeachment articles removing the <unk> judge from his $ <unk> lifetime job mr. hastings 's case was particularly <unk> because it marked the first time a u.s. official was <unk> on charges of which a jury had acquitted him in N mr. hastings was found not guilty of accepting a $ N bribe in a case before him the central charge on which the senate convicted him he was only the sixth federal judge ever ousted from office after an impeachment trial with no floor debate the senate on friday voted N to <unk> mr.\n"
     ]
    }
   ],
   "source": [
    "seed = lm_IDEAL.get_seed()\n",
    "print(\"SEED:\", seed)\n",
    "generate(lm_IDEAL, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, this is a deterministic method that will always result in the same (or very similar in case of an equal probabilty) results. In order to create a more random generator the Temprature parameter was introduced to the generator. \n",
    "\n",
    "Temperature sampling uses a parameter t to determine how strong the change will be. We use the parameter to calculate a new probability to every option,it will generally be a number between 0 and  1, and once we set it we sample with it. \n",
    "the new probability will then be defined by :\n",
    "<br>$\\tilde{p_i} & = f_{\\mathcal{T}}(p)_i & =  \\frac{p_i ^ {\\frac{1}{\\mathcal{T}}}}{\\sum_{j}p_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{align}\n",
       "\\tilde{p_i} & = f_{\\mathcal{T}}(p)_i & =  \\frac{p_i ^ {\\frac{1}{\\mathcal{T}}}}{\\sum_{j}p_i} \\\\\n",
       "\\end{align}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{align}\n",
    "\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now examine this - we note that for t = 1, the probability stays identical. \n",
    "for t = 0.5 we are squaring the probabilities and renormalizing.\n",
    "\n",
    "Lets assume we have two probabilities, p1=0.6, p2=0.4, and see how this formula affects them for t=0.5 and t=0.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\tilde{p_1}_{(\\mathcal{T}=0.5)} =  \\frac{0.36}{0.52} \\sim 0.69 $\n",
    "<br>$\\tilde{p_2}_{(\\mathcal{T}=0.5)} =  \\frac{0.16}{0.52} \\sim 0.30 $\n",
    "<br>$\\tilde{p_1}_{(\\mathcal{T}=0.1)} \\sim 0.98 $\n",
    "<br>$\\tilde{p_2}_{(\\mathcal{T}=0.1)} \\sim 0.02$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this example we can observe how the changed in t affect the result - a lower t value gives the \"stronger\" values more options, and when t approaches 0 the strongest option is becoming certain, effectively going back to deterministic input.\n",
    "We can use the temperature parameter to express how much we trust the model, and to add controlled amounts of randomness to our generations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at the code by Sameer Sing. He chose to work with log probabilities in order to achieve the same results.\n",
    "The first part of his code does two things:\n",
    "- Create a list called wps. Each element in his list is a pair of a word w, and the log probability of w divided by temp $\\log(\\frac{p(w)}{t})$.\n",
    "- Sums the total of the $\\frac{log}{temp}$ values, so he can use it as a normalizing factor.\n",
    "\n",
    "by the end of the first part tot is equal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\log_2(2^{\\log\\frac{p_1(w)}{t}} + 2^{\\log(\\frac{p_2(w)}{t}}+ \\dots + 2^{\\log(\\frac{p_n(w)}{t}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after he assigned weighted log probabilities, its time to choose a random word based on the new probability, which is what he does in the second part of the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first a random number p $(0<p<1)$ is chosen. Now for every possible word, he perform the following steps:\n",
    "- Sum the logs probabilities so far $s$.\n",
    "- Subtract $s$ from the total we counted in the first part, and calculate $2^{ourResult}$.\n",
    "- if $p$ is smaller than the answer, we choose this word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand why it works, lets continue with our example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_1 = -1.473$ . This is the only log weve encountered yet, so $s = p_1$.\n",
    "<br>$2^{s-tot} = 0.69$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one can note this is the __exact same probability we calculated in the first part.__\n",
    "How did this happen?\n",
    "- In the first part we computed $p^{\\frac{1}{t}}$\n",
    "- In the second part we computer $2^{\\log \\frac{p}{t}}$\n",
    "<br>But those expressions are equivalent :\n",
    "$2^{\\frac{\\log_2{p}}{t}} = 2^{\\log_2{p} * \\frac{1}{t}} = p^{\\frac{1}{t}} = p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same applies to the normalizing sum below. And obviously, our random number p has a probability of 0.69 to be smaller than 0.69, hence giving the same word an 0.69 chance to be chosen.\n",
    "So as we see, the code by Sameer Sing effectively uses the exact same mathematical idea of temperature expressed by Russel Stewart, just implemented using logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got to read two articles about RNNs, the first by Andrej Karpathy talks about what RNNs are - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike normal Neural Networks, Recurrent Neural Networks (RNNs) allow us to operate over sequences of vectors, and not just one. Considering it is still a Neural Network it operates similarly to any other  it has a step function that gets an input vector x and returns an output vector y, but this type of network differs in one thing  RNNs keep a hidden vector h, whose value are updated in every computation and are used to generate the output, so the output depends not only on the current input, but also on the entire history of vectors it ever computed.\n",
    "\n",
    "More precisely, the RNN keeps three matrices: W_xh (input to hidden),  W_hh (hidden to hidden), W_hy (hidden to output). We first initialized them to random matrices and every time step is called, we use them to update h and calculate the output.\n",
    "\n",
    "When constructing a language char-based model using RNN Karpathy gets pretty good results (even for surprising things like Linux source code), and he even sample up the process in different stages and see how it learns  in the first iterations it composes gibberish, by the 700 iteration it learns words and by 2000 it can write full sentences of legal words. He can also visualize values in neurons and learn how they differ in tasks, even though it wasnt coded by humans  the network decided upon it.\n",
    "\n",
    "RNNs are a growing field that is widely used for models of NLP, Computer Vision and other fields.\n",
    "They still have flaws (like being good at memorizing but not always in generalization, or using a very large computations in each step), but there is a lot of progress and they are evolving daily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second article by Yoav Goldberg talks about char-based language models and why even simpler models work -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<TODO>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to use Yoav Goldberg's n-gram model code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_char_lm(fname, order=4):\n",
    "    data = open(fname, 'r').read()\n",
    "    lm = defaultdict(Counter)\n",
    "    pad = \"~\" * order\n",
    "    data = pad + data\n",
    "    for i in range(len(data)-order):\n",
    "        history, char = data[i:i+order], data[i+order]\n",
    "        lm[history][char]+=1\n",
    "    def normalize(counter):\n",
    "        s = float(sum(counter.values()))\n",
    "        return [(c,cnt/s) for c,cnt in counter.items()]\n",
    "    outlm = {hist:normalize(chars) for hist, chars in lm.items()}\n",
    "    return outlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see this model is unsmooth - so if we try running this on a text that has a lot of unseen ngrams we will face  some problems with calculating the perplexity (too many 0 probabilities) - so we need to modify this model to smooth out those probabilities so we won't have 0 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to use our new model and train it on the cooking recipes \n",
    "but first we need to gather all the recipes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recipes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-82e13fb98ea6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtokenized_recipes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecipes_from_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Recipes:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MMMMM----- Recipe via Meal-Master (tm) v8.05'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# tokenized_recipes = word_tokenize(recipes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mnum_of_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_recipes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'recipes' is not defined"
     ]
    }
   ],
   "source": [
    "!wget http://www.ffts.com/recipes/lg/lg32965.zip\n",
    "\n",
    "with zipfile.ZipFile(\"lg32965.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"recipes\")\n",
    "file_list = os.listdir(\"recipes\")\n",
    "tokenized_recipes = []\n",
    "enc = 'iso-8859-15'\n",
    "for file in file_list:\n",
    "    recipes_from_file = open(\"recipes/\" + file, 'r', encoding=enc).read()\n",
    "    recipes_from_file.translate(string.punctuation)\n",
    "    tokenized_recipes += word_tokenize(recipes_from_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and gather some basic statistics on them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of Recipes:\", recipes.count('MMMMM----- Recipe via Meal-Master (tm) v8.05'))\n",
    "# tokenized_recipes = word_tokenize(recipes)\n",
    "num_of_tokens = len(tokenized_recipes)\n",
    "print(\"Number of Tokens:\", num_of_tokens)\n",
    "vocab = sorted(set(tokenized_recipes))\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "\n",
    "print(\"Number of Chars:\")\n",
    "\n",
    "print(\"Distribution of the size of recipes in words and in chars:\")\n",
    "\n",
    "print(\"Distribution of length of words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then would like to split the data into a training set, dev set and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_of_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b78b9b737723>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meighty_per\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_of_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mten_per\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_of_tokens\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recipes/training.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_recipes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0meighty_per\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_of_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "eighty_per = round(0.8 * num_of_tokens)\n",
    "ten_per = round(num_of_tokens * 0.1)\n",
    "with open(\"recipes/training.txt\",'w') as f:\n",
    "    f.write(\" \".join(tokenized_recipes[:eighty_per]))\n",
    "    f.close()\n",
    "with open(\"recipes/dev.txt\",'w') as f:\n",
    "    f.write(\" \".join(tokenized_recipes[eighty_per: eighty_per + ten_per]))\n",
    "    f.close()\n",
    "    \n",
    "with open(\"recipes/testing.txt\",'w') as f:\n",
    "    f.write(\" \".join(tokenized_recipes[eighty_per + ten_per: ]))\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_training = abspath(join(dirname(\"__file__\"), \"recipes/training.txt\"))\n",
    "path_to_dev = abspath(join(dirname(\"__file__\"), \"recipes/dev.txt\"))\n",
    "path_to_test =abspath(join(dirname(\"__file__\"), \"recipes/testing.txt\"))\n",
    "ptb_preprocess([path_to_training, path_to_dev, path_to_test], 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can use the modified Yoav Goldberg's n-gram model code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order = 4\n",
    "path_to_training_tokenized = abspath(join(dirname(\"__file__\"), \"recipes/training.txt.out\"))\n",
    "recipe_model = train_char_lm(path_to_training_tokenized, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to modify our perplexity method from earlier to fit a character model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def char_model_entropy(model, text, n=2):\n",
    "    H = 0.0\n",
    "    processed_ws = 0 \n",
    "    for i in range(n - 1, len(text)):\n",
    "        context, word = tuple(text[i - n:i]), text[i]\n",
    "        context = \"\".join(context)\n",
    "        score = 0\n",
    "        if(context in model):\n",
    "            for c,v in model[context]:\n",
    "                if c == word:\n",
    "                    score = v\n",
    "        if(not(score == 0 or score == 0.0)) :\n",
    "            processed_ws += 1\n",
    "            H += log(score, 2)\n",
    "    return - (H / float(len(text) - n))\n",
    "\n",
    "def calc_preplexity_char(model, text, n=2):\n",
    "    text_entropy = char_model_entropy(model, text, n)\n",
    "    return 2 ** (text_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the perplexity of such model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_testing_tokenized = abspath(join(dirname(\"__file__\"), \"recipes/testing.txt.out\"))\n",
    "test_data = open(path_to_testing_tokenized, 'r').read()\n",
    "pad = \"~\" * order\n",
    "test_data = pad + test_data\n",
    "tokenized_test = list(test_data)\n",
    "print(calc_preplexity_char(recipe_model, tokenized_test, order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to sample a few recipes from that model, so we will use Yoav's samling methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_letter(lm, history, order):\n",
    "        history = history[-order:]\n",
    "        dist = lm[history]\n",
    "        x = random.random()\n",
    "        for c,v in dist:\n",
    "            x = x - v\n",
    "            if x <= 0: return c\n",
    "            \n",
    "def generate_text(lm, order, nletters=1000):\n",
    "    history = \"~\" * order\n",
    "    out = []\n",
    "    for i in range(nletters):\n",
    "        c = generate_letter(lm, history, order)\n",
    "        history = history[-order:] + c\n",
    "        out.append(c)\n",
    "    return \"\".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(generate_text(recipe_model, order, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(generate_text(recipe_model, order, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(generate_text(recipe_model, order, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(generate_text(recipe_model, order, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(generate_text(recipe_model, order, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to generate a dataset of N points for some function y(x).\n",
    "We will start by writing a function that generates x and t such that - t_i = y(x_i) + N(mu, sigma):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb9d7881470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generateDataset(N, f, sigma):\n",
    "    mu = 0\n",
    "    s = np.array(np.random.normal(mu, sigma, N))\n",
    "    x = np.array(np.linspace(0.0, 1.0, N))\n",
    "    vf = np.vectorize(f)\n",
    "    t = np.add(vf(x), s)\n",
    "    return (x, t)\n",
    "\n",
    "sigma = 0.03\n",
    "def f(x): return math.sin(2 * math.pi * x)\n",
    "N = 100\n",
    "\n",
    "x, t = generateDataset(N, f, sigma)\n",
    "plt.scatter(x, t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to learn the function y by using x and t using a least squares estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OptimizeLS(x, t, M):\n",
    "    phi = np.vstack([np.power(x_i, m) for m in range(M)] for x_i in x)\n",
    "    prod = np.dot(phi.T, phi)\n",
    "    i = np.linalg.inv(prod)\n",
    "    phi_mults = np.dot(i, phi.T)\n",
    "    w = np.dot(phi_mults, t)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we shall test it using the sin function from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = 0.03\n",
    "def f(x): return math.sin(2 * math.pi * x)\n",
    "N = 10\n",
    "\n",
    "x, t = generateDataset(N, f, sigma)\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.subplot(5 ,1, 1)\n",
    "plt.scatter(x, t)\n",
    "\n",
    "M = [1, 3, 5, 10]\n",
    "j = 2\n",
    "for m in M:\n",
    "    w = OptimizeLS(x, t, m)\n",
    "    res = np.zeros(len(t))\n",
    "    for i in range(len(w)):\n",
    "        res = res + np.dot(w[i], np.power(x, i))\n",
    "    subp = plt.subplot(5, 1, j)    \n",
    "    j += 1\n",
    "    subp.set_title('M = {}'.format(m))\n",
    "    subp.scatter(x, res)\n",
    "plt.tight_layout(pad=0.5, w_pad=0.5, h_pad=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of least squares we would like to estimate y using polynomial curve fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizePLS(x, t, M, l):\n",
    "    phi = np.vstack([np.power(x_i, m) for m in range(M)] for x_i in x)\n",
    "    prod = np.dot(phi.T, phi)\n",
    "    l_i = np.dot(l, np.identity(prod.shape[0]))\n",
    "    fixed = np.add(prod, l_i)\n",
    "    i = np.linalg.inv(fixed)\n",
    "    phi_mults = np.dot(i, phi.T)\n",
    "    w = np.dot(phi_mults, t)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we will test our function with a random value of lambda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = 0.03\n",
    "def f(x): return math.sin(2 * math.pi * x)\n",
    "N = 10\n",
    "\n",
    "x, t = generateDataset(N, f, sigma)\n",
    "M = 5\n",
    "l = 0.05\n",
    "w = optimizePLS(x, t, M, l)\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.subplot(2 ,1, 1)\n",
    "plt.scatter(x, t)\n",
    "\n",
    "res = np.zeros(len(t))\n",
    "for i in range(len(w)):\n",
    "    res = res + np.dot(w[i], np.power(x, i))\n",
    "subp = plt.subplot(2, 1, 2)\n",
    "subp.scatter(x, res)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, adding the lambda didn't result in the best outcome- \n",
    "We want to optimize the lambda value, to do so we will firstly need to construct a training set, a dataset and a validation set in order to optimize it. \n",
    "to do so we will extend the function we created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generateDataset3(N, f, sigma):\n",
    "    mu = 0\n",
    "    s = np.array(np.random.normal(mu, sigma, N))\n",
    "    \n",
    "    x_1 = np.array(np.linspace(0.0, 1.0, N))\n",
    "    x_2 = np.copy(x_1)\n",
    "    x_3 = np.copy(x_1)\n",
    "    np.random.shuffle(x_2)\n",
    "    np.random.shuffle(x_3)\n",
    "    \n",
    "    vf = np.vectorize(f)\n",
    "    \n",
    "    t_1 = np.add(vf(x_1), s)\n",
    "    t_2 = np.add(vf(x_2), s)\n",
    "    t_3 = np.add(vf(x_3), s)\n",
    "    \n",
    "    return [(x_1, t_1), (x_2, t_2), (x_3, t_3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and testing this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = 0.03\n",
    "def f(x): return math.sin(2 * math.pi * x)\n",
    "N = 100\n",
    "\n",
    "res = generateDataset3(N, f, sigma)\n",
    "x_1, t_1 = res[0]\n",
    "x_2, t_2 = res[1]\n",
    "x_3, t_3 = res[2]\n",
    "plt.scatter(x_1, t_1)\n",
    "plt.scatter(x_2, t_2)\n",
    "plt.scatter(x_3, t_3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to estimate the error of a model from the real thing, so we define the normalized error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def N_E(x, t, w):\n",
    "    err = 0.0\n",
    "    for i in range(len(t)):\n",
    "        poly = 0.0\n",
    "        for m in range(len(w)):\n",
    "            poly += (w[m] * (x[i] ** m))\n",
    "        err += (t[i] - poly) ** 2\n",
    "    err = err ** 0.5\n",
    "    err = (1 / float(len(t))) * err\n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for example, for the previously generates results we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sigma = 0.03\n",
    "def f(x): return math.sin(2 * math.pi * x)\n",
    "N = 100\n",
    "\n",
    "x, t = generateDataset(N, f, sigma)\n",
    "M = 5\n",
    "l = 0.05\n",
    "w = optimizePLS(x, t, M, l)\n",
    "\n",
    "print(N_E(x, t, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanna show how different lambda values will give different result - the following graph plots will show the normalized error on different lambda value for all x types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = 0.03\n",
    "def f(x): return math.sin(2 * math.pi * x)\n",
    "N = 10\n",
    "\n",
    "res = generateDataset3(N, f, sigma)\n",
    "x_test, t_test = res[0]\n",
    "x_validate, t_validate = res[1]\n",
    "x_train, t_train = res[2]\n",
    "\n",
    "def run_lambdas(x_train, t_train, x_test, t_test, x_validate, t_validate):\n",
    "    j = 0\n",
    "    normalized_test = np.zeros(20)\n",
    "    normalized_train = np.zeros(20)\n",
    "    normalized_validate = np.zeros(20)\n",
    "    for i in range(-40, -20):\n",
    "        l = np.exp(i)\n",
    "        w_test = optimizePLS(x_train, t_train, M, l)\n",
    "        normalized_test[j] = N_E(x_test, t_test, w_test)\n",
    "        normalized_validate[j] = N_E(x_validate, t_validate, w_test)\n",
    "        normalized_train[j] = N_E(x_train, t_train, w_test)\n",
    "        j += 1\n",
    "    return [normalized_test, normalized_train, normalized_validate]\n",
    "\n",
    "def run(N, sigma, f):\n",
    "    res = generateDataset3(N, f, sigma)\n",
    "    x_test, t_test = res[0]\n",
    "    x_validate, t_validate = res[1]\n",
    "    x_train, t_train = res[2]\n",
    "    \n",
    "    return run_lambdas(x_train, t_train, x_test, t_test, x_validate, t_validate)\n",
    "\n",
    "sigma = 0.03\n",
    "def f(x): return math.sin(2 * math.pi * x)\n",
    "N = 10\n",
    "\n",
    "res = run(N, sigma, f)\n",
    "n_tst5, n_trn5, n_vld5 = res[0], res[1], res[2]\n",
    "\n",
    "N = 100\n",
    "res = run(N, sigma, f)\n",
    "n_tst10, n_trn10, n_vld10 = res[0], res[1], res[2]\n",
    "\n",
    "plt.figure(num = 2, figsize=(10, 10))\n",
    "\n",
    "a_tst = plt.subplot(2 ,3, 1)\n",
    "a_tst.set_title('N = 10, testing set', y=1.08)\n",
    "a_tst.plot(n_tst5)\n",
    "a_v = plt.subplot(2 ,3, 2)\n",
    "a_v.set_title('N = 10,validtion set', y=1.08)\n",
    "a_v.plot(n_vld5)\n",
    "a_t = plt.subplot(2 ,3, 3)\n",
    "a_t.set_title('N = 10,training set', y=1.08)\n",
    "a_t.plot(n_trn5)\n",
    "\n",
    "a_tst_b = plt.subplot(2 ,3, 4)\n",
    "a_tst_b.set_title('N = 100, testing set', y=1.08)\n",
    "a_tst_b.plot(n_tst10)\n",
    "a_v_b = plt.subplot(2 ,3, 5)\n",
    "a_v_b.set_title('N = 100,validtion set', y=1.08)\n",
    "a_v_b.plot(n_vld10)\n",
    "a_t_b = plt.subplot(2 ,3, 6)\n",
    "a_t_b.set_title('N = 100,training set', y=1.08)\n",
    "a_t_b.plot(n_trn10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create w based on an optimized lambda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizePLSLambda(xt, tt, xv, tv, M):\n",
    "    w_max = np.zeros(M)\n",
    "    avg_err = 1.0\n",
    "    for i in range(-40, -20):\n",
    "        l = np.exp(i)\n",
    "        w_curr = optimizePLS(xt, tt, M, l)\n",
    "        curr_err = N_E(xv, tv, w_curr)\n",
    "        if(curr_err < avg_err):\n",
    "            avg_err = curr_err\n",
    "            w_max = np.copy(w_curr)\n",
    "    return w_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and testing it on the same example from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = 0.03\n",
    "def f(x): return math.sin(2 * math.pi * x)\n",
    "N = 10\n",
    "\n",
    "res = generateDataset3(N, f, sigma)\n",
    "x_test, t_test = res[0]\n",
    "x_validate, t_validate = res[1]\n",
    "x_train, t_train = res[2]\n",
    "\n",
    "plt.figure(num = 3, figsize=(8, 10))\n",
    "origin = plt.subplot(2 ,1, 1)\n",
    "origin.scatter(x_test, t_test)\n",
    "\n",
    "M = 5\n",
    "w_res = optimizePLSLambda(x_train, t_train, x_validate, t_validate, M)\n",
    "res = np.zeros(len(t_test))\n",
    "for i in range(len(w_res)):\n",
    "    res = res + np.dot(w_res[i], np.power(x_test, i))\n",
    "subp = plt.subplot(2, 1, 2)\n",
    "subp.scatter(x_test, res)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesianEstimator(x, t, M, alpha, sigma2):\n",
    "    N = len(x)\n",
    "    \n",
    "    #builds phi for given vector xx\n",
    "    def phi(xx):\n",
    "        return np.array([(xx ** i) for i in range(M + 1)])\n",
    "    \n",
    "    #calculate S\n",
    "    alpha_i = alpha * np.eye(M + 1)\n",
    "    S = np.zeros((M + 1, M + 1))\n",
    "    for i in range(N):\n",
    "        phi_xi = phi(x[i])\n",
    "        S += np.outer(phi_xi, phi_xi.T)\n",
    "    S = np.linalg.inv(alpha_i + (S / sigma2))\n",
    "    \n",
    "    #calculate m(x)\n",
    "    def m(xx):\n",
    "        phi_t = phi(xx).T\n",
    "        xt_sum = np.zeros(M + 1)\n",
    "        for i in range(N):\n",
    "            xt_sum += phi(x[i]) * t[i]\n",
    "        return (1 / sigma2) * np.dot(np.dot(phi_t, S), xt_sum)\n",
    "    \n",
    "    #calculate s2(xx)\n",
    "    def var(xx):\n",
    "        phi_x = phi(xx)\n",
    "        return sigma2 + np.dot(phi_x.T, np.dot(S, phi_x))\n",
    "    \n",
    "    mean = lambda x_t: m(x_t)\n",
    "    variance = lambda x_t: var(x_t)\n",
    "    \n",
    "    return (mean, variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAFpCAYAAACVjP/1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naQJQSqkQ9f/lHZ5aPBLmmAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb9d23605f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha = 0.005\n",
    "sigma2 = 1/11.1\n",
    "M = 9\n",
    "def f(x): return math.sin(2 * math.pi * x)\n",
    "vf = np.vectorize(f)\n",
    "N = 10\n",
    "\n",
    "res = generateDataset(N, f, sigma)\n",
    "x_t, t_t = res\n",
    "\n",
    "m, var = bayesianEstimator(x_test, t_test, M, alpha, sigma2)\n",
    "\n",
    "plt.figure(num = 10, figsize=(6, 6))\n",
    "upperBound = np.vectorize(lambda x: m(x) + np.sqrt(var(x)))\n",
    "lowerBound = np.vectorize(lambda x: m(x) - np.sqrt(var(x)))\n",
    "plt_graph = plt.subplot(111)\n",
    "plt_graph.fill_between(x_t, upperBound(x_t), lowerBound(x_t), alpha=0.3, color='r')\n",
    "plt_graph.scatter(x_t, t_t, edgecolor='b', facecolor='none', marker='o', s=60, lw=2)\n",
    "plt_graph.plot(x_t, m(x_t), lw=2, color='g')\n",
    "plt_graph.plot(x_t, vf(x_t), lw=2, color='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
