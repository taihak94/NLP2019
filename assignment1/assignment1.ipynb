{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submitted by :\n",
    "Tair Hakman & Yaniv Bin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First We'd like to start with importing all the modules we're going to use in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re, pprint, collections\n",
    "from urllib import request\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "from os.path import abspath, dirname, join\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import math, statistics\n",
    "from math import log\n",
    "import random\n",
    "from collections import *\n",
    "import zipfile, tarfile\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following code in order to tokenize the data based on the ptb method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_numbers(tokens):\n",
    "    return [x if not x.isdigit() else 'N' for x in tokens]\n",
    "\n",
    "def find_most_common(tokens, top):\n",
    "    counter = collections.Counter(tokens)\n",
    "    most_common = counter.most_common(top)\n",
    "    return [a for a, b in most_common]\n",
    "\n",
    "def replace_noncommon_tokens(sentences, most_common):\n",
    "    res = []\n",
    "    for sentence in sentences:\n",
    "        res.append([x if (x in most_common) else '<unk>' for x in sentence])\n",
    "    return res\n",
    "\n",
    "def ptb_preprocess(filenames, top=10000):\n",
    "    for single_file in filenames:\n",
    "        path = nltk.data.find(single_file)\n",
    "        raw = open(path, 'r').read()\n",
    "        segments = raw.split(\"\\n\")\n",
    "        sentences = []\n",
    "        for segment in segments:\n",
    "            tokens = word_tokenize(segment)\n",
    "            # remove punctuation\n",
    "            tokens = [x for x in tokens if x not in string.punctuation]\n",
    "            # to lowercase\n",
    "            words = [w.lower() for w in tokens]\n",
    "            # filter numbers\n",
    "            sentence = replace_numbers(words)            \n",
    "            sentences.append(sentence)\n",
    "       \n",
    "        # get most common words and replace all other words with unk\n",
    "        common_tokens = find_most_common([word for sentence in sentences for word in sentence], top)\n",
    "        sentences = replace_noncommon_tokens(sentences, common_tokens)\n",
    "        \n",
    "        # write out the new data into a file \n",
    "        new_filename = single_file + \".out\"\n",
    "        with open(new_filename, 'w') as f:\n",
    "            for sentence in sentences:\n",
    "                for word in sentence:\n",
    "                    f.write(\"%s \" % word)\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to test the above code on a few example files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_first_file = abspath(join(dirname(\"__file__\"), \"segmentationExample.txt\"))\n",
    "path_to_second_file = abspath(join(dirname(\"__file__\"), \"SplitAndPuncExample.txt\"))\n",
    "path_to_third_file = abspath(join(dirname(\"__file__\"), \"numbersExample.txt\"))\n",
    "path_to_file_four = abspath(join(dirname(\"__file__\"), \"uncommonExample.txt\"))\n",
    "\n",
    "ptb_preprocess([path_to_first_file, path_to_second_file, path_to_third_file], 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example the file \"segmentationExample.text\" which contains the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We expect this file\n",
      "to be with the same lines\n",
      "as shown here\n",
      "so let's hope it does\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_first_file) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will result in the file \"segmentationExample.text.out\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we expect this file \n",
      "to be with the same lines \n",
      "as shown here \n",
      "so let 's hope it does \n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_to_first_result = abspath(join(dirname(\"__file__\"), \"segmentationExample.txt.out\"))\n",
    "with open(path_to_first_result) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file \"SplitAndPuncExample.txt\" :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we want to make sure this text reader splits words like don't the right way,\n",
      "and also that it will remove all the punctioation marks, because we don't need those in our life :\n",
      "here, they're gone. \n"
     ]
    }
   ],
   "source": [
    "with open(path_to_second_file) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will result in the files \"SplitAndPuncExample.txt.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we want to make sure this text reader splits words like do n't the right way \n",
      "and also that it will remove all the punctioation marks because we do n't need those in our life \n",
      "here they 're gone \n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_to_second_result = abspath(join(dirname(\"__file__\"), \"SplitAndPuncExample.txt.out\"))\n",
    "with open(path_to_second_result) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file \"numbersExample.txt\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "see how 123 will turn into the number format\n",
      "even if it's 123$\n",
      "or #123 or 1 2 3 \n",
      "a number is always a number\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_third_file) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will result in the files \"numbersExample.txt.out\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "see how N will turn into the number format \n",
      "even if it 's N \n",
      "or N or N N N \n",
      "a number is always a number \n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_to_third_result = abspath(join(dirname(\"__file__\"), \"numbersExample.txt.out\"))\n",
    "with open(path_to_third_result) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to show uncommon words filtering we will reduce the size of filtering (indtead of a 10000) to 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptb_preprocess([path_to_file_four], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we test it with the file \"uncommonExample.txt\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have over ten words in this file, \n",
      "but the word file and the word word are more frequent for example...\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file_four) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have over ten words in this file \n",
      "<unk> the word file <unk> the word word <unk> <unk> <unk> <unk> <unk> <unk> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_to_file_four_result = abspath(join(dirname(\"__file__\"), \"uncommonExample.txt.out\"))\n",
    "with open(path_to_file_four_result) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the results for running our tokenizer on shakespears work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_shakespear = abspath(join(dirname(\"__file__\"), \"shakespear.txt\"))\n",
    "ptb_preprocess([path_to_shakespear], 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can be found under a file called \"shakespear.txt.out\" attached to the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### part 1.1.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we'll be using the code of the previous part on the PTB dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptb_train_path = abspath(join(dirname(\"__file__\"), \"ptb.train.txt\"))\n",
    "ptb_test_path = abspath(join(dirname(\"__file__\"), \"ptb.test.txt\"))\n",
    "with open(ptb_train_path) as f:\n",
    "    ptb_train = f.read()\n",
    "with open(ptb_test_path) as f:\n",
    "    ptb_test = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to report some basic statistics on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 978726\n",
      "Number of characters: 5108686\n",
      "Number of distinct tokens: 9996\n",
      "Total number of tokens corresponding to the top-N most frequent words in the vocabulary: 785187\n",
      "Type distribution: \n",
      "unk:  0.045998573655956825\n",
      "numbers:  0.033187020677901684\n",
      "rest:  0.9208144056661415\n",
      "Average number and standard deviation of characters per token: 4.2197315694075765 2.705652415851888\n",
      "Distinct n-grams of words of length 2 : 277419\n",
      "Distinct n-grams of words of length 3 : 598666\n",
      "Distinct n-grams of words of length 4 : 754993\n",
      "Distinct n-grams of characters of length 2 : 763\n",
      "Distinct n-grams of characters of length 3 : 6246\n",
      "Distinct n-grams of characters of length 4 : 29334\n",
      "Distinct n-grams of characters of length 5 : 105574\n",
      "Distinct n-grams of characters of length 6 : 291396\n",
      "Distinct n-grams of characters of length 7 : 600865\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEOCAYAAACTqoDjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d8cc39eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gather_stats (text, n):\n",
    "    tokens = word_tokenize(text)\n",
    "    text = \" \".join(tokens)\n",
    "    corpus_counts = nltk.FreqDist(tokens)\n",
    "    common = find_most_common (tokens, n)\n",
    "    words_len = [len(x) for x in tokens]\n",
    "    print(\"Number of tokens:\", len(tokens))\n",
    "    print(\"Number of characters:\", len(text))\n",
    "    print(\"Number of distinct tokens:\", len(corpus_counts))\n",
    "    print(\"Total number of tokens corresponding to the top-N most frequent words in the vocabulary:\",\n",
    "           len([x for x in tokens if x in common]))\n",
    "    \n",
    "    print(\"Type distribution: \")\n",
    "    unk_count, num_count = corpus_counts.get(\"unk\"), corpus_counts.get(\"N\")\n",
    "    print(\"unk: \", unk_count / len(tokens))\n",
    "    print(\"numbers: \", num_count / len(tokens))\n",
    "    print(\"rest: \", (len(tokens) - unk_count - num_count) / len(tokens))\n",
    "\n",
    "    print(\"Average number and standard deviation of characters per token:\", \n",
    "           statistics.mean(words_len), statistics.stdev(words_len))\n",
    "    \n",
    "    wordGrams=[]\n",
    "    for i in range (2,5):\n",
    "        ng = generate_ngrams (tokens, i)\n",
    "        gramCount = nltk.FreqDist(ng)\n",
    "        print (\"Distinct n-grams of words of length\", i, \":\", len(gramCount))\n",
    "        wordGrams.append([i, len(gramCount)])    \n",
    "    \n",
    "    chars = [c for c in text]\n",
    "    for i in range (2,8):\n",
    "        ng = generate_ngrams (chars, i)\n",
    "        gramCount = nltk.FreqDist(ng)\n",
    "        print (\"Distinct n-grams of characters of length\", i, \":\", len(gramCount))\n",
    "        \n",
    "    plt.loglog([val for word,val in corpus_counts.most_common(4000)])\n",
    "    plt.xlabel('rank')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.show()\n",
    "    \n",
    "def generate_ngrams (tokens, n):\n",
    "    g = ngrams(tokens, n)\n",
    "    return [x for x in g]\n",
    "\n",
    "gather_stats(ptb_train, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph confirms our assumption, that word count distributions in the PTB dataset indeed do follow power law distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following is an implementation of the ngram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NgramModel(object):\n",
    "    def __init__(self, n, train, smoothing=False, estimator=None):\n",
    "        self._n = n\n",
    "        self.is_unigram_model = (n == 1)\n",
    "        self.is_smooth = smoothing\n",
    "        \n",
    "        cfd = nltk.ConditionalFreqDist((\" \".join(train[i : i + n - 1]), \"\".join(train[i + n - 1])) for i in range(len(train) - n + 1))\n",
    "        self._probdist = nltk.ConditionalProbDist(cfd, estimator)\n",
    "        \n",
    "        # if we are not using smoothing we should implement a backoff model and keep all the seen ngrams\n",
    "        if not self.is_smooth:\n",
    "            self._ngramsData = ngrams(train, n)\n",
    "            self._ngrams = set()\n",
    "            for ngram in self._ngramsData:\n",
    "                self._ngrams.add(ngram)\n",
    "        \n",
    "        if not self.is_unigram_model:\n",
    "            if not self.is_smooth:\n",
    "                self._backoff = NgramModel(n - 1, train, estimator=estimator)\n",
    "                self._lambda = 1\n",
    "    \n",
    "    def prob(self, word, context):\n",
    "        if (self.is_smooth and self._probdist[context].logprob(word) != 0):\n",
    "            return self._probdist[context].logprob(word)\n",
    "        \n",
    "        # if we are not using smoothing we need to use a different method for avoiding 0 probability \n",
    "        elif (tuple(context.split()) + (word, ) in self._ngrams) or (self.is_unigram_model):\n",
    "            return self._probdist[context].logprob(word)\n",
    "        else:\n",
    "            new_context = \" \".join(context.split()[1:])\n",
    "            backoff = self._backoff.prob(word, new_context)\n",
    "            return self._lambda * backoff\n",
    "        \n",
    "    def logprob(self, word, context):\n",
    "        return - self.prob(word, context)\n",
    "    \n",
    "    def get_seed(self):\n",
    "        return random.choice(self._probdist.conditions())\n",
    "    \n",
    "    def generate(self, seed, length):\n",
    "        out = []\n",
    "        curr = seed\n",
    "        end = self._probdist.conditions()[-1]\n",
    "        i = 0\n",
    "        while (i <= length and (not curr == end)):\n",
    "            i += 1\n",
    "            word = self._probdist[curr].generate()\n",
    "            curr = \" \".join((curr.split())[1:] + [word])\n",
    "            out.append(word)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_word_lm(dataset, n=2):\n",
    "    model = NgramModel(n, dataset, estimator=nltk.MLEProbDist)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data structure required to build the model is - a dictionary of size at most the number of N-grams multiplied by the size of the vocabulary - so we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{align}\n",
       "{\\mathbf{dictSize}} & = \\frac{{\\mathbf{Vocabulary}}}{N} * \\mathbf{Vocabulary}\n",
       "\\end{align}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{align}\n",
    "{\\mathbf{dictSize}} & = \\frac{{\\mathbf{Vocabulary}}}{N} * \\mathbf{Vocabulary}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from that we also save an array of all possible ngrams, and the backoff in case we don't use a smoothing technique.\n",
    "To save the backoff - that is in order to use the backoff technique over the perplexity calculations - the data structure is similar and defined recursively.\n",
    "One can also note we did not implement a smoothing as a Katz backoff using alpha - but a more accturate results would have been preduced if we did so. \n",
    "\n",
    "As we can see in the given class implementation - the model should export methods for evaluating itself, for generating random text, and for calculating the probabilty and entropy of a word given a context. \n",
    "\n",
    "the memory required for holding the model(non smoothing) is therefore at most: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{align}\n",
       "{\\mathbf{dictSize_N}} & = \\frac{{\\mathbf{Vocabulary}}}{N} * \\mathbf{Vocabulary} \\\\\n",
       "{\\mathbf{Memory}} & = {\\mathbf{dictSize_N}} + {\\mathbf{dictSize_(N + 1)}} + \\dots + {\\mathbf{dictSize_1}} + {\\mathbf(size-of-ngrams)}\n",
       "\\end{align}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{align}\n",
    "{\\mathbf{dictSize_N}} & = \\frac{{\\mathbf{Vocabulary}}}{N} * \\mathbf{Vocabulary} \\\\\n",
    "{\\mathbf{Memory}} & = {\\mathbf{dictSize_N}} + {\\mathbf{dictSize_(N + 1)}} + \\dots + {\\mathbf{dictSize_1}} + {\\mathbf(size-of-ngrams)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a smoothing using model - we don't need to save anything other than the initial ngram dictionary (no need to build it using recursion or keeping all seen ngrams), in that case it takes a much smaller memory of at most: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{align}\n",
       "{\\mathbf{dictSize_N}} & = \\frac{{\\mathbf{Vocabulary}}}{N} * \\mathbf{Vocabulary} \\\\\n",
       "\\end{align}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{align}\n",
    "{\\mathbf{dictSize_N}} & = \\frac{{\\mathbf{Vocabulary}}}{N} * \\mathbf{Vocabulary} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to measure how well our model is doing, we can do so by using a measure called perplexity - a model perplexity can be evaluated as followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_entropy(model, text, n=2):\n",
    "    H = 0.0\n",
    "    for i in range(n - 1, len(text)):\n",
    "        context, word = tuple(text[i - n + 1:i]), text[i]\n",
    "        context = \" \".join(context)\n",
    "        H += model.logprob(word, context)\n",
    "    return H / float(len(text) - (n - 1))\n",
    "\n",
    "def calc_preplexity(model, text, n=2):\n",
    "    text_entropy = model_entropy(model, text, n)\n",
    "    return 2 ** (text_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the perplexity - the better the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When testing the above implemented model on the ptb training and validation data we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptb_train_tokenized = ptb_train.split()\n",
    "ptb_test_tokenized = ptb_test.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107.32412922857847\n"
     ]
    }
   ],
   "source": [
    "# Train the ngram model with n = 3\n",
    "n = 3\n",
    "lm_MLE = train_word_lm(ptb_train_tokenized, n)\n",
    "\n",
    "print(calc_preplexity(lm_MLE, ptb_test_tokenized, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, we can always use a different estimator in order to change our model perplexity, for example, the following model is using the Lidstone estimator with a gamma instead of the MLE one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_word_lm_lidstone(dataset, n=2, gamma=0.01):\n",
    "    lidstone_estimator = lambda fd: nltk.LidstoneProbDist(fd, gamma, fd.B() + 100)\n",
    "    model = NgramModel(n, dataset, smoothing=True, estimator=lidstone_estimator)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we don't need to use the backoff technique because the lidstone estimator provides a smoothing over the probabilities - instead of using regular MLE it's like creating \"bins\" and the depositing an initial amount into each bin, and then add to that all the actual probabilities. By doing so we will never reach a point where the probability is 0 (unless gamma is 0).\n",
    "The formula it uses to do so is described as (for an experiment with count c, B bins and N outcomes) - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{align}\n",
       "\\frac{{\\mathbf{c + gamma}}}{\\mathbf{N + B * gamma}} \\\\\n",
       "\\end{align}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{align}\n",
    "\\frac{{\\mathbf{c + gamma}}}{\\mathbf{N + B * gamma}} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how selecting the size of B can change the probability - We chose this value after some trial and error with the values, intending to not create a too sparse result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the following perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131.03836275826552\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "gamma = 0.01\n",
    "lm_LIDSTONE = train_word_lm_lidstone(ptb_train_tokenized, n, gamma)\n",
    "print(calc_preplexity(lm_LIDSTONE, ptb_test_tokenized, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following graph shows how differnt gamma values in such model change the results of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gammas = np.linspace(0.01, 1, 20)\n",
    "n = 3\n",
    "perplexities_l = list(range(20))\n",
    "i = 0\n",
    "for gamma in gammas:\n",
    "    lm_LIDSTONE = train_word_lm_lidstone(ptb_train_tokenized, n, gamma)\n",
    "    perplexities_l[i] = calc_preplexity(lm_LIDSTONE, ptb_test_tokenized, n)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d6c0846d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(gammas, perplexities_l)\n",
    "plt.axis([0, 1, 0, 150])\n",
    "plt.ylabel(\"perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another way is to change the value of the n-grams , and that results in different perplexity - an example of the difference can be seen in the following graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perplexities_m = list(range(2, 20))\n",
    "gamma = 0.01\n",
    "\n",
    "for n in range(2, 20):\n",
    "    lm_LIDSTONE = train_word_lm_lidstone(ptb_train_tokenized, n, gamma)    \n",
    "    perp = calc_preplexity(lm_LIDSTONE, ptb_test_tokenized, n)\n",
    "    perplexities_m[n - 2] = perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD8CAYAAACGsIhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCSSwkJERCIpLEREJJLCQkREIv1/DzRiG15HtRYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d6f34d8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_vals = list(range(2, 20))\n",
    "\n",
    "plt.plot(n_vals, perplexities_m)\n",
    "plt.axis([2, 20, 1, 250])\n",
    "plt.ylabel(\"perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see starting with a low value of n we get really high perplexity(bad) and as we increase the value the perplexity gets better, that is until it reaches 7, that is because the ngrams are getting too large so most of them become unseen ngrams (a 7 words sentence is too long to be frequently repeated in the text) So the ideal size for n is at about n=6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the two models above and the two graphs, we can now compose the \"ideal\" model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.92074266919535\n"
     ]
    }
   ],
   "source": [
    "n = 6\n",
    "gamma = 0.2\n",
    "lm_IDEAL = train_word_lm_lidstone(ptb_train_tokenized, n, gamma)\n",
    "print(calc_preplexity(lm_IDEAL, ptb_test_tokenized, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results we get are pretty bad compared to the best models there are out today - \n",
    "for example a Neural Sequence Model with Dynamic Evaluation suggested by *Ben Krause, Emmanuel Kahembwe, Iain Murray, & Steve Renals* (can be found in https://arxiv.org/pdf/1709.07432.pdf) gets a perplexity of 46.4 on the Penn Treebank.\n",
    "<br> We do get decent results considering the fact we don't use any deep learning techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 1.3.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of using the model is by generating text using it, the following method generates text given a model and a seed(a starting prefix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(model, seed):\n",
    "    out = model.generate(seed, 100)\n",
    "    out = seed + \" \" + \" \".join(out)\n",
    "\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can note this method is not ideal if the seed length is smaller than the ngram size we used to train the model, a way to avoid such problem is to find a matching ngram starting with the seed and running the method from there ) or just making sure the seed is big enough. Another way is the issue of halting - here I gave it a number of iteration or forced it to halt when it sees the last possible ngram. \n",
    "Another issue that might arise is if the model run into an unknown history, in my case the code will break, so one must make sure no unknown history will occure - to do so we have the condition in the while loop, another way to avoid it is similar to the backoff we can also reduce the ngram size down and go back a step with the probability (selecting next word based on that). \n",
    "I made this generator limited to 100 iterations if possible because I didn't want to make it generate to big of a file given a large model, but one can easily change that number to anything. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few different examples of using the previously trained model with different seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: coupon rate on the issue\n",
      "coupon rate on the issue when it is reset is N N debenture holders also will receive the equivalent of N N of the concern in a securities and exchange commission filing norwood said it 's part of a group that holds N N of its stock in friendly hands manville is the rare publicly traded company that can ignore short-term stock fluctuations and plan for the long haul manville nyse symbol <unk> business forest products and <unk> year ended dec. N N revenue $ N million net loss $ N million N cents a share third quarter sept. N N per-share earnings N cents vs.\n"
     ]
    }
   ],
   "source": [
    "seed = lm_IDEAL.get_seed()\n",
    "print(\"SEED:\", seed)\n",
    "generate(lm_IDEAL, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: robert redford the <unk> kid\n",
      "robert redford the <unk> kid gets little respect around <unk> <unk> conversations with us mr. <unk> argued that mr. redford 's environmental views are at odds with utah residents this may have been true N years ago but times have changed even in utah mr. redford no longer stands out as an <unk> he has not changed but those around him have many of his views on the protection of <unk> areas <unk> and <unk> are now embraced by mainstream conservative <unk> recently some N environmental and <unk> groups representing such <unk> points of view as the sierra club the league of women voters and the\n"
     ]
    }
   ],
   "source": [
    "seed = lm_IDEAL.get_seed()\n",
    "print(\"SEED:\", seed)\n",
    "generate(lm_IDEAL, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: more on <unk> advertising and\n",
      "more on <unk> advertising and promotion in the second half of N usair stock could hit N says <unk> <unk> of shearson lehman hutton says if you <unk> your income to another you still have controlled its <unk> and enjoyed the <unk> of your labor even if indirectly ben earns any fees sent directly to charity and is taxable on them the irs says of course he also may take a charitable deduction for them briefs ways and means veteran <unk> d. <unk> moves to the house budget committee rep. <unk> d. md <unk> him seattle 's license fees for adult <unk> shows vary from those\n"
     ]
    }
   ],
   "source": [
    "seed = lm_IDEAL.get_seed()\n",
    "print(\"SEED:\", seed)\n",
    "generate(lm_IDEAL, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: when there are few that\n",
      "when there are few that tends to tilt the public debate toward solutions even when some of the most knowledgeable scientists are skeptical about the <unk> of the separate documents <unk> to the new york times the papers which israel says were discovered in <unk> <unk> refer to terrorist acts to be carried out in the name of a group called the revolutionary <unk> some supporters of israel say u.s. policy on palestinian terrorism is <unk> by an intense desire to maintain the dialogue with the plo but state department officials accuse israel of <unk> questionable claims to <unk> the u.s. the dollar finished lower yesterday\n"
     ]
    }
   ],
   "source": [
    "seed = lm_IDEAL.get_seed()\n",
    "print(\"SEED:\", seed)\n",
    "generate(lm_IDEAL, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: intelogic affiliate the action followed\n",
      "intelogic affiliate the action followed by one day an intelogic announcement that it will retain an investment banker to explore alternatives to maximize shareholder value including the possible sale of the financially struggling company <unk> <unk> mass. makes digital electronic <unk> instruments used by professional recording musicians it recently introduced a line for the home market however raymond c. <unk> chairman and chief executive officer noted that the new repurchase program should serve to enhance shareholder value a spokeswoman said the company will finance the buy-back with cash on hand borrowing and cash norfolk expects to generate analysts said they expected the action and investors <unk>\n"
     ]
    }
   ],
   "source": [
    "seed = lm_IDEAL.get_seed()\n",
    "print(\"SEED:\", seed)\n",
    "generate(lm_IDEAL, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, this is a deterministic method that will always result in the same (or very similar in case of an equal probabilty) results. In order to create a more random generator the Temprature parameter was introduced to the generator. \n",
    "\n",
    "Temperature sampling uses a parameter t to determine how “strong” the change will be. We use the parameter to calculate a new probability to every option,it will generally be a number between 0 and  1, and once we set it we sample with it. \n",
    "the new probability will then be defined by :\n",
    "<br>$\\tilde{p_i} = f_{\\mathcal{T}}(p)_i =  \\frac{p_i ^ {\\frac{1}{\\mathcal{T}}}}{\\sum_{j}p_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now examine this - we note that for t = 1, the probability stays identical. For t = 0.5 we are squaring the probabilities and renormalizing.\n",
    "Let's take an example of two probabilities, p1=0.6, p2=0.4, and see how this formula affects them for t=0.5 and t=0.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\tilde{p_1}_{(\\mathcal{T}=0.5)} =  \\frac{0.36}{0.52} \\sim 0.69 $\n",
    "<br>$\\tilde{p_2}_{(\\mathcal{T}=0.5)} =  \\frac{0.16}{0.52} \\sim 0.30 $\n",
    "<br>$\\tilde{p_1}_{(\\mathcal{T}=0.1)} \\sim 0.98 $\n",
    "<br>$\\tilde{p_2}_{(\\mathcal{T}=0.1)} \\sim 0.02$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we can observe how the change in t affect the result. A lower t value gives the \"stronger\" values an even higher probability, and when t approaches 0 the strongest option is becoming certain, effectively going back to deterministic input.\n",
    "We can use the temperature parameter to express how much we ‘trust’ the model, and to add controlled amounts of randomness to our generations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at the code by Sameer Sing. He chose to work with log probabilities in order to achieve the same results.\n",
    "The first part of his code does two things:\n",
    "- Create a list called ‘wps’. Each element in his list is a pair of a word w, and the log probability of w divided by temp $\\frac{\\log (p(w))}{t}$.\n",
    "- Sums the total of the $\\frac{\\log (p(w))}{t}$ values, so he can use it as a normalizing factor. By the time the loop finishes, tot is equal to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tot = $\\log_2(2^{\\frac{\\log (p(w_1)}{t}} + 2^{\\frac{\\log (p(w_2)}{t}}+ \\dots + 2^{\\frac{\\log (p(w_n))}{t}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after he assigned weighted log probabilities, it’s time to choose a random word based on the new probability. This is how it's done:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first a random number p $(0<p<1)$ is chosen. Now for every possible word, he perform the following steps:\n",
    "- Sum the logs probabilities so far $s$.\n",
    "- Subtract $s$ from the total we counted in the first part, and calculate $2^{ourResult}$.\n",
    "- if $p$ is smaller than the answer, the word is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand why it works, lets continue with our example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_1 = -1.473$ . This is the only log we’ve encountered yet, so $s = p_1$.\n",
    "<br>$2^{s-tot} = 0.69$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can note this is the __exact same probability we calculated in the first part.__\n",
    "How did this happen?\n",
    "- In the first part we computed $p^{\\frac{1}{t}}$\n",
    "- In the second part we computer $2^{\\log \\frac{p}{t}}$\n",
    "<br>But those expressions are equivalent :\n",
    "$2^{\\frac{\\log_2{p}}{t}} = 2^{\\log_2{p} * \\frac{1}{t}} = p^{\\frac{1}{t}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same applies to the normalizing sum below. And obviously, our random number p has a probability of 0.69 to be smaller than 0.69, hence giving the same word an 0.69 chance to be chosen.\n",
    "So as we see, the code by Sameer Sing effectively uses the exact same mathematical idea of temperature expressed by Russel Stewart, but implemented using logs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were asked to read and summarize two articles about RNNs. \n",
    "The first article, by Andrej Karpathy, provides a general introduction and description of RNNs. Here is the summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Unlike normal Neural Networks, Recurrent Neural Networks (RNNs) allow us to operate over __sequences of vectors.__, and not just one. Considering it is still a Neural Network, it operates similarly to any other - it has a “step” function that gets an input vector x and returns an output vector y. But RNN's have one major difference – they keep a hidden vector h, whose value are updated in every computation and are used to generate the output. So the output depends not only on the current input vector, but also on the entire history of vectors it ever computed.\n",
    "\n",
    "More precisely, the RNN keeps three matrices: W_xh (input to hidden),  W_hh (hidden to hidden), W_hy (hidden to output). We first initialized them to random matrices and every time “step” is called, we use them to update h and calculate the output.\n",
    "\n",
    "When constructing a language char-based model using RNN, Karpathy gets pretty good results (even for surprising things like Linux source code). In order to better understand the process, Karpathy samples it in different stages to see how it “learns”. In the first iterations it composes gibberish, by the 700 iteration it learns words, and by 2000 it can write full complex sentences. Karpathy also visualize values in neurons, and observe how certain neurons are used for certain tasks, even though it wasn’t coded by humans – the network ‘decided’ upon it.\n",
    "\n",
    "RNN’s are a growing field that is widely used for models of NLP, Computer Vision and other fields.\n",
    "They still have flaws (like being good at memorizing but not always in generalization, or using a very large computations in each step), but there is a lot of progress and they are evolving daily\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second article, by Yoav Goldberg, discusses using char-based language models for the same task, and discusses the differences between the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Replying to the previous article, Goldberg points out that text generation can also be achieved in a much simpler method: Unsmoothed Maximum Likelihood Character Level Language Model. This is a simple model that “learns” by reading texts. It has a fixed parameter n called “order”, and for every string of n characters the model encounters, it keeps a count of what the next character was. Hence, for a combination of n chars (ngram), the model can give us a distribution of the probability of the next char, and answer about the likelihood of a specific char to be next. If the model never saw a character after a given ngram, the probability will be 0.\n",
    "\n",
    "After training the model on Shakespeare texts, we now test it in text generation. Lower orders are gibberish, but the higher the order is, the results are getting better and are looking more like convincing Shakespeare text. However, this model has limitations – it isn't aware of context, or anything beyond the last n characters. For example, when generating Linux code, the RNN model is able to generate code that is well indented, with correctly nested brackets. The language model isn’t capable of doing that, even when choosing high orders. Of course, it’s possible to update the model to support it; but what’s so impressive about RNN is, it learned to do it on it’s own, without being specifically coded to do so\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, it's time to start working on the recipe databse. We'll be using Yoav Goldberg's model, adapted to Python3 and NLTK, with Lindstone estimator in order to avoid zero probabilites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lm_Lidstone(data, order=4, gamma = 0.01):\n",
    "    pad = '*' * order\n",
    "    data = pad + data\n",
    "    cfd = nltk.ConditionalFreqDist((data[i : i + order], data[i + order]) for i in range(len(data) - order))\n",
    "    cpd = nltk.ConditionalProbDist(cfd, nltk.LidstoneProbDist, gamma, bins=1000)\n",
    "    return cpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to use new model and train it on the cooking recipes.\n",
    "First we need to gather the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-11-27 23:52:00--  http://www.ffts.com/recipes/lg/lg32965.zip\n",
      "Resolving www.ffts.com (www.ffts.com)... 66.96.149.1\n",
      "Connecting to www.ffts.com (www.ffts.com)|66.96.149.1|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11805248 (11M) [application/zip]\n",
      "Saving to: ‘lg32965.zip’\n",
      "\n",
      "lg32965.zip         100%[===================>]  11.26M  2.63MB/s    in 5.0s    \n",
      "\n",
      "2018-11-27 23:52:06 (2.25 MB/s) - ‘lg32965.zip’ saved [11805248/11805248]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-272eda9aab10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lg32965.zip\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recipes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfile_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recipes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1556\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m\"\"\"copy data from file-like object fsrc to file-like object fdst\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mZIP_DEFLATED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMIN_READ_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m             self._eof = (self._decompressor.eof or\n\u001b[1;32m    927\u001b[0m                          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!wget http://www.ffts.com/recipes/lg/lg32965.zip\n",
    "\n",
    "with zipfile.ZipFile(\"lg32965.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"recipes\")\n",
    "\n",
    "file_list = os.listdir(\"recipes\")\n",
    "enc = 'iso-8859-15'\n",
    "recipes = \"\"\n",
    "for file in file_list:\n",
    "    recipes_from_file = open(\"recipes/\" + file, 'r', encoding=enc).read()\n",
    "    recipes_from_file.translate(string.punctuation)\n",
    "    recipes += recipes_from_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to gather some statistics on them. We'll split the file to recipes using the string \"MMMMM----- Recipe via Meal-Master (tm) v8.05\", which seperates strings in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(recipes):\n",
    "    #(number of recipes, tokens, characters, vocabulary size, \n",
    "    #distribution of the size of recipes in words and in chars, distribution of length of words).\n",
    "    recipes_list = recipes.split(\"MMMMM----- Recipe via Meal-Master (tm) v8.05\")[1:]\n",
    "    all_tokens = word_tokenize(recipes)\n",
    "    print (\"number of recipes:\", len(recipes_list))\n",
    "    print (\"number of tokens:\", len(all_tokens))\n",
    "    print (\"number of characters:\", len (recipes))\n",
    "    print (\"size of vocabulary:\", len (set(all_tokens)))\n",
    "    print (\"distribution of size of recipes in chars:\")\n",
    "    print_char_dist (recipes_list)\n",
    "    print (\"distribution of size of recipes in tokens:\")\n",
    "    print_words_dist (recipes_list)\n",
    "    print (\"distribution of length of words:\")\n",
    "    print_words_length (all_tokens)\n",
    "    \n",
    "def print_char_dist (recipes_list):\n",
    "    recipes_chars_len = [len(i) for i in recipes_list]\n",
    "    plt.hist(recipes_chars_len, bins=100)\n",
    "    plt.xlabel (\"Number of chars in recipe\")\n",
    "    plt.ylabel (\"Amount\")\n",
    "    plt.show()\n",
    "    \n",
    "def print_words_dist (recipes_list):\n",
    "    tokened_recipes = [word_tokenize(r) for r in recipes_list]\n",
    "    recipes_words_len = [len(i) for i in tokened_recipes]\n",
    "    plt.hist(recipes_words_len, bins=100)\n",
    "    plt.xlabel (\"Number of tokens in recipe\")\n",
    "    plt.ylabel (\"Amount\")\n",
    "    plt.show()\n",
    "\n",
    "def print_words_length (all_tokens):\n",
    "    words_len = [len(i) for i in all_tokens]\n",
    "    plt.hist(words_len, bins=100)\n",
    "    plt.xlabel (\"Word length\")\n",
    "    plt.ylabel (\"Amount\")\n",
    "    plt.show()\n",
    "    \n",
    "get_stats(recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the one long word in the recipes dataset is a string of 70 \"+\" characters in a row, used as a delimiter in the text.\n",
    "\n",
    "Our next step will be to split the dataset. The Python interface chosen is a dictionary with three keys: train, dev and test, so we can easily access every set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data (recipes):\n",
    "    #returns a dictionary of train, dev, test\n",
    "    eighty_per = round(len(recipes) * 0.8)\n",
    "    ten_per = round(len(recipes) * 0.1)\n",
    "    return {\n",
    "        \"train\": recipes[:eighty_per], \n",
    "        \"dev\": recipes[eighty_per : eighty_per + ten_per],\n",
    "        \"test\": recipes[eighty_per + ten_per:]\n",
    "    }\n",
    "\n",
    "sets = split_data(recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model on our data. We'll try with order 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm4 = create_lm_Lidstone(sets.get(\"train\"), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to modify our perplexity method from earlier to fit a character model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def char_model_entropy(model, text, n=2):\n",
    "    H = 0.0\n",
    "    for i in range(n - 1, len(text)):\n",
    "        context, char = text[i - n:i], text[i]\n",
    "        H += model[context].logprob(char)\n",
    "    return -(H / (len(text) - (n - 1)))\n",
    "\n",
    "def calc_perplexity_char(model, text, n=2):\n",
    "    text_entropy = char_model_entropy(model, text, n)\n",
    "    return 2 ** (text_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the perplexity of such model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.986077705910871"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_perplexity_char(lm4, sets.get(\"dev\"), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we observe that the perplexity is very low. Let's try other orders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm7 = create_lm_Lidstone(sets.get(\"train\"), 7)\n",
    "calc_perplexity_char(lm7, sets.get(\"dev\"), 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm10 = create_lm_Lidstone(sets.get(\"train\"), 10)\n",
    "calc_perplexity_char(lm10, sets.get(\"dev\"), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for order 7 the results are still low. For order 10, it gets a big higher, but still very low compared to the values we've witnessed at the previous part. How is it possible? Let's examine the probabilities (rather than the log probabilities) and calculate their average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_average_prob (model, text, n, isWordsModel):\n",
    "    s = 0\n",
    "    for i in range(n - 1, len(text)):\n",
    "        context, char = text[i - n:i], text[i]\n",
    "        if isWordsModel:\n",
    "            context = \" \".join(context)\n",
    "        s += model[context].prob(char)\n",
    "    return (s / (len(text) - (n - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"order 4:\", get_average_prob(lm4, sets.get(\"dev\"), 4, False))\n",
    "print(\"order 7:\", get_average_prob(lm7, sets.get(\"dev\"), 7, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in both cases, the average probability is very high. For comparison, in the example we've discussed in previous questions (based on ptb model), the average (after smoothing) was about 0.06. There are two possible reasons for it:\n",
    "\n",
    "-The recipe database is a lot easier to predict. It is limited to one subject and one types of texts, all of whom are formatted similarly and using similar expressions. The text isn't diverse, so it's easier to predict.\n",
    "\n",
    "-The use of a character-based model, rather than a words-based model, might cause lower perplexity - there are less options to predict each way.\n",
    "\n",
    "So which of those observations are correct? Let's try and test the PTB dataset on a character model, and the recipes dataset on a word model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptb_char = create_lm_Lidstone (ptb_train, 4)\n",
    "print(\"ptb perplexity:\", calc_perplexity_char(ptb_char, ptb_test, 4))\n",
    "print(\"ptb average probability:\", get_average_prob(ptb_char, ptb_test, 4, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = word_tokenize(sets.get(\"train\"))\n",
    "dev_tokenized = word_tokenize(sets.get(\"dev\"))\n",
    "recipes_words_lm = train_word_lm_lidstone(train_tokenized, 5)\n",
    "print(\"recipes word perplexity:\", calc_preplexity(recipes_words_lm, dev_tokenized, 5))\n",
    "print(\"recipes word average probability:\", get_average_prob(recipes_words_lm._probdist, dev_tokenized, 4, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our conclusion is: it seems that for the character model, the results for the ptb dataser are similar to the recipes database - a very low perplexity, and a very high average probability (0.5 is a bit lower than 0.57, perhaps because of the subject variety in ptb, but still relatively very high).\n",
    "\n",
    "For the word model, we get a perplexity of 36 - which is much lower than the perplexities ftb had (around 100), but also much higher than the word models perplexities. The average probability is also between them - 0.2. It seems that the recipes dataset is indeed less varied (and hence easier to guess), but the deciding factor seems to be the type of model - character based models generally seem to have much lower perplexity.\n",
    "\n",
    "Now we'll generate a few samples from those langauge models, and see how they turn out. We'll be using different orders to see which one brings the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_letter(lm, history, order):\n",
    "    history = history[-order:]\n",
    "    return lm[history].generate()\n",
    "     \n",
    "def generate_text(lm, order, nletters=1000):\n",
    "    history = \"~\" * order\n",
    "    out = []\n",
    "    for i in range(nletters):\n",
    "        c = generate_letter(lm, history, order)\n",
    "        history = history[-order:] + c\n",
    "        out.append(c)\n",
    "    return \"\".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(lm4, 4, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(generate_text(lm7, 7, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(lm7, 7, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(lm10, 10, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(lm10, 10, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few observations about those generations:\n",
    "\n",
    "- All models create mostly existing, and relevant words. Order 4 occasionally creates gibberish words (such as \"DIPPERSON\"), but this seems to become much rarer in orders 7 and 10.\n",
    "\n",
    "- The recipe delimiter (\"MMMMM...\") appears frequently in generations. Sometimes the complete phrase (including \"Recipe via...\") appears too. In order 7, the delimiter seems to appear fairly randomly. However, in order 10, his positions makes sense: it appears after the end of a recipe (after \"Posted by\", \"Courtesy of\") and immediately followed by the beginning of a new recipe (\"Title: \").\n",
    "\n",
    "- Both order 10 and order 7 seems to keep some basic formatting. \"Title:\" will be followed by an uppercase recipe name, \"Categories:\" will be followed by a list of one or more categories, separated by commas, where every item begins with a cpital letter. \"Yield:\" is always followed by the number of servings.\n",
    "\n",
    "- List of ingredients seems to somewhat stick together. The formatting remains: number, unit, and name of ingredient, and usually several of those lines appear in a row. However, all ingredients lists seem to suffer from fairly random indentations. \n",
    "\n",
    "- It seems that only order 10 is capable of forming full sentences that makes sense, such as \"Add cumin and other ingredients. Roll out again to boil\". Credit lines seem to be written correctly (\"From an article by Jay Harlow, The San Francisco Chronicle Typed by Bob Stein\"), presumably because they always appear in the same formatting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to generate a dataset of N points for some function y(x).\n",
    "We will start by writing a function that generates x and t such that - t_i = y(x_i) + N(mu, sigma):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataset(N, f, sigma):\n",
    "    mu = 0\n",
    "    s = np.array(np.random.normal(mu, sigma, N))\n",
    "    x = np.array(np.linspace(0.0, 1.0, N))\n",
    "    vf = np.vectorize(f)\n",
    "    t = np.add(vf(x), s)\n",
    "    return (x, t)\n",
    "\n",
    "sigma = 0.03\n",
    "def f(x): return math.sin(2 * math.pi * x)\n",
    "N = 100\n",
    "\n",
    "x, t = generateDataset(N, f, sigma)\n",
    "plt.scatter(x, t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to learn the function y by using x and t using a least squares estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OptimizeLS(x, t, M):\n",
    "    phi = np.vstack([np.power(x_i, m) for m in range(M)] for x_i in x)\n",
    "    prod = np.dot(phi.T, phi)\n",
    "    i = np.linalg.inv(prod)\n",
    "    phi_mults = np.dot(i, phi.T)\n",
    "    w = np.dot(phi_mults, t)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we shall test it using the sin function from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyXW (x, w):\n",
    "#given a vector w and a number x, computes y(x) = w0 + w1x + w2x2 + ... + wMxM\n",
    "    sum = w[0]\n",
    "    for i in range (1,len(w)):\n",
    "        sum+= (pow(x,i) * w[i])\n",
    "    return sum\n",
    "\n",
    "sigma = 0.03\n",
    "N = 10\n",
    "def f(x): return math.sin(2 * math.pi * x)\n",
    "\n",
    "x, t = generateDataset(N, f, sigma)\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.subplot(5 ,1, 1)\n",
    "plt.scatter(x, t)\n",
    "\n",
    "M = [1, 3, 5, 10]\n",
    "j = 2\n",
    "for m in M:\n",
    "    w = OptimizeLS(x, t, m)\n",
    "    res =  [applyXW(xi,w) for xi in x]\n",
    "    subp = plt.subplot(5, 1, j)    \n",
    "    j += 1\n",
    "    subp.set_title('M = {}'.format(m))\n",
    "    subp.scatter(x, res)\n",
    "plt.tight_layout(pad=0.5, w_pad=0.5, h_pad=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of least squares we would like to estimate y using polynomial curve fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizePLS(x, t, M, l):\n",
    "    phi = np.vstack([np.power(x_i, m) for m in range(M)] for x_i in x)\n",
    "    prod = np.dot(phi.T, phi)\n",
    "    l_i = np.dot(l, np.identity(prod.shape[0]))\n",
    "    fixed = np.add(prod, l_i)\n",
    "    i = np.linalg.inv(fixed)\n",
    "    phi_mults = np.dot(i, phi.T)\n",
    "    w = np.dot(phi_mults, t)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will test our function with a random value of lambda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.03\n",
    "N = 10\n",
    "def f(x): return math.sin(2 * math.pi * x)\n",
    "\n",
    "x, t = generateDataset(N, f, sigma)\n",
    "M = 5\n",
    "l = 0.05\n",
    "w = optimizePLS(x, t, M, l)\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.subplot(2 ,1, 1)\n",
    "plt.scatter(x, t)\n",
    "\n",
    "res = applyXW (x, w)\n",
    "subp = plt.subplot(2, 1, 2)\n",
    "subp.scatter(x, res)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, adding the lambda didn't result in the best outcome- \n",
    "We want to optimize the lambda value, to do so we will firstly need to construct a training set, a dataset and a validation set in order to optimize it. \n",
    "to do so we will extend the function we created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generateDataset3(N, f, sigma):\n",
    "    mu = 0\n",
    "    s = np.array(np.random.normal(mu, sigma, N))\n",
    "    \n",
    "    x_1 = np.array(np.linspace(0.0, 1.0, N))\n",
    "    x_2 = np.copy(x_1)\n",
    "    x_3 = np.copy(x_1)\n",
    "    np.random.shuffle(x_2)\n",
    "    np.random.shuffle(x_3)\n",
    "    \n",
    "    vf = np.vectorize(f)\n",
    "    \n",
    "    t_1 = np.add(vf(x_1), s)\n",
    "    t_2 = np.add(vf(x_2), s)\n",
    "    t_3 = np.add(vf(x_3), s)\n",
    "    \n",
    "    return [(x_1, t_1), (x_2, t_2), (x_3, t_3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and testing this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.03\n",
    "N = 100\n",
    "def f(x): return math.sin(2 * math.pi * x)\n",
    "\n",
    "res = generateDataset3(N, f, sigma)\n",
    "x_1, t_1 = res[0]\n",
    "x_2, t_2 = res[1]\n",
    "x_3, t_3 = res[2]\n",
    "plt.scatter(x_1, t_1)\n",
    "plt.scatter(x_2, t_2)\n",
    "plt.scatter(x_3, t_3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to estimate the error of a model from the real thing, so we define the normalized error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalized_error (x,t,w):\n",
    "    n = len(x)\n",
    "    sig = 0\n",
    "    for i in range(n):\n",
    "        sig+= (t[i] - applyXW(x[i],w))**2\n",
    "    return (1/n) * sig**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for example, for the previously generates results we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sigma = 0.03\n",
    "N = 100\n",
    "\n",
    "x, t = generateDataset(N, f, sigma)\n",
    "M = 5\n",
    "l = 0.05\n",
    "w = optimizePLS(x, t, M, l)\n",
    "\n",
    "print(normalized_error(x, t, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanna show how different lambda values will give different result - the following graph plots will show the normalized error on different lambda value for all x types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.03\n",
    "def f(x): return math.sin(2 * math.pi * x)\n",
    "N = 10\n",
    "\n",
    "res = generateDataset3(N, f, sigma)\n",
    "x_test, t_test = res[0]\n",
    "x_validate, t_validate = res[1]\n",
    "x_train, t_train = res[2]\n",
    "\n",
    "def run_lambdas(x_train, t_train, x_test, t_test, x_validate, t_validate):\n",
    "    j = 0\n",
    "    normalized_test = np.zeros(20)\n",
    "    normalized_train = np.zeros(20)\n",
    "    normalized_validate = np.zeros(20)\n",
    "    for i in range(-40, -20):\n",
    "        l = np.exp(i)\n",
    "        w_test = optimizePLS(x_train, t_train, M, l)\n",
    "        normalized_test[j] = normalized_error(x_test, t_test, w_test)\n",
    "        normalized_validate[j] = normalized_error(x_validate, t_validate, w_test)\n",
    "        normalized_train[j] = normalized_error(x_train, t_train, w_test)\n",
    "        j += 1\n",
    "    return [normalized_test, normalized_train, normalized_validate]\n",
    "\n",
    "def run(N, sigma, f):\n",
    "    res = generateDataset3(N, f, sigma)\n",
    "    x_test, t_test = res[0]\n",
    "    x_validate, t_validate = res[1]\n",
    "    x_train, t_train = res[2]\n",
    "    \n",
    "    return run_lambdas(x_train, t_train, x_test, t_test, x_validate, t_validate)\n",
    "\n",
    "sigma = 0.03\n",
    "N = 10\n",
    "\n",
    "res = run(N, sigma, f)\n",
    "n_tst5, n_trn5, n_vld5 = res[0], res[1], res[2]\n",
    "\n",
    "N = 100\n",
    "res = run(N, sigma, f)\n",
    "n_tst10, n_trn10, n_vld10 = res[0], res[1], res[2]\n",
    "\n",
    "plt.figure(num = 2, figsize=(10, 10))\n",
    "\n",
    "a_tst = plt.subplot(2 ,3, 1)\n",
    "a_tst.set_title('N = 10, testing set', y=1.08)\n",
    "a_tst.plot(n_tst5)\n",
    "a_v = plt.subplot(2 ,3, 2)\n",
    "a_v.set_title('N = 10,validtion set', y=1.08)\n",
    "a_v.plot(n_vld5)\n",
    "a_t = plt.subplot(2 ,3, 3)\n",
    "a_t.set_title('N = 10,training set', y=1.08)\n",
    "a_t.plot(n_trn5)\n",
    "\n",
    "a_tst_b = plt.subplot(2 ,3, 4)\n",
    "a_tst_b.set_title('N = 100, testing set', y=1.08)\n",
    "a_tst_b.plot(n_tst10)\n",
    "a_v_b = plt.subplot(2 ,3, 5)\n",
    "a_v_b.set_title('N = 100,validtion set', y=1.08)\n",
    "a_v_b.plot(n_vld10)\n",
    "a_t_b = plt.subplot(2 ,3, 6)\n",
    "a_t_b.set_title('N = 100,training set', y=1.08)\n",
    "a_t_b.plot(n_trn10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create w based on an optimized lambda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w_error_pair (xt, tt, xv, tv, M, lamb):\n",
    "#returns a pair of vector w and his normalized_error\n",
    "    w = optimizePLS (xt,tt,M,lamb)\n",
    "    return (w, normalized_error (xv,tv,w))\n",
    "\n",
    "def optimizePLSLambda(xt, tt, xv, tv, M):\n",
    "    all_lambdas = [get_w_error_pair (xt, tt, xv, tv, M, (np.exp(i))) for i in range (-40, -20)]\n",
    "    errors = [x[1] for x in all_lambdas]\n",
    "    pos = errors.index(min(errors))\n",
    "    return all_lambdas[pos][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and testing it on the same example from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.03\n",
    "def f(x): return math.sin(2 * math.pi * x)\n",
    "N = 10\n",
    "\n",
    "res = generateDataset3(N, f, sigma)\n",
    "x_test, t_test = res[0]\n",
    "x_validate, t_validate = res[1]\n",
    "x_train, t_train = res[2]\n",
    "\n",
    "plt.figure(num = 3, figsize=(8, 10))\n",
    "origin = plt.subplot(2 ,1, 1)\n",
    "origin.scatter(x_test, t_test)\n",
    "\n",
    "M = 5\n",
    "w_res = optimizePLSLambda(x_train, t_train, x_validate, t_validate, M)\n",
    "res = np.zeros(len(t_test))\n",
    "for i in range(len(w_res)):\n",
    "    res = res + np.dot(w_res[i], np.power(x_test, i))\n",
    "subp = plt.subplot(2, 1, 2)\n",
    "subp.scatter(x_test, res)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bayesianEstimator(x, t, M, alpha, sigma2):\n",
    "    N = len(x)\n",
    "    \n",
    "    #builds phi for given vector xx\n",
    "    def phi(xx):\n",
    "        return np.array([(xx ** i) for i in range(M + 1)])\n",
    "    \n",
    "    #calculate S\n",
    "    alpha_i = alpha * np.eye(M + 1)\n",
    "    S = np.zeros((M + 1, M + 1))\n",
    "    for i in range(N):\n",
    "        phi_xi = phi(x[i])\n",
    "        S += np.outer(phi_xi, phi_xi.T)\n",
    "    S = np.linalg.inv(alpha_i + (S / sigma2))\n",
    "    \n",
    "    #calculate m(x)\n",
    "    def m(xx):\n",
    "        phi_t = phi(xx).T\n",
    "        xt_sum = np.zeros(M + 1)\n",
    "        for i in range(N):\n",
    "            xt_sum += phi(x[i]) * t[i]\n",
    "        return (1 / sigma2) * np.dot(np.dot(phi_t, S), xt_sum)\n",
    "    \n",
    "    #calculate s2(xx)\n",
    "    def var(xx):\n",
    "        phi_x = phi(xx)\n",
    "        return sigma2 + np.dot(phi_x.T, np.dot(S, phi_x))\n",
    "    \n",
    "    mean = lambda x_t: m(x_t)\n",
    "    variance = lambda x_t: var(x_t)\n",
    "    \n",
    "    return (mean, variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.005\n",
    "sigma2 = 1/11.1\n",
    "M = 9\n",
    "def f(x): return math.sin(2 * math.pi * x)\n",
    "vf = np.vectorize(f)\n",
    "N = 10\n",
    "\n",
    "res = generateDataset(N, f, sigma)\n",
    "x_t, t_t = res\n",
    "\n",
    "m, var = bayesianEstimator(x_test, t_test, M, alpha, sigma2)\n",
    "\n",
    "plt.figure(num = 10, figsize=(6, 6))\n",
    "upperBound = np.vectorize(lambda x: m(x) + np.sqrt(var(x)))\n",
    "lowerBound = np.vectorize(lambda x: m(x) - np.sqrt(var(x)))\n",
    "plt_graph = plt.subplot(111)\n",
    "plt_graph.fill_between(x_t, upperBound(x_t), lowerBound(x_t), alpha=0.3, color='r')\n",
    "plt_graph.scatter(x_t, t_t, edgecolor='b', facecolor='none', marker='o', s=60, lw=2)\n",
    "plt_graph.plot(x_t, m(x_t), lw=2, color='g')\n",
    "plt_graph.plot(x_t, vf(x_t), lw=2, color='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
