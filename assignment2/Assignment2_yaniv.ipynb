{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the solution to HW2, written by Yaniv Bin and Tair Hakman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first would like to import all the required modules in order for our code to run properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "import os.path\n",
    "from os.path import abspath, dirname, join\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import gzip, string, nltk\n",
    "from nltk.corpus import conll2002\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.externals.six.moves import html_parser\n",
    "from sklearn.externals.six.moves.urllib.request import urlretrieve\n",
    "from sklearn.datasets import get_data_home\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "import re, os, sys, itertools, tarfile\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from collections import Counter\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first task is identifying complex words. In order to do so we will use the data provided. \n",
    "first we need to load the data using the provided functions. \n",
    "<br>(For a better flow we modified the skeleton code within the notebook, but we added the edit into the skeleton file as well) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_file(data_file):\n",
    "    words = []\n",
    "    labels = []   \n",
    "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i > 0:\n",
    "                line_split = line[:-1].split(\"\\t\")\n",
    "                words.append(line_split[0].lower())\n",
    "                labels.append(int(line_split[1]))\n",
    "            i += 1\n",
    "    return words, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_file = abspath(join(dirname(\"__file__\"), \"data/complex_words_training.txt\"))\n",
    "development_file = abspath(join(dirname(\"__file__\"), \"data/complex_words_development.txt\"))\n",
    "test_file = abspath(join(dirname(\"__file__\"), \"data/complex_words_test_unlabeled.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data we may start working on the actual assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 - Evaluation Matrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually in order to evaluate how well a classifcation algorithm works we use three measures - \n",
    "- Precision: Measure how 'useful' the results are (how many are hits and how many are miss)\n",
    "- Recall: Measure how 'complete' the results are (out off all the actually possible results, how many did we hit)\n",
    "- Fscore: Measure the balance between the Precision and Recall\n",
    "\n",
    "We would like to implement functions that compose each of these matrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by calculating the precision - the precision is defined as:\n",
    "<br>$precision = \\frac{tp}{tp + fp}$\n",
    "<br>Where $tp$ stands for true-positive meaning a hit, and $fp$ stands for false-positive meaning the prediction is positive but it's actually a false alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Input: y_pred, a list of length n with the predicted labels,\n",
    "## y_true, a list of length n with the true labels\n",
    "\n",
    "## Calculates the precision of the predicted labels\n",
    "def get_precision(y_pred, y_true):\n",
    "    positive_hits_array = np.array([(1 if(y_pred[i] == y_true[i] == 1) else 0) for i in range(len(y_pred))])\n",
    "    positive_hits = np.count_nonzero(positive_hits_array == 1)\n",
    "    total_positive = np.count_nonzero(np.array(y_pred) == 1)\n",
    "    precision = float(positive_hits) / float(total_positive)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [0, 0, 1, 1]\n",
    "get_precision(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second measure we will implement is the recall, calculated as follows\n",
    "<br>$recall = \\frac{tp}{tp + fn}$\n",
    "<br>Where $fn$ stands for false-negative, meaning the prediction is negative also it should have been positive(a miss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Calculates the recall of the predicted labels\n",
    "def get_recall(y_pred, y_true):\n",
    "    positive_hits_array = np.array([(1 if(y_pred[i] == y_true[i] == 1) else 0) for i in range(len(y_pred))])\n",
    "    positive_hits = np.count_nonzero(positive_hits_array == 1)\n",
    "    total_positive = np.count_nonzero(np.array(y_true) == 1)\n",
    "    recall = float(positive_hits) / float(total_positive)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [1, 0, 1, 1]\n",
    "get_recall(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the last measure we want to implement is the fscore(also known as f1) which is calculated as:\n",
    "<br>$F = 2 * \\frac{precision*recall}{precision + recall}$\n",
    "<br> F score measure the relation between the precision and recall, the results vary between 0 and 1, but once it's equal 1 that means the relation is exact meaning the classifier is perfect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Calculates the f-score of the predicted labels\n",
    "def get_fscore(y_pred, y_true):\n",
    "    precision = get_precision(y_pred, y_true)\n",
    "    recall = get_recall(y_pred, y_true)\n",
    "    fscore = 2 * float(precision * recall) / float(precision + recall)\n",
    "    return fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [1, 0, 1, 1]\n",
    "get_fscore(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our own good we would also like to implement a function that prints out all the information given the two arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_predictions(y_pred, y_true):\n",
    "    print(\"Precision:\", get_precision(y_pred, y_true))\n",
    "    print(\"Recall:\", get_recall(y_pred, y_true))\n",
    "    print(\"Fscore:\", get_fscore(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5\n",
      "Recall: 0.3333333333333333\n",
      "Fscore: 0.4\n"
     ]
    }
   ],
   "source": [
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [1, 0, 1, 1]\n",
    "test_predictions(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after implementing the functions, we can go on and implement actual classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 - Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2.1 - All complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classifier we will implement is a very simple one that classifies all the words as complex no matter what they actually are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Labels every word complex\n",
    "def all_complex(data_file):\n",
    "    words, actual_labels = load_file(data_file)\n",
    "    all_complex_labels = np.ones((len(words),), dtype=int)\n",
    "    precision = get_precision(all_complex_labels, actual_labels)\n",
    "    recall = get_recall(all_complex_labels, actual_labels)\n",
    "    fscore = get_fscore(all_complex_labels, actual_labels)\n",
    "    performance = [precision, recall, fscore]\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we shall test it with each of the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Precision: 0.43275 \n",
      "Training Recall: 1.0 \n",
      "Training Fscore: 0.604083057058105\n"
     ]
    }
   ],
   "source": [
    "tr_precision, tr_recall, tr_fscore = all_complex(training_file)\n",
    "print(\"Training Precision: {} \\nTraining Recall: {} \\nTraining Fscore: {}\".format(tr_precision, tr_recall, tr_fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Precision: 0.418 \n",
      "Dev Recall: 1.0 \n",
      "Dev Fscore: 0.5895627644569816\n"
     ]
    }
   ],
   "source": [
    "dv_precision, dv_recall, dv_fscore = all_complex(development_file)\n",
    "print(\"Dev Precision: {} \\nDev Recall: {} \\nDev Fscore: {}\".format(dv_precision, dv_recall, dv_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can note how the recall of this model is always 1 - that makes sense because although it probably has a lot of false positives - it never \"miss\" in terms of false negtive because it's always positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2.2 - word length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second baseline we will implement is a word length based one, which gives a positive value to a word if it's length goes past a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1.2.2: Word length thresholding\n",
    "def word_length_baseline(data_file, threshold):\n",
    "    words, actual_labels = load_file(data_file)\n",
    "    threshold_labels = [(1 if(len(word) >= threshold) else 0) for word in words]\n",
    "    \n",
    "    precision = get_precision(threshold_labels, actual_labels)\n",
    "    recall = get_recall(threshold_labels, actual_labels)\n",
    "    fscore = get_fscore(threshold_labels, actual_labels)\n",
    "    preformance = [precision, recall, fscore]\n",
    "    return preformance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to run this for both the training dataset and the development dataset for different threshold values, and plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAAJwCAYAAADWXSa8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3Xl8VOXZ//HPlZ0shCUB2XdEUBCM\nuKAorqhVfCq2orZi3ZdqbX9trV3sYzdrF59accG1te5aFetKFcUFhICKLIIhCAQQwk4SyHr9/pgD\nDjGQQTKZyeT7fr3mlZxz7jNzTY/QL/d97vuYuyMiIiIi8Skp1gWIiIiIyJ4prImIiIjEMYU1ERER\nkTimsCYiIiISxxTWREREROKYwpqIiIhIHFNYE5GEY2YXmNnrEbS7x8x+2Rw1RYuZuZn1D35/2Mx+\nG+uaRKRppcS6ABGRpubujwKPRtDuyqb8XDObDBS6++SmfF8Rad3UsyYiccnMWuI/JscCL9ff2UK/\ni4jECYU1EWlWZva5mf3MzBaa2SYze8jMMszseDMrMbOfmtkXwENB+2+Y2UdmttnM3jezoWHv1cPM\n/m1mpWa2wczuDPZPNLN3g9/NzG43s3VmtsXM5pnZwcGx3YYNzewyMysys41mNsXMuoYdczO70sw+\nC+qeZGYWdnwosNndS4LPfy/43I3Ar4M23zOzRcH5r5lZr7Dzh5jZ1OCz15rZTcH+kWY2I/j+a8zs\nTjNLi8a1EZH4pLAmIrFwAXAq0A8YCPwi2H8A0AHoBVxuZiOAB4ErgI7AvcAUM0s3s2TgP8ByoDfQ\nDXiigc86BRgdfE474NvAhvqNzOwE4A/At4AuwfvWf79vAIcDw4J2p4YdOx14KWz7CKAY6AT8zszO\nBm4CvgnkA+8AjwefnQP8F3gV6Ar0B94I3qcWuAHIA44CTgSubuB7ikiCUlgTkVi4091XuvtG4HfA\nhGB/HXCzu1e6+3bgMuBed//A3Wvd/R9AJXAkMJJQsPmxu5e7+w53f7eBz6oGcoBBgLn7Indf00C7\nC4AH3X2uu1cCPwOOMrPeYW1udffN7r4CmAYcGnbsDHYfAl3t7n9395rgu1wB/CH4/Brg98ChQe/a\nN4Av3P0vwffY5u4fALj7HHefGbzP54QC63GN/Q8sIolDYU1EYmFl2O/LCYUugFJ33xF2rBfwo2AI\ncLOZbQZ6BO17AMuD4LNH7v4mcCcwCVhrZpPNrG0DTbsGtew8r4xQD1y3sDZfhP1eAWQDmFk7QmHw\n/T18x53f5W9h32MjYMH79wCWNlS/mQ00s/+Y2RdmtpVQyMvb23cWkcSisCYisdAj7PeewOrgd6/X\nbiXwO3dvF/bKdPfHg2M9I7l5393vcPfDgCGEhkN/3ECz1YQCFQBmlkVo6HVVBN/nVOANd68N/9gG\nvssV9b5LG3d/PzjWbw/vfTfwKTDA3dsSGkq1PbQVkQSksCYisXCNmXU3sw6EwseTe2h3H3ClmR0R\nTBTIMrMzgnu8ZgFrgFuD/RlmNqr+G5jZ4cH5qUA5sIPQfWD1PQZcbGaHmlk6oR6sD4Khx8bUHwJt\nyD3Az8xsSFBXrpmdGxz7D3CAmf0guB8vx8yOCI7lAFuBMjMbBFwVQT0ikkAU1kQkFh4DXid0A34x\n0OBCru5eSOi+tTuBTUARMDE4VgucSehm/BVACaHJA/W1JRT6NhEa5twA/LmBz3oD+CXwLKEQ2A84\nr7EvEswIPZnQ5IA9cvfngD8CTwTDmfOB04Jj24L3OJPQUOtnwJjg1P8HnA9sC77HnoKtiCQoc6/f\nUy8iEj1m9jlwqbv/N9a1NAUzG0lowsTIWNciIolJPWsiIvvv5lgXICKJS6tqi4jsB3efFesaRCSx\naRhUREREJI5pGFREREQkjimsiYiIiMQxhTURERGROKawJiIiIhLHFNZERERE4pjCmoiIiEgcU1gT\nERERiWMKayIiIiJxTGFNREREJI4prImIiIjEMYU1ERERkTimsCYiIiISxxTWREREROKYwpqIiIhI\nHFNYExEREYljCmsiIiIicUxhTURERCSOKayJiIiIxDGFNREREZE4prAmIiIiEscU1kRERETiWEqs\nC2gqeXl53rt371iXISIiItKoOXPmrHf3/EjaJkxY6927N4WFhbEuQ0RERKRRZrY80rYaBhURERGJ\nYwprIiIiInFMYU1EREQkjkU1rJnZWDNbbGZFZnZjA8d7mtk0M/vQzOaZ2elhx34WnLfYzE6NZp0i\nIiIi8SpqEwzMLBmYBJwMlACzzWyKuy8Ma/YL4Cl3v9vMBgMvA72D388DhgBdgf+a2UB3r41WvSIi\nIiLxKJo9ayOBIncvdvcq4AlgXL02DrQNfs8FVge/jwOecPdKd18GFAXvJyIiItKqRDOsdQNWhm2X\nBPvC/Rq40MxKCPWqfX8fzsXMLjezQjMrLC0tbaq6RUREROJGNMOaNbDP621PAB529+7A6cAjZpYU\n4bm4+2R3L3D3gvz8iNaVExEREWlRohnWSoAeYdvd+XKYc6dLgKcA3H0GkAHkRXiuRGBTeRXTl5Sy\ntLSMurqv5F0RERGJc9F8gsFsYICZ9QFWEZowcH69NiuAE4GHzewgQmGtFJgCPGZmfyU0wWAAMCuK\ntSaM6to6PlyxmXc+K2X6klLmrdqCBxktMy2ZQQfkMKRrLoO7tmVI17YM7JxDRmpybIsWERGRPYpa\nWHP3GjO7FngNSAYedPcFZnYLUOjuU4AfAfeZ2Q2EhjknursDC8zsKWAhUANco5mge7ZiQwVvf1bK\nO0tKmbF0A9sqa0gyGN6zPdefOIDDe3dg9ebtLFyzlQWrt/L8h6t4ZGboKRfJSUb//Oxd4W1w17YM\n7tKWdplpMf5WIiIiAmDuiTE0VlBQ4K3l2aBllTXMWLqB6UtKmf5ZKcs3VADQrV0bRg/M57iBeRzV\nL4/cNqkNnl9X55Rs2s6C1Vt2BbiFq7fyxdYdu9p0a9fmywDXpS1DuuXSNTcDs4ZuJxQREZF9YWZz\n3L0gkrYJ8yD3RFZX58xfvYV3PlvP20tKmbt8EzV1TmZaMkf17cjFR/dm9MB8+uRlRRSmkpKMnh0z\n6dkxk9MO6bJr//qyShYF4S0U4Lbw30Vrdw2jtstMZXCXneGtLUO65tI3L4uUZD0IQ0REJFrUsxan\n1m7dEfScrefdz0rZVFENwMHd2nLsgHxGD8hnRK92pKdE936ziqoaPv1i267et4Wrt/DpF9uorKkD\nID0liUEH5ISGT7vmMrhLWw7qkkNmmv4dICIisif70rOmsBYndlTXMmvZxmBiwHoWr90GQH5OOscO\nyGP0gHyOGZBHXnZ6jCuFmto6iteXh4ZRV3/ZE7dleyhQmkGfvKzQRIYuoaHUIV3b0jEOahcREYkH\nCmstgLvz2boypi8p5e0lpcxatpHKmjrSkpMY2adDKKANzGfQATkt4j4xd2f1lh1BeNuyqydu1ebt\nu9p0bpu+W4Ab3LUtPTtktojvJyIi0pR0z1qc2lRexbtF65m+pJR3Plu/64b+/p2yueCIXhw7MI8j\n+3SkTVrLW0rDzOjWrg3d2rXh5MGdd+3fXFHFwjU7h1BDPXBvLymlNljzLSc9hYO6BLNQgx64AZ1y\nSEvRfXAiIiKgsNZs/jVzOb96YT51DrltUjmmfx6jB+Zx7IB8urZrE+vyoqZdZhpH98vj6H55u/bt\nqK5lydov74NbsHoLT85eyfbq0OosqcnGgE45u3rf+nfKJi87nY7ZaXTMSic5ST1xIiLSeiisNYPX\nF3zBr16Yz7ED8vnBSQMY2r1dqw4cGanJDO3ejqHd2+3aV1vnfL6hPOweuC28+ek6np5Tstu5ZtAh\nM4287HTyckLhbefvednp5Aehbme4i/YEDBERkWhTWIuyj1Zu5ronPuSQ7u2458LDWuQQZ3NITjL6\n5WfTLz+bM4d1BUL3wa3bVsnyDRVsKKtkfVklpWVVrC+rZP22SjaUV/FxyWbWb6ukvKrhNZPbZqSQ\nl5NOXtaXgW7na2eoyw/CnmawiohIPNL/O0XRig0VXPLwbPJz0nngogIFtX1kZnRum0HnthmNtt1e\nVRsKcWWVrA8LdOvLKllfXsX6bZUs/mIb75Vt2DVrtb7MtORdAW7nKz87jY67ttN2Bb+2bVI0MUJE\nRJqFwlqUbCqvYuJDs6h15+GLR8bFkhuJrE1aMj06ZNKjQ2ajbatq6thYXhX01O0MdVW7eu/Wl1Wx\ncmMFH67YzMbySuoamDCdlpwUFux2D3T5Obv33LXPTGvVw94iIrJ/FNaiYEd1LZf9s5CSzdt59NIj\n6JefHeuSJExaShIH5GZwQG7jPXa1dc6mip09dVVsKK+kdFtY710Q7j79Yhvryyqprv1qsksy6JAV\n9Mxlh/0MC3X5YUOzqXoihIiIhFFYa2J1dc6PnvqYwuWbmHT+CA7v3SHWJcl+SE6yXUOiHLD3tu7O\n1u01lJZVBr104YHuy+3lK8pZv61q1+zX+tplptIx68tAl5+dHtrOSd897GWna2hdRKQVUFhrYre+\n+ikvfbKGn59+EGcM7dL4CZIwzIzczFRyM1Pp36nx3tTyyho2lFWFhmJ3voLeu52/L1q9lelllWzb\nUdPge2SlJe8W4rq3z6RffjZ987Pom59Ffna67q0TEWnhFNaa0D/e/5zJ04v57lG9uPTYPrEuR+Jc\nVnoKWekp9OzY+H12lTW1bKg39Loz0O3cV1xazttLStlRXbfrvJz0lCC4ZdM3L/iZn0WfvCwyUtUr\nJyLSEiisNZHXF3zB/764gJMO6sTNZw5Rb4Y0qfSUZLq2a9PoAsp1dc6arTtYuq6M4tIyiteXU1xa\nzgfFG3juw1W72plB19w29M3P+rInLi/0s0tuhv77FRGJIwprTWDXWmrdcrljwnDN/JOYSUr68rFf\nowfm73asoqqGZUF4Ky4tp3h9GcWl5TxduHK3derapCbTJy9rV49cv7Agl5WuvzJERJqb/ubdT+Fr\nqd1/0eFaWFXiVmZaCkO65jKka+5u+3cuPry0tGy3IDevZAsvf7Jmt6VLDmibset+uJ0Brl9+Nl3b\ntdE/UkREokTJYj/UX0stP0drqUnLE774cPgzXCG0DM2KjRWhYdX15bsC3ZSPVrM1bNJDWkoSfTpm\nfSXI9c3PJrdNanN/JRGRhKKw9jVpLTVpDTJSkxnYOYeBnXN22+/ubCivCnridt4bV8biL7YxdeFa\nasK64/Ky08LC25dBrmeHTFK0ppyISKMU1r6G8LXU7jx/uNZSk1bH7Mv150b22f2//+raOlZsrPgy\nyAXDqlMXrmVDedWudilJRq+OmbtmqPYL643rkJXW3F9JRCRuKax9DTvXUrvp9EF8Y2jXWJcjEldS\nk5Pol58d9DZ33u3Y5oqqXTNUdwa5paVlvL24lKraL5ccaZeZuttSI33zQhMdenbMJD1FS46ISOui\nsLaPwtdSu+zYvrEuR6RFaZeZxoieaYzo2X63/bV1Tsmmil3hbeew6vQlpTwzp2RXuySDHh0yGwxy\n+TlaAFhEElNUw5qZjQX+BiQD97v7rfWO3w6MCTYzgU7u3i44Vgt8Ehxb4e5nRbPWSGgtNZHoSE4y\nenXMolfHLMYM6rTbsW07qsOWHCljafD7jOINX1kAuE9+Fn3zdq4dpwWARSQxmPtXHzzdJG9slgws\nAU4GSoDZwAR3X7iH9t8Hhrv794LtMneP+K79goICLyws3P/C92DL9mqOufVN+uZn8fjlR2qJDpEY\n27kA8K774sIWAV61efuudloAWETikZnNcfeCSNpGM3GMBIrcvTgo6glgHNBgWAMmADdHsZ79ktsm\nlXu/cxgDOucoqInEgfAFgI8d0LQLAPfJzyJbCwCLSJyI5t9G3YCVYdslwBENNTSzXkAf4M2w3Rlm\nVgjUALe6+/MNnHc5cDlAz549m6jsPTu6f17jjUQk5ppiAeDObdN3Wy+ub34W/bUAsIjEQDTDWkN/\nm+1pzPU84Bl3rw3b19PdV5tZX+BNM/vE3Zfu9mbuk4HJEBoGbYqiRSRx7W0B4MqaWpZvqAjdFxcW\n5P4zbw1btlfvaqcFgEWkuUUzrJUAPcK2uwOr99D2POCa8B3uvjr4WWxmbwHDgaVfPVVEZP+lp+x5\nAeCN5VVBgGt8AeCDurTloqN6c+JBnXQ/nIg0iWiGtdnAADPrA6wiFMjOr9/IzA4E2gMzwva1Byrc\nvdLM8oBRwG1RrFVEpEFmRsfsdDpGuADwu0XrufSfhQw6IIdrxvTn9EO6aNhURPZL1MKau9eY2bXA\na4SW7njQ3ReY2S1AobtPCZpOAJ7w3aelHgTca2Z1QBKhe9b2NDFBRCQmGloAuLq2jikfreaut4r4\n/uMfcvvUJVx1fD/OHt6NVD1eS0S+hqgt3dHcor10h4jIvqitc16d/wV3Titi0ZqtdGvXhiuP68u5\nBT207puI7NPSHQprIiJR5O5MW7yOO98sYu6KzeTnpHPZsX244IheZGl5EJFWS2FNRCTOuDszijcw\naVoR7xVtoF1mKhcf3YeJR/cmN1OzSEVaG4U1EZE4NnfFJu6aVsR/F60jOz2FC4/sxaXH9iEvOz3W\npYlIM1FYExFpARau3sqkt4p4+ZM1pKckcd7hPbniuL50yW0T69JEJMoU1kREWpClpWXc/dZSnv9w\nFWZwzojuXHlcP3rnZcW6NBGJEoU1EZEWqGRTBfe+XcyThSupqa3jzGFduWZM/68s1CsiLZ/CmohI\nC7Zu6w7uf3cZ/5q5nIqqWk4Z3JlrT+jP0O7tYl2aiDQRhTURkQSwqbyKh97/nIffW8bWHTWMHpjP\ntWP6f+VJCiLS8iisiYgkkG07qnlk5nIeeGcZG8qrGNm7A9ec0J/RA/L0/FGRFkphTUQkAW2vquWJ\n2SuYPL2YNVt2cEi3XK4Z059TBncmSc8fFWlRFNZERBJYVU0d/55bwt1vL2X5hgoGds7m6uP7842h\nXUjR80dFWgSFNRGRVqCmto6XPlnDpGlFLFlbRq+OmVx5XD++OaIb6Sl6/qhIPFNYExFpRerqnKmL\n1jJpWhHzSrbQJTeDy0f35bzDe9ImTaFNJB4prImItELuzvTP1jPpzSJmfb6RjllpXHJsH75zZC9y\nMvT8UZF4orAmItLKzVq2kTunFTF9SSltM1KYeHRvLh7Vh/ZZabEuTURQWBMRkcC8ks1MmlbEawvW\nkpmWzAVH9OSyY/vSqW1GrEsTadUU1kREZDdL1m7jrmlFTPl4NSnJSXyroDtXjO5Hjw6ZsS5NpFVS\nWBMRkQZ9vr6ce95eyrNzS3CHs4d346rj+9EvPzvWpYm0KgprIiKyV6s3b2fy9GKemL2Cypo6Tj+k\nC9cc35/BXdvGujSRVkFhTUREIrK+rJIH3l3GIzOWU1ZZw4mDOnHNCf0Z0bN9rEsTSWgKayIisk+2\nVFTzjxmf8+B7y9hcUc3R/Tpy7Zj+HNWvo54/KhIFCmsiIvK1lFfW8NgHK5j8TjGl2yoZ3rMd3z+h\nP2MO7KTQJtKE9iWsRfUhcmY21swWm1mRmd3YwPHbzeyj4LXEzDaHHbvIzD4LXhdFs04REQnJSk/h\nstF9eecnY/jNuCGs21rJ9x4u5Iw73uWleWuorUuMf+CLtCRR61kzs2RgCXAyUALMBia4+8I9tP8+\nMNzdv2dmHYBCoABwYA5wmLtv2tPnqWdNRKTpVdfW8fyHq7j7raUUry+nb34WVx/fn3GHdiVVD40X\n+dripWdtJFDk7sXuXgU8AYzbS/sJwOPB76cCU919YxDQpgJjo1iriIg0IDU5iXMLejD1h8dx5/nD\nSUtO4v89/TFj/vwWj8xczo7q2liXKJLwohnWugErw7ZLgn1fYWa9gD7Am/tyrpldbmaFZlZYWlra\nJEWLiMhXJScZ3xjalVeuP5YHLiogLzudXz4/n9G3TeP+d4qpqKqJdYkiCSuaYa2hO1H3NOZ6HvCM\nu+/8J1pE57r7ZHcvcPeC/Pz8r1mmiIhEysw48aDOPHf10Tx66RH0y8/mty8tYtStb/L3Nz5jy/bq\nWJcoknCiGdZKgB5h292B1Xtoex5fDoHu67kiItLMzIxR/fN4/PIjefaqoxnesz1/mbqEY259k9te\n/ZQNZZWxLlEkYURzgkEKoQkGJwKrCE0wON/dF9RrdyDwGtDHg2KCCQZzgBFBs7mEJhhs3NPnaYKB\niEhsLVi9hbumLeXl+WtIT0liwsieXD66L11y28S6NJG4sy8TDFKiVYS715jZtYSCWDLwoLsvMLNb\ngEJ3nxI0nQA84WGp0d03mtlvCAU8gFv2FtRERCT2hnTNZdIFIyhaV8bdby3lnzOW86+Zyxl/WHeu\nPK4fvTpmxbpEkRZJi+KKiEhUrNxYwT1vL+XpwhJq6uo4a1hXrhnTnwGdc2JdmkjM6QkGIiISN9Zu\n3cF904t59IMVbK+uZeyQA7hmTH8O6Z4b69JEYkZhTURE4s7G8ioeem8ZD7//Odt21HDcwHyuPaE/\nh/fuEOvSRJqdwpqIiMStrTuqeWTGch54dxkby6sY2acD147pz7ED8vT8UWk1FNZERCTuba+q5fFZ\nK5g8vZgvtu5gWPdcrh7Tn5MP6kxSkkKbJDaFNRERaTEqa2r599zQ80dXbKzgwM45XD2mH98Y2pVk\nhTZJUAprIiLS4tTU1vHivNVMmraUonVl9O6YyVXH9+N/hncnLUUPjZfEorAmIiItVl2d8/rCL7hz\nWhHzV22la24Gl4/uy3kje5KRmhzr8kSahMKaiIi0eO7O20tKmTStiNmfb6Jnh0z+eM5QjurXMdal\niey3fQlr6lcWEZG4ZGYcf2Annr4y9NB4M5hw30x++fx8yitrYl2eSLNRWBMRkbg3qn8er1x/LN8b\n1Yd/fbCcU/9vOu8VrY91WSLNQmFNRERahMy0FH515mCevuIoUpOTuOD+D7jpuU/YtqM61qWJRJXC\nmoiItCgFvTvwyvXHctmxfXh81gpOvX0605eUxroskahRWBMRkRYnIzWZn58xmGeuPJo2acl898FZ\n/PSZeWxVL5skIIU1ERFpsQ7r1Z6XrjuWK4/rx9NzVnLq7dOZtnhdrMsSaVIKayIi0qJlpCZz42mD\n+PfVo8hOT+Hih2bzo6c+ZkuFetkkMSisiYhIQji0Rzv+c90xXDOmH89/tIqTb3+b/y5cG+uyRPab\nwpqIiCSM9JRkfnzqIJ6/ehTtM9O49J+F3PDkR2yuqIp1aSJfm8KaiIgknEO65/Li94/huhMH8OLH\nqznpr9N5bcEXsS5L5GtRWBMRkYSUlpLED08eyAvXjiI/J50rHpnDdY9/yMZy9bJJy6KwJiIiCW1I\n11ymXDuKG04ayCvz13DK7W/z8idrYl2WSMQU1kREJOGlJidx/UkDmHLtMRyQm8HVj87lmkfnsr6s\nMtaliTRKYU1ERFqNg7q05bmrR/HjUw9k6sK1nHL7dF78eDXuHuvSRPYoqmHNzMaa2WIzKzKzG/fQ\n5ltmttDMFpjZY2H7a83so+A1JZp1iohI65GanMQ1Y/rzn+uOoUf7Nnz/8Q+56l9zKd2mXjaJTxat\nf02YWTKwBDgZKAFmAxPcfWFYmwHAU8AJ7r7JzDq5+7rgWJm7Z0f6eQUFBV5YWNik30FERBJbTW0d\n972zjNv/u4TMtGT+96whnDWsK2YW69IkwZnZHHcviKRtNHvWRgJF7l7s7lXAE8C4em0uAya5+yaA\nnUFNRESkOaQkJ3HV8f14+bpj6N0xi+uf+IjL/jmHdVt3xLo0kV2iGda6ASvDtkuCfeEGAgPN7D0z\nm2lmY8OOZZhZYbD/7IY+wMwuD9oUlpaWNm31IiLSavTvlMOzVx3NTacP4p3PSjnpr2/z7JwS3csm\ncSGaYa2hPuT6/9WnAAOA44EJwP1m1i441jPoHjwf+D8z6/eVN3Of7O4F7l6Qn5/fdJWLiEirk5xk\nXD66H69cfywDO+fwo6c/5pJ/FPLFFvWySWxFM6yVAD3CtrsDqxto84K7V7v7MmAxofCGu68OfhYD\nbwHDo1iriIgIAH3zs3nyiqP45TcG8/7S9Zx8+9s8VbhSvWwSMxGFNTNLN7PzzewmM/vVzlcjp80G\nBphZHzNLA84D6s/qfB4YE3xGHqFh0WIza29m6WH7RwELERERaQbJScYlx/Th1etHc9ABbfnJM/O4\n6KHZrN68PdalSSsUac/aC4QmB9QA5WGvPXL3GuBa4DVgEfCUuy8ws1vM7Kyg2WvABjNbCEwDfuzu\nG4CDgEIz+zjYf2v4LFIREZHm0DsviycuP5L/PWsIs5dt5JTbp/P4rBXqZZNmFdHSHWY2390PboZ6\nvjYt3SEiItG0YkMFP3n2Y2YWb+TYAXn84ZuH0L19ZqzLkhYqGkt3vG9mh+xHTSIiIi1az46ZPHbp\nkfzm7IOZu3wTp94+nX/NXE5dnXrZJLoiDWvHAHOCpxHMM7NPzGxeNAsTERGJN0lJxneO7MWrPxjN\noT3b8Yvn53PB/R+wcmNFrEuTBBbpMGivhva7+/Imr+hr0jCoiIg0J3fn8Vkr+f3Li6hz56djB/Gd\nI3uRlKSnH0jjmnwYNAhl7YAzg1e7eApqIiIizc3MOP+Inrx2w2gKenfg5ikLOO++mSzfsNf5dyL7\nLNKlO64HHgU6Ba9/mdn3o1mYiIhIS9CtXRv+cfHh3HbOUBat3sqp/zedB99dpnvZpMlEOgw6DzjK\n3cuD7SxghrsPjXJ9EdMwqIiIxNqaLdu56d+fMG1xKQW92nPb+KH0zc+OdVkSh6IxG9SA2rDtWhp+\nnJSIiEir1SW3DQ9OPJy/nDuMJWu3cdrf3uG+6cXUqpdN9kNKhO0eAj4ws+eC7bOBB6JTkoiISMtl\nZpxzWHeOGZDHz5/7hN+9vIiX56/hT+OH0b+Tetlk30U0DApgZiMILeFhwHR3/zCahe0rDYOKiEi8\ncXde+Gg1v35xARVVtfzw5IFcekwfUpKj+WhuaQn2ZRh0r2HNzNq6+1Yz69DQcXff+DVrbHIKayIi\nEq/WbdvBL56bz+sL1zKsey5/OncYAzvnxLosiaGmvGftseDnHKAw7LVzW0RERBrRKSeDe79zGHdM\nGM6KjRV84453mTStiJraulhFfxVKAAAgAElEQVSXJi1AxMOg8U49ayIi0hKsL6vk5hcW8NInazik\nWy5/Oncogw5oG+uypJk1+WxQMxsVLNeBmV1oZn81s577U6SIiEhrlJedzqQLRjDp/BGs3rydM//+\nLne88RnV6mWTPYj0Dse7gQozGwb8BFgOPBK1qkRERBLcGUO78PoNoxl7cBf+OnUJ4+58jwWrt8S6\nLIlDkYa1Gg+Nl44D/ubufwN0Z6SIiMh+6Jidzt8nDOeeCw9j3bZKxt35Hn+duoSqGvWyyZciDWvb\nzOxnwIXAS2aWDKRGrywREZHWY+zBBzD1htGcOawrd7zxGWfd+S7zV6mXTUIiDWvfBiqBS9z9C6Ab\n8KeoVSUiItLKtM9K4/ZvH8p93y1gY3kV4ya9x59fW0xlTW3jJ0tC02xQERGROLOloppb/rOQZ+eW\nMLBzNn8aP4xhPdrFuixpQk02G9TM3g1+bjOzrWGvbWa2tSmKFRERkd3lZqbyl28N48GJBWzdXsP/\n3PUet77yKTuq1cvWGu01rLn7McHPHHdvG/bKcXctCiMiIhJFJwzqzGs3jGb8Yd255+2lnHHHO8xd\nsSnWZUkzi3SdtSPNLCdsO9vMjoheWSIiIgKQ2yaV28YP4x/fG0lFVS3j736f37+8SL1srci+rLNW\nFrZdEewTERGRZnDcwHxev2E03z68J5OnF3P6396h8PO4eUS3RFGkYc08bCaCu9cBKY2eZDbWzBab\nWZGZ3biHNt8ys4VmtsDMHgvbf5GZfRa8LoqwThERkYSVk5HKH755CP+65Agqa+o4994Z3PLiQrZX\nqZctkUUa1orN7DozSw1e1wPFezshWIttEnAaMBiYYGaD67UZAPwMGOXuQ4AfBPs7ADcDRwAjgZvN\nrP0+fC8REZGEdcyAPF67YTQXHtGLB99bxml/m84HxRtiXZZESaRh7UrgaGAVUEIoRF3eyDkjgSJ3\nL3b3KuAJQk9ACHcZMMndNwG4+7pg/6nAVHffGBybCoyNsFYREZGEl52ewm/OPpjHLjuCWne+PXkm\nt09dQqIsySVfiiisufs6dz/P3Tu5e2d3Pz8sWO1JN2Bl2HZJsC/cQGCgmb1nZjPNbOw+nIuZXW5m\nhWZWWFpaGslXERERSShH98vj1etH880R3fjbG59x03PzqdFD4RNKpLNBB5rZG2Y2P9geama/aOy0\nBvbVj/spwADgeGACcL+ZtYvwXNx9srsXuHtBfn5+Y19DREQkIWWlp/CXc4dx9fH9eHzWCq5+dK5m\niyaQSIdB7yN0b1k1gLvPA85r5JwSoEfYdndgdQNtXnD3andfBiwmFN4iOVdEREQCZsZPxg7i5jMH\n8/rCtXz3gVls2V4d67KkCUQa1jLdfVa9fTWNnDMbGGBmfcwsjVC4m1KvzfPAGAAzyyM0LFoMvAac\nYmbtg4kFpwT7REREZC8uHtWHOyYM58OVm/j2vTNYu3VHrEuS/RRpWFtvZv0IhiLNbDywZm8nuHsN\ncC2hkLUIeMrdF5jZLWZ2VtDsNWCDmS0EpgE/dvcN7r4R+A2hwDcbuCXYJyIiIo04a1hXHpo4kpUb\nK/jmXe+ztLSs8ZMkbkX0IHcz6wtMJjQjdBOwDLjA3ZdHt7zI6UHuIiIiu/ukZAsTH5pFnTsPXTyS\nQ/Uw+LjRZA9yD94sCShw95OAfGCQux8TT0FNREREvuqQ7rk8c9XRZGekcP59M3l7iVZOaIkaDWvB\n0wquDX4vd/dtUa9KREREmkSfvCyevepoenXM4pKHZ/P8h6tiXZLso0jvWZtqZv/PzHqYWYedr6hW\nJiIiIk2iU04GT15xJAW92/ODJz/i/nf2+hAiiTONPt8z8D1Ckwuurre/b9OWIyIiItHQNiOVhy8e\nyQ1PfsRvX1pE6bZKbjxtEGYNLW0q8STSnrXBhJ7z+THwEfB3YEi0ihIREZGml5GazJ3nj+DCI3ty\n7/RifvT0x1TraQdxL9KetX8AW4E7gu0Jwb5vRaMoERERiY7kJOM34w4mPzuD2/+7hE3lVUy6YASZ\naZFGAmlukV6ZA919WNj2NDP7OBoFiYiISHSZGdefNIC8nDR++fx8zr/vAx6aeDjts9JiXZo0INJh\n0A/N7MidG2Z2BPBedEoSERGR5nDBEb2464IRLFyzlfH3vM+qzdtjXZI0INKwdgTwvpl9bmafAzOA\n48zsEzObF7XqREREJKrGHtyFf35vJOu2VnLOXe+z+Aut0BVvIn2CQa+9HY+HBXL1BAMREZGvb9Ga\nrXz3wVlUVtfywMTDOby3VuiKpiZ9ggGEwtjeXvtXroiIiMTaQV3a8u+rjqZjdjoX3v8BUxeujXVJ\nEoh0GFREREQSXI8OmTxz5VEceEAOVzxSyJOzV8S6JEFhTURERMJ0zE7n8cuOZFT/PH767CdMmlZE\nJLdMSfQorImIiMhustJTeOCiwxl3aFf+9Npi/vfFhdTVKbDFilbAExERka9IS0ni9m8dSl52Og+8\nu4z1ZZX85VvDSE9JjnVprY7CmoiIiDQoKcn4xRkHkZ+Tzq2vfMrmimru+c5hZKcrPjQnDYOKiIjI\nHpkZVx7Xjz+fO4wZxRs4b/IM1pdVxrqsVkVhTURERBo1/rDu3PfdwyhaV8b4u99nxYaKWJfUaiis\niYiISEROGNSZRy89ks3bq/nm3e+zYPWWWJfUKiisiYiISMQO69WeZ648irRk49v3zuT9petjXVLC\nU1gTERGRfdK/Uw7PXn00XXIzmPjgbF7+ZE2sS0poCmsiIiKyz7rktuHpK4/ikO65XPPYXB6Z8Xms\nS0pYUQ1rZjbWzBabWZGZ3djA8YlmVmpmHwWvS8OO1YbtnxLNOkVERGTftctM41+XHMEJB3bily8s\n4K+vL9bTDqIgagulmFkyMAk4GSgBZpvZFHdfWK/pk+5+bQNvsd3dD41WfSIiIrL/2qQlc+93DuOm\n5z7hjjeLKC2r4jfjhpCSrMG7phLNVe1GAkXuXgxgZk8A44D6YU1ERERasJTkJP54zlDystO5662l\nbCir5I4Jw8lI1dMOmkI0Y283YGXYdkmwr75zzGyemT1jZj3C9meYWaGZzTSzsxv6ADO7PGhTWFpa\n2oSli4iIyL4wM34ydhA3nzmY1xeu5bsPzGLL9upYl5UQohnWrIF99QeyXwR6u/tQ4L/AP8KO9XT3\nAuB84P/MrN9X3sx9srsXuHtBfn5+U9UtIiIiX9PFo/pwx4ThfLhyE9++dwZrt+6IdUktXjTDWgkQ\n3lPWHVgd3sDdN7j7zmdW3AccFnZsdfCzGHgLGB7FWkVERKSJnDWsKw9NHMnKjRV88673WVpaFuuS\nWrRohrXZwAAz62NmacB5wG6zOs2sS9jmWcCiYH97M0sPfs8DRqF73URERFqMYwbk8cTlR7Gjupbx\nd7/PRys3x7qkFitqYc3da4BrgdcIhbCn3H2Bmd1iZmcFza4zswVm9jFwHTAx2H8QUBjsnwbc2sAs\nUhEREYljh3TP5ZmrjiY7I4Xz75vJ20t0f/nXYYmyHkpBQYEXFhbGugwRERGpZ922HVz04Gw+W7uN\nP587jLOHNzTfsHUxsznBvfmN0iIoIiIiElWdcjJ48oojKejdnh88+RH3v1Mc65JaFIU1ERERibq2\nGak8fPFITj/kAH770iIefHdZrEtqMRTWREREpFlkpCbz9wkjOOmgTvzx1U9Ztr481iW1CAprIiIi\n0mySk4zf/c8hpKUk8dNn51FXlxj3zkeTwpqIiIg0q85tM/jFGQcxa9lGHp21ItblxD2FNREREWl2\n3yrowTH987j15UWs2rw91uXENYU1ERERaXZmxh++eQh1Dj9/7hMSZSmxaFBYExERkZjo0SGTn4w9\nkLcWl/Lch6tiXU7cUlgTERGRmPnuUb0Z0bMdt/xnIaXbKhs/oRVSWBMREZGYSU4ybhs/lIrKWn49\nZUGsy4lLCmsiIiISU/075XD9SQN46ZM1vDr/i1iXE3cU1kRERCTmLh/dl8Fd2vLLF+azpaI61uXE\nFYU1ERERibnU5CRuGz+UjeVV/PalhbEuJ64orImIiEhcOLhbLleM7svTc0qYvqQ01uXEDYU1ERER\niRvXnTiAvvlZ/Ozfn1BeWRPrcuKCwpqIiIjEjYzUZG47Zyirt2zntlc/jXU5cUFhTUREROJKQe8O\nXHRUb/45czmzP98Y63JiTmFNRERE4s6PTz2Qrrlt+Omz89hRXRvrcmJKYU1ERETiTlZ6Cn/45iEU\nl5bztzc+i3U5MaWwJiIiInFp9MB8zj2sO5OnFzN/1ZZYlxMzCmsiIiISt35xxmA6ZKXxk2fmUV1b\nF+tyYkJhTUREROJWbmYqvz37YBau2cq9by+NdTkxEdWwZmZjzWyxmRWZ2Y0NHJ9oZqVm9lHwujTs\n2EVm9lnwuiiadYqIiEj8OnXIAZxxSBfueKOIonXbYl1Os4taWDOzZGAScBowGJhgZoMbaPqkux8a\nvO4Pzu0A3AwcAYwEbjaz9tGqVUREROLbr88aQmZ6Mj95Zh61dR7rcppVNHvWRgJF7l7s7lXAE8C4\nCM89FZjq7hvdfRMwFRgbpTpFREQkzuXnpHPzmYOZu2Iz/3j/81iX06yiGda6ASvDtkuCffWdY2bz\nzOwZM+uxL+ea2eVmVmhmhaWleoaYiIhIIjv70G4cf2A+f3ptMSs2VMS6nGYTzbBmDeyr32/5ItDb\n3YcC/wX+sQ/n4u6T3b3A3Qvy8/P3q1gRERGJb2bG7//nEJKTjJ89Nw/31jEcGs2wVgL0CNvuDqwO\nb+DuG9y9Mti8Dzgs0nNFRESk9enarg03njaI94o28FThysZPSADRDGuzgQFm1sfM0oDzgCnhDcys\nS9jmWcCi4PfXgFPMrH0wseCUYJ+IiIi0cueP7MkRfTrw25cWsXbrjliXE3VRC2vuXgNcSyhkLQKe\ncvcFZnaLmZ0VNLvOzBaY2cfAdcDE4NyNwG8IBb7ZwC3BPhEREWnlkpKMW88ZSlVNHT9/bn7CD4da\nonzBgoICLywsjHUZIiIi0kwmT1/K71/+lL9PGM6Zw7rGupx9YmZz3L0gkrZ6goGIiIi0SN8b1Ydh\n3XP59ZQFbCyvinU5UaOwJiIiIi1SSnISfxw/lK07qrnlxQWxLidqFNZERESkxRp0QFuuPr4/z3+0\nmjc/XRvrcqJCYU1ERERatGvG9OfAzjnc9O/5bN1RHetympzCmoiIiLRoaSmh4dB123Zw6yufxrqc\nJqewJiIiIi3eoT3acfGoPjw+awUfr9wc63KalMKaiIiIJIQfnDSAvOx0fvXCfOrqEmNpMlBYExER\nkQSRk5HKz08/iI9LtvBkAj2KSmFNREREEsa4Q7sysk8H/vjqp2xKkLXXFNZEREQkYZgZt4wbwrYd\nNfzp9cWxLqdJKKyJiIhIQhl0QFsuOqo3j89awbySlj/ZQGFNREREEs4PTh5Ax6x0fvnCghY/2UBh\nTURERBJO24xUbjp9EB+v3MxTLXyygcKaiIiIJKT/Gd6Nw3u354+vfsrmipY72UBhTURERBJSaLLB\nwWzdUcOfW/BkA4U1ERERSVgHdWnLd47sxaMfrOCTki2xLudrUVgTERGRhHbDyQPpmJXGL1vokw0U\n1kRERCSh5bZJ5WenHcRHKzfzzJySWJezzxTWREREJOF9c0Q3Cnq159ZXP2VLRXWsy9knCmsiIiKS\n8HZONthcUcVfprasyQYKayIiItIqDO4ammzwr5nLmb+q5Uw2UFgTERGRVuOHpxxI+8w0ftWCJhtE\nNayZ2VgzW2xmRWZ2417ajTczN7OCYLu3mW03s4+C1z3RrFNERERah9w2qdx42iDmrtjMs3NbxmSD\nqIU1M0sGJgGnAYOBCWY2uIF2OcB1wAf1Di1190OD15XRqlNERERal3NGdGdEz3bc+sqnbNke/5MN\notmzNhIocvdid68CngDGNdDuN8BtwI4o1iIiIiICQFJSaLLBpooq/toCnmwQzbDWDQh/cmpJsG8X\nMxsO9HD3/zRwfh8z+9DM3jazYxv6ADO73MwKzaywtLS0yQoXERGRxHZwt1wuPLIXj8xczoLV8T3Z\nIJphzRrYt+tOPjNLAm4HftRAuzVAT3cfDvwQeMzM2n7lzdwnu3uBuxfk5+c3UdkiIiLSGvzo5NBk\ng5tfWIB7/E42iGZYKwF6hG13B1aHbecABwNvmdnnwJHAFDMrcPdKd98A4O5zgKXAwCjWKiIiIq1M\nbmYqPx07iMLlm/j33FWxLmePohnWZgMDzKyPmaUB5wFTdh509y3unufuvd29NzATOMvdC80sP5ig\ngJn1BQYAxVGsVURERFqh8Yd159Ae7fjDK4uoqKqJdTkNilpYc/ca4FrgNWAR8JS7LzCzW8zsrEZO\nHw3MM7OPgWeAK919Y7RqFRERkdYpKcm48bRBrC+r4uVPvoh1OQ2yeB6j3RcFBQVeWFgY6zJERESk\nhXF3xvz5LTq1zeCpK45qls80sznuXhBJWz3BQERERFo1M+Pcgh7MWraRZevLY13OVyisiYiISKs3\n/rDuJBk8M2dl442bmcKaiIiItHqd22Zw/IGdeGZOCTW1dbEuZzcKayIiIiLAtwp6sHZrJe98tj7W\npexGYU1EREQEOGFQJzpmpfFUYXwNhSqsiYiIiABpKUmML+hOnTt1dfGzWkZKrAsQERERiRc3jh2E\nWUNPzIwd9ayJiIiIBOItqIHCmoiIiEhcU1gTERERiWMKayIiIiJxTGFNREREJI4prImIiIjEMYU1\nERERkTimsCYiIiISxxTWREREROKYucfP4xT2h5mVAsvr7c4D4utprNIQXaf4p2vUMug6tQy6TvGv\nOa5RL3fPj6RhwoS1hphZobsXxLoO2Ttdp/ina9Qy6Dq1DLpO8S/erpGGQUVERETimMKaiIiISBxL\n9LA2OdYFSER0neKfrlHLoOvUMug6xb+4ukYJfc+aiIiISEuX6D1rIiIiIi2awpqIiIhIHEuIsGZm\nY81ssZkVmdmNDRxPN7Mng+MfmFnv5q9SIrhOPzSzhWY2z8zeMLNesaizNWvsGoW1G29mbmZxM7W9\nNYnkOpnZt4I/TwvM7LHmrrG1i+Dvu55mNs3MPgz+zjs9FnW2Zmb2oJmtM7P5ezhuZnZHcA3nmdmI\n5q5xpxYf1swsGZgEnAYMBiaY2eB6zS4BNrl7f+B24I/NW6VEeJ0+BArcfSjwDHBb81bZukV4jTCz\nHOA64IPmrVAgsutkZgOAnwGj3H0I8INmL7QVi/DP0i+Ap9x9OHAecFfzVinAw8DYvRw/DRgQvC4H\n7m6GmhrU4sMaMBIocvdid68CngDG1WszDvhH8PszwIlmZs1Yo0Rwndx9mrtXBJszge7NXGNrF8mf\nJYDfEArSO5qzONklkut0GTDJ3TcBuPu6Zq6xtYvkGjnQNvg9F1jdjPUJ4O7TgY17aTIO+KeHzATa\nmVmX5qlud4kQ1roBK8O2S4J9DbZx9xpgC9CxWaqTnSK5TuEuAV6JakVSX6PXyMyGAz3c/T/NWZjs\nJpI/SwOBgWb2npnNNLO99R5I04vkGv0auNDMSoCXge83T2myD/b1/7eiJiUWH9rEGuohq78eSSRt\nJLoivgZmdiFQABwX1Yqkvr1eIzNLInQbwcTmKkgaFMmfpRRCQzfHE+qhfsfMDnb3zVGuTUIiuUYT\ngIfd/S9mdhTwSHCN6qJfnkQobrJDIvSslQA9wra789Xu5F1tzCyFUJfz3ro+pelFcp0ws5OAnwNn\nuXtlM9UmIY1doxzgYOAtM/scOBKYokkGzS7Sv/NecPdqd18GLCYU3qR5RHKNLgGeAnD3GUAGoYeH\nS/yI6P+3mkMihLXZwAAz62NmaYRu1JxSr80U4KLg9/HAm67VgJtbo9cpGGK7l1BQ0z02zW+v18jd\nt7h7nrv3dvfehO4rPMvdC2NTbqsVyd95zwNjAMwsj9CwaHGzVtm6RXKNVgAnApjZQYTCWmmzVimN\nmQJ8N5gVeiSwxd3XxKKQFj8M6u41ZnYt8BqQDDzo7gvM7Bag0N2nAA8Q6mIuItSjdl7sKm6dIrxO\nfwKygaeD+R8r3P2smBXdykR4jSTGIrxOrwGnmNlCoBb4sbtviF3VrUuE1+hHwH1mdgOhobWJ6kRo\nXmb2OKFbBfKCewdvBlIB3P0eQvcSng4UARXAxbGpVI+bEhEREYlriTAMKiIiIpKwFNZERERE4pjC\nmoiIiEgcU1gTERERiWMKayIiIiJxTGFNRGQ/mVlvM5sf/H68melxXCLSZBTWRKTVCha71N+DIhLX\n9JeUiLQqQS/YIjO7C5gLfMfMZpjZXDN72syyg3aHm9n7Zvaxmc0ys5zg3HeCtnPN7OjYfhsRaQ0U\n1kSkNToQ+CdwMqFnNJ7k7iOAQuCHwSOCngSud/dhwEnAdmAdcHLQ9tvAHbEoXkRalxb/uCkRka9h\nubvPNLNvAIOB94JHnKUBMwiFuTXuPhvA3bcCmFkWcKeZHUroMU4DY1G8iLQuCmsi0hqVBz8NmOru\nE8IPmtlQQs9rrO8GYC0wjNDIxI5oFikiAhoGFZHWbSYwysz6A5hZppkNBD4FuprZ4cH+HDNLAXIJ\n9bjVAd8h9JBuEZGoUlgTkVbL3UuBicDjZjaPUHgb5O5VhO5J+7uZfQxMBTKAu4CLzGwmoSHQ8gbf\nWESkCZl7Qz39IiIiIhIP1LMmIiIiEscU1kSk2ZjZgWb2oZltM7PrYl2PiEhLoNmgItKcfgK85e7D\nY12IiEhLoZ41EWlOvYAFzfVhwQzOhGZmmpEqkuAU1kSkWZjZm8AYQovKlpnZQDM73cwWBsOiq8zs\n/4W1H2dmH5nZVjNbamZjg/1dzWyKmW00syIzuyzsnF+b2TNm9i8z2wpMNLMkM7sxeI8NZvaUmXXY\nQ43Hm1mJmf3EzNaZ2RozOzuoc0nwmTeFtd/rewePr/rCzLaY2XQzGxJ2rMHvbmYTzezdenV52PIi\nD5vZ3Wb2spmVA2PMLN3M/mxmK8xsrZndY2Zt9u+KiUi8UFgTkWbh7icA7wDXunu2uy8BHgCucPcc\n4GDgTQAzG0nocVA/BtoBo4HPg7d6HCgBugLjgd+b2YlhHzUOeCY471HgOuBs4LjgnE3ApL2UegCh\nZTq6Ab8C7gMuBA4DjgV+ZWZ9g7aNvfcrwACgE6HnkD4adqzB7x6h84HfATnAu8AfCS0lcijQP6x2\nEUkACmsiEkvVwGAza+vum9x9brD/EuBBd5/q7nXuvsrdPzWzHsAxwE/dfYe7fwTcT2iB2p1muPvz\nwXnbgSuAn7t7ibtXAr8Gxu9liLQa+J27VwNPAHnA39x9m7svIDSMOzRou9f3dvcHg/N2HhtmZrmN\nfPdIvODu7wWL81YClwE3uPtGd98G/B44bx/eT0TimMKaiMTSOcDpwHIze9vMjgr29wCWNtC+K7Az\nkOy0nFBP0k4r653TC3jOzDab2WZgEaHnenbeQ00b3L02+H178HNt2PHtQHZj721myWZ2azBEupUv\newbzGvnukQj/jvlAJjAnrI5Xg/0ikgAU1kQkZtx9truPIzRM+DzwVHBoJdCvgVNWAx3MLCdsX09g\nVfjb1jtnJXCau7cLe2W4+yr2397e+3xCQ7InEXpMVe/gHIO9fvdyQuEr1NjsgAY+N/w7ricUIIeE\n1ZDr7tkNnCciLZDCmojEhJmlmdkF9v/bu+/4qur7j+OvTxKSQEgIkIQRwt4rQeLCKoJaURFwQ6vV\nqrVaV+uq/qytWrXWrdW6V6vVukXrFhyoUAIk7BF2QEgYIQmQhCTf3x/3QiOGmeSec2/ez8fjPsg5\nOffed86DXN6c8f2atQqeciwhcFQKAtdz/dLMjgtexJ9uZn2dc6uBb4G/mFl8cML1i/jhtWC7ewK4\n08y6BN831czGNtCPsbfXTiRwinIjgfJ1137+7HnAADPLMrN4AqdP9yh4KvRp4EEzSwu+frqZndhA\nP6OIeExlTUS8dB6wInia8FICF/LjnPsv8EvgQWAL8CWBU44AEwgcpVoLvA38yTn36V7e42FgIvCJ\nmZUSmP/z8AbKv7fX/geBU7RrgPnB79W2p599MXA78BmwhMANBPvyeyAfmBp8vc+APgf/Y4mIn2hu\nUBEREREf05E1ERERER9TWRMRERHxMZU1ERERER9TWRMRERHxMZU1ERERER9TWRMRERHxMZU1ERER\nER/b00TGYSclJcV17drV6xgiIiIi+zRjxowNzrn9msM3Yspa165dycnJ8TqGiIiIyD6Z2cr93Van\nQUVERER8TGVNRERExMdU1kRERER8TGVNRERExMdU1kRERER8TGVNRERExMc8KWtmNsrMFplZvpnd\nWMf3HzSz3OBjsZkVe5FTRERExGshH2fNzKKBx4ATgAJguplNdM7N37mNc+53tba/EhgS6pwiIiIi\nfuDFoLiHAfnOuWUAZvYqMBaYv4ftJwB/ClE2EWniKqtqWLiuhLzVxcxaXcyyoq0kt2hGWmIcaYnx\npCXFkZYYR2pifGBdUhxxMdFexxaRCOZFWUsHVtdaLgAOr2tDM+sCdAMmhSCXiDQxzjlWbtxG7upi\nclcXk1dQzLy1JVRW1QCQ0jKW3u0S2VhWyYLvS9hQVkl1jfvR67Rq3mxXcWuXGE9qUrDYJcYF1we+\nToiLmEljRCSEvPjksDrW/fjTL2A88IZzrrrOFzK7BLgEoHPnzg2TTkQi1sayCvIKisldVUxuwRby\nVhezZfsOAJo3i2ZQeisuGNaVzE7JZGa0Ij25OWb/+8iqrnFs3FpBYUkFRaUVFJaWU1hSQeHOr0sr\nmLZ8E0WlFVRW1/zo/RNio0lLiic1WOLaJf3v6Nz/yl08Sc1jfvC+ItK0eVHWCoCMWsudgLV72HY8\ncPmeXsg59xTwFEB2dvaeCp+INEHbK6uZuzZQyHYeOSvYvB2AKIPe7RI5aWB7MjOSycpIpldaS2Ki\n937PVXSUBUtV/F63c44HXFsAACAASURBVM5RvG3H/0rcboWuqKSCuWu2MGlhIdsqf/x/0biYqF2F\nrvap17TEeNq1iufwbm2Ib6ZTryJNhRdlbTrQy8y6AWsIFLKf7b6RmfUBWgPfhTaeiISb6hpHfmHZ\nruvM8lYXs2h96a5TlunJzcnKSOYXR3Yhs1Mygzq1okVs4338mRmtE2JpnRBLn/aJe922rKKK9SU7\nC1158IhdBYUlgWKXX1TGt0s3UFJetes53VISuOu0QRzZo22j/Qwi4h8hL2vOuSozuwL4GIgGnnPO\nzTOz24Ec59zE4KYTgFedczpiJiK7OOdYV1IePJUZKGZzCrawNXiEKjE+hqyMZC7r24OsjGQGZ7Ta\n55EwL7WMi6Flakt6pLbc63blO6opLKlg/vcl3PXBAiY8PZVzsjO46eS+JLeIDVFaEfGCRUoXys7O\ndjk5OV7HEJEGVlK+gzkFW/53E8DqYgpLKwBoFm3075BEVkYymcFHt7YJREVF9vVe2yureejzxTzz\n9XJat2jGn04dwOjBHXSdm0gYMbMZzrns/dpWZU1E/KK6xjF/bQm5qzeTu3oLeQXFLC0qY+fHVPeU\nhF3XmGVmJNOvQ2KTHjZj3tot3PTWHGYXbGFk3zT+PG4g6cnNvY4lIvtBZU1EwoZzjjlrtvDOrLVM\nzFvLhrLAUbOUlrGBUtYpeNSsUzKtWjTzOK3/VNc4Xvh2Bfd/sgiA637ah/OHdSU6wo8uioQ7lTUR\n8b1VG7fxTu4a3sldw7KircRGRzGibyonD+rA0C6tfzRshuxdweZt/OGduXyxqIjBnVpx9+mD6d8x\nyetYIrIHKmsi4ksbyyr4z5zveWfWGmauCkz5e3i3Npw2JJ2TBnbQkbN6cs7x3uzvuf29eWzetoNf\nHd2d3x7fS8N8iPiQypqI+Mb2ymo+mb+Od3PX8tXiIqpqHH3bJzI2K50xWR11jVUjKN5WyV0fLOC1\nnAK6tG3BneMG8ZNeKV7HEpFaVNZExFNV1TV8s3Qj785aw0fz1rGtspoOreIZk9WRcVnp9Oug03Oh\n8O3SDdz89lyWb9jK6Yek84dT+tMmQcN8iPiBypqIhJxzjtkFW3gndw3v5X3PhrIKEuNjOGVQB8YN\nSeewrm0ifkgNPyrfUc2jk/J54sulJDVvxi2j+zEuK13XA4p4TGVNREJm5catvDNrLe/mrmHZhsCN\nAiP7pjFuSEeO7ZOm66V8YuG6Em58cw65q4s5ulcKd44bROe2LbyOJdJkqayJSKPaWFbB+7O/5+1Z\na8hdHbhR4IjubRiXpRsF/Ky6xvHS1JXc89FCqp3jmhN6c+FR3fY5J6qINDyVNRFpcNsqq/h0/nre\nnrWGr5dsoDp4o8C4IemMyexIR90oEDbWFm/nj+/O47MF6xnQMYm7Tx/MoE6tvI4l0qSorIlIg6iq\nrmFK/gbembWGT+avZ1tlNR1bxTMmK51xQzrSt71uFAhXzjk+mruOP06cx8ayCi48qhvX/LR3o05w\nLyL/cyBlTb+VIvIDzjnyCrbwzqw1vD97LRvKKkmKj2FsVkfGZulGgUhhZpw0qAPDeqbw148W8syU\n5Xw4dx13njaQY/ukeR1PRGrRkTURAWD5hq28M2sN7+auYcXGbcTGRHFc3zTGZqUzom9qk56DsymY\nvmITN745m6VFWxmb1ZFbRvcnpWWc17FEIpZOg4rIfiksLeeD2d/zdu5a8lYXYwZHdGvLuCEdGTWw\nA62a60aBpqSiqprHv1jKY5PzSYiL4eaT+3Hm0E4a5kOkEaisiUidnHPMW1vC5wsKmbRwPXkFWwDo\n1yGJcVkdGZPVkQ6tdKNAU5dfWMpNb81h+orNDOvRlrtOG0TXlASvY4lEFJU1EdllW2UVU5ZsYNLC\nQiYtLKSwtAIzyMpIZmSfNH46oD192id6HVN8pqbG8cr0Vdz9wUIqq2u46rheXHJMd5ppmA+RBqGy\nJtLErd60jUkLC/l8YSFTl22ksqqGlnExHNM7hZF923Fsn1RdjyT7ZX1JObdOnMeHc9fRt30ifzl9\nEEM6t/Y6lkjYU1kTaWKqqmuYuao4ePRsPYvXlwHQLSWBkX3TOK5vGtld2xAbo6MicnA+mbeOP747\nj/Wl5Zx/ZFeuO7EPLeM0oIDIwdLQHSJNQPG2Sr5cXMSkhYV8saiILdt3EBNlHNatDWdnZzCybxrd\nU1t6HVMixE8HtOfIHm257+NFvPjdCj6Zt44/jxvIcf3aeR1NJOLpyJpImHDOkV9YxucLC5m0oJCc\nlZuocdAmIZYRfdIY2TeNo3unkBSvOzilcc1YuZmb3prN4vVlnDKoA7eOGUBqok6rixwIHVkTiRDl\nO6qZtnwTkxasZ9KiQlZv2g5A/w5J/ObYnozsl0Zmp2SiNUithNDQLq15/8qjeeqrpTwyKZ+lRWW8\nc/lRxDfTWHwijUFlTcRnCkvKmbyokM8XFDIlfwPbKquJi4niJz1TuHR4D0b0SdM8nOK52JgorhjZ\niwHprfjl89P5ywcLuG3sQK9jiUQklTURj9XUOOau3RIc+6yQOWsCY591bBXP6Yekc1zfdhzZo62O\nWogvjeiTxkU/6cazU5ZzdK9Uju+va9hEGprKmogHyip2jn22nkkLi9hQFhj77JDOrbn+xD6M7JtG\n3/aJGjlewsINo/owddlGrn8jjw+vPob2reK9jiQSUVTWRBrR1ooq1peUs76kgsLSctYWl/Pt0g1M\nW7aJyuoaEuNjOKZ3Ksf1TWN471TaauwzCUNxMdE8MmEIox+Zwu/+nctLFx+u6yhFGpDKmshB2FpR\nRWFpRbCIlVO06+tAKSssCSxvraz+0XO7pyZw/rAujOzbjuyurTUivESEHqktuW3MAG54czZPfLmU\ny0f09DqSSMRQWROpZVtlFeuDRauwtILC4J87S1lgXQVlFVU/em5cTBTtkuJplxRHvw5JDO+TSlpi\nYLldUjxpiXGkJcVrcnSJWGdld+KrJUU88OlihvVoq5kORBqIJ2XNzEYBDwPRwDPOubvr2OZs4FbA\nAXnOuZ+FNKRElG2VVbuOdq3frYQVllSwPng0bG8lLC0xjn7tkzimV9yuUrazjKUlxZMUH6NrzKRJ\nMzPuPG0Qs1YVc9Wrs/jPVUdr3D+RBhDyQXHNLBpYDJwAFADTgQnOufm1tukFvAaMdM5tNrM051zh\n3l43FIPijn3sG7bW8Y+5+Fd1jWNDaQWleyhhaUlxtEuMJ21X8QqUstplLKm5SpjIgZixchNnPzmV\n0YM78NA5Wfr9EamD3wfFPQzId84tAzCzV4GxwPxa2/wKeMw5txlgX0UtVHqmtqR8x4+vQRL/MoOU\nlnG7Slm7pPhdX6uEiTSOoV3a8NvjenH/p4s5plcqZwzt5HUkkbDmRVlLB1bXWi4ADt9tm94AZvYN\ngVOltzrnPtr9hczsEuASgM6dOzdK2NruPzuz0d9DRCQS/GZET6bkb+CWd+dySJfWdEtJ8DqSSNjy\n4ja0ug5l7H4uNgboBRwLTACeMbPkHz3Juaecc9nOuezU1NQGDyoiIgcnOsp4aHwWsTFRXPXKLCqr\naryOJBK2vChrBUBGreVOwNo6tnnXObfDObccWESgvImISJjo0Ko5fz1jMHPWbOG+TxZ5HUckbHlR\n1qYDvcysm5nFAuOBibtt8w4wAsDMUgicFl0W0pQiIlJvJw5oz7lHdOapr5bx1eIir+OIhKWQlzXn\nXBVwBfAxsAB4zTk3z8xuN7Mxwc0+Bjaa2XxgMnC9c25jqLOKiEj9/eGU/vRu15JrXstjQ1mF13FE\nwk7Ih+5oLKEYukNERA7OonWljHl0Ckd0b8vzFxxKlKajkibuQIbu0Dw3IiLS6Pq0T+QPp/Tjy8VF\nPPfNcq/jiIQVlTUREQmJc4/owgn92/HXjxYyd80Wr+OIhA2VNRERCQkz454zBtM2IY6rXpmlGWFE\n9pPKmoiIhEzrhFgePCeL5Ru3ctt787yOIxIWVNZERCSkjuzRlsuP7clrOQW8l7f7MJsisjuVNRER\nCbmrj+/FIZ2T+b+35rB60zav44j4msqaiIiEXLPoKB4ePwSAq1+dRVW1pqMS2ROVNRER8URGmxbc\ndfogZq4q5uHPl3gdR8S3VNZERMQzp2Z25KyhnXh0cj7fLdVENSJ1UVkTERFP3TpmAN3aJvC7f+ey\neWul13FEfEdlTUREPJUQF8MjE4awcWsFN7w5m0iZBlGkoaisiYiI5wamt+L3o/ry6fz1vDRtlddx\nRHxFZU1ERHzhwqO6Mbx3Kne8P59F60q9jiPiGyprIiLiC1FRxn1nZZIY34wrX5lJ+Y5qryOJ+ILK\nmoiI+EZqYhwPnJ3J4vVl3PGf+V7HEfEFlTUREfGVY3qncskx3Xlp6io+nrfO6zginlNZExER37nu\np30YlN6K3785m++3bPc6joinVNZERMR3YmOieGTCECqravjtq7lU12g4D2m6VNZERMSXuqUkcPvY\ngUxbvom/T873Oo6IZ1TWRETEt844JJ0xmR156PMlzFi5yes4Ip5QWRMREd8yM+44bSAdk+O56pVc\ntmzf4XUkkZBTWRMREV9Lim/Gw+OHsK6knJvfnqPpqKTJUVkTERHfO6Rza645oTfvz/6e13MKvI4j\nElIqayIiEhYuHd6DI7u35U8T57G0qMzrOCIho7ImIiJhITrKePCcLOKbRXHlv2ZRUaXpqKRpUFkT\nEZGw0b5VPPeemcn870u456NFXscRCQmVNRERCSvH92/H+Ud24dkpy5m8qNDrOCKNzpOyZmajzGyR\nmeWb2Y11fP8CMysys9zg42IvcoqIiD/ddHI/+rZP5LrX8igsLfc6jkijCnlZM7No4DHgJKA/MMHM\n+tex6b+dc1nBxzMhDSkiIr4W3yyav00YwtbKKq59LY8aTUclEcyLI2uHAfnOuWXOuUrgVWCsBzlE\nRCSM9WqXyC2j+/P1kg08M2WZ13FEGo0XZS0dWF1ruSC4bndnmNlsM3vDzDLqeiEzu8TMcswsp6io\nqDGyioiIj/3ssM6MGtCeez5axOyCYq/jiDQKL8qa1bFu9+PX7wFdnXODgc+AF+t6IefcU865bOdc\ndmpqagPHFBERvzMz7j5jEKmJcVz1yizKKqq8jiTS4LwoawVA7SNlnYC1tTdwzm10zlUEF58GhoYo\nm4iIhJnkFrE8dE4WqzZt40/vzvM6jkiD86KsTQd6mVk3M4sFxgMTa29gZh1qLY4BFoQwn4iIhJnD\nu7flipG9eHNmAe/mrvE6jkiDCnlZc85VAVcAHxMoYa855+aZ2e1mNia42VVmNs/M8oCrgAtCnVNE\nRMLLVSN7kt2lNX94ey5rird7HUekwZhzkXG7c3Z2tsvJyfE6hoiIeGjVxm2MevgrhnZpzT8uPAyz\nui6TFvGemc1wzmXvz7aawUBERCJG57Yt+L+T+/H1kg28PG2V13FEGoTKmoiIRJSfH96Zn/RM4a4P\nFrBq4zav44jUm8qaiIhEFDPjr2cOJtqM69/Q7AYS/lTWREQk4qQnN+eW0f2ZtnwTL3y7wus4IvWi\nsiYiIhHprOxOjOiTyj0fL2RZUZnXcUQOmsqaiIhEpMDsBoOJi4nmutfzqNbpUAlTKmsiIhKx2iXF\nc9uYAcxcVcwzX2uydwlPKmsiIhLRxmZ15MQB7bj/08UsWV/qdRyRA6ayJiIiEc3MuPO0QbSMi+Ha\n1/Ooqq7xOpLIAVFZExGRiJfSMo4/jx3I7IItPP7FUq/jiBwQlTUREWkSThncgdGDO/DIpCXMX1vi\ndRyR/aayJiIiTcafxw6kVfNYrnktl8oqnQ6V8KCyJiIiTUbrhFj+cvogFq4r5W+TlngdR2S/qKyJ\niEiTckL/dpx+SDp//2IpeauLvY4jsk8qayIi0uT86dQBpLaM49rX8yjfUe11HJG9UlkTEZEmp1Xz\nZtx9xiDyC8t48LPFXscR2SuVNRERaZKO7ZPGhMMyePqrZcxYudnrOCJ7pLImIiJN1s2n9KdDq+Zc\n93oe2yt1OlT8SWVNRESarJZxMdx75mCWb9jKPR8v9DqOSJ1U1kREpEkb1jOFXxzZhee/WcHUZRu9\njiPyIyprIiLS5N14Ul+6tG3B9W/ksbWiyus4Ij/QIGXNzBIa4nVERES80CI2hvvOyqRg83bu+mCB\n13FEfqBeZc3MhpnZfGBBcDnTzP7eIMlERERC6NCubbjoqG68PG0VXy8p8jqOyC71PbL2IHAisBHA\nOZcHHFPfUCIiIl647sQ+9EhN4IY3ZlNSvsPrOCJAA5wGdc6t3m2V7n0WEZGwFN8smvvOymR9STl3\nvD/f6zgiQP3L2mozGwY4M4s1s+sInhIVEREJR0M6t+bS4T14LaeASQvXex1HpN5l7VLgciAdKACy\ngssiIiJh6+rje9G3fSI3vjmH4m2VXseRJu6gy5qZRQPnOed+7pxr55xLc86d65zb5yA1ZjbKzBaZ\nWb6Z3biX7c40M2dm2QebU0RE5EDFxQROh27aWsmtE+d5HUeauIMua865amDsgT4vWPIeA04C+gMT\nzKx/HdslAlcB0w42o4iIyMEamN6KK0b25J3ctXw0d53XcaQJq+9p0G/M7FEzO9rMDtn52MdzDgPy\nnXPLnHOVwKvUXfr+DNwDlNczo4iIyEG5fERPBnRM4ua357CxrMLrONJE1besDQMGALcD9wcf9+3j\nOelA7TtIC4LrdjGzIUCGc+79vb2QmV1iZjlmllNUpDFxRESkYTWLjuL+szMpKd/BLe/OxTnndSRp\ngmLq82Tn3IiDeJrV9VK7vmkWRWD8tgv24/2fAp4CyM7O1m+QiIg0uL7tk/jt8b259+NFvDf7e8Zk\ndvQ6kjQx9Z3BoJWZPbDz6JaZ3W9mrfbxtAIgo9ZyJ2BtreVEYCDwhZmtAI4AJuomAxER8cqvj+lO\nZkYyf3x3LoWlujpHQqu+p0GfA0qBs4OPEuD5fTxnOtDLzLqZWSwwHpi485vOuS3OuRTnXFfnXFdg\nKjDGOZdTz6wiIiIHJSY6ivvPymR7ZTX/99YcnQ6VkKpvWevhnPtT8GaBZc6524Due3uCc64KuAL4\nmMAAuq855+aZ2e1mNqaeeURERBpFz7SWXH9iHz5bUMhbM9d4HUeakHpdswZsN7OfOOemAJjZUcD2\nfT3JOfcB8MFu6/64h22PrWdGERGRBvHLo7rx8bx13PrePIb1bEuHVs29jiRNQH2PrF0GPGZmK4LX\nlz1KYFYDERGRiBMdZdx3ViZV1Y7fv6nToRIa9Sprzrlc51wmMBgY7Jwb4pzLa5hoIiIi/tOlbQI3\nndyXrxYX8er01ft+gkg91fdu0LvMLNk5V+KcKzGz1mZ2R0OFExER8aNzD+/Ckd3bcsf781m9aZvX\ncSTC1fc06EnOueKdC865zcDJ9XxNERERX4uKMu45czAAN7wxm5oanQ6VxlPfshZtZnE7F8ysORC3\nl+1FREQiQkabFvxhdH++W7aRf05d6XUciWD1LWsvAZ+b2UVmdiHwKfBi/WOJiIj43/hDMxjeO5W7\nP1zIig1bvY4jEaq+NxjcA9wB9CMwR+ifg+tEREQinplx9xmDiIk2rns9j2qdDpVGUN8bDBKAT5xz\n1xGYozPOzJo1SDIREZEw0KFVc249dQA5Kzfz/DfLvY4jEai+p0G/AuLNLB34DPgl8EJ9Q4mIiIST\n0w9J5/h+7bjn40XkF5Z5HUciTH3LmjnntgGnA39zzp0G9K9/LBERkfBhZtx1+kBaxEZz7et5VFXX\neB1JIki9y5qZHQn8HPhPcF19p7ASEREJO2mJ8fx57EDyVhfz5FfLvI4jEeSgypqZ/TP45dvATcDb\nwcnYuwOTGyqciIhIOBk9uAMnD2rPQ58tZuG6Eq/jSIQ42CNrQ82sC3AGcAHwtJm1AYqBWxsmmoiI\nSHgxM/48diBJ8c249rU8duh0qDSAgy1rTwAfAX2BnOBjRvCR0zDRREREwk/blnHcedog5q0t4dFJ\n+V7HkQhwUGXNOfeIc64f8Jxzrnvw0S346N7AGUVERMLKqIHtGZfVkccm5zN3zRav40iYq++guJc1\nVBAREZFIctuYgbRJiOXa1/Io31HtdRwJY/W9G1RERETq0KpFM/565mAWrS/l+jdm45xmN5CDo7Im\nIiLSSEb0SeOGUX14L28tf9P1a3KQNCaaiIhII7pseA/y15fxwKeL6ZHaklMGd/A6koQZHVkTERFp\nRGbGX84YxNAurbn29VxmFxR7HUnCjMqaiIhII4uLiebJ84bSNiGOi1/MYd2Wcq8jSRhRWRMREQmB\nlJZxPHtBNlsrqrj4H9PZXqk7RGX/qKyJiIiESN/2STwyYQjz1pZw7eu51NToDlHZN5U1ERGREDqu\nXzv+76R+fDBnHQ99ttjrOBIGdDeoiIhIiF18dDeWFJbyyKR8eqS1ZGxWuteRxMd0ZE1ERCTEzIw7\nxg3isG5tuP6N2cxctdnrSOJjnpQ1MxtlZovMLN/Mbqzj+5ea2RwzyzWzKWbW34ucIiIijSU2Joon\nzh1K+6R4LvnHDNYUb/c6kvhUyMuamUUDjwEnAf2BCXWUsX855wY557KAe4AHQhxTRESk0bVJiOXZ\n87Op2FHNxS/msLWiyutI4kNeHFk7DMh3zi1zzlUCrwJja2/gnCuptZgA6HYZERGJSL3aJfK3nw1h\n0boSfvtv3SEqP+ZFWUsHVtdaLgiu+wEzu9zMlhI4snZViLKJiIiE3LF90rhldH8+nb+eez9Z5HUc\n8RkvyprVse5H/41wzj3mnOsB/B74Q50vZHaJmeWYWU5RUVEDxxQREQmdC4Z15WeHd+bxL5by5owC\nr+OIj3hR1gqAjFrLnYC1e9n+VWBcXd9wzj3lnMt2zmWnpqY2YEQREZHQMjNuGzOAYT3actNbc5i+\nYpPXkcQnvChr04FeZtbNzGKB8cDE2huYWa9ai6cAS0KYT0RExBPNoqP4+88PIb11c379zxms3rTN\n60jiAyEva865KuAK4GNgAfCac26emd1uZmOCm11hZvPMLBe4Bjg/1DlFRES8kNwilmfOz6aquoaL\nX8yhtHyH15HEY+ZcZNx1kp2d7XJycryOISIi0iCmLNnA+c//l+G9U3n6F9lER9V1ybeEKzOb4ZzL\n3p9tNYOBiIiID/2kVwq3jhnApIWF3P3hAq/jiIc0N6iIiIhPnXdEF/LXl/L018vpmdaScw7t7HUk\n8YCOrImIiPjYLaP7c3SvFG5+ey5Tl230Oo54QGVNRETEx2Kio3j0Z4fQpW0LLn1pBis3bvU6koSY\nypqIiIjPtWrejGfPPxSAC1+YTonuEG1SVNZERETCQNeUBB7/+VBWbtzGFf+aRVV1jdeRJERU1kRE\nRMLEkT3acse4gXy1uIg7/qM7RJsK3Q0qIiISRsYf1pklhWU8OyVwh+i5R3TxOpI0Mh1ZExERCTP/\nd3I/RvRJ5U8T5/FN/gav40gjU1kTEREJM9FRxiMThtAjNYHLXprBsqIyryNJI1JZExERCUOJ8YE7\nRGOio7joxRy2bNMdopFKZU1ERCRMZbRpwZPnDaVg8zZ+868Z7NAdohFJZU1ERCSMHdq1DXedNohv\n8jdy68R5OOe8jiQNTHeDioiIhLmzsjPILyrjyS+X0btdIucP6+p1JGlAKmsiIiIR4IYT+7K0cCu3\nvTePrikJDO+d6nUkaSA6DSoiIhIBoqOMh8dn0ad9Ele8PJP8wlKvI0kDUVkTERGJEAlxMTxzfjZx\nzaK48IUcNm+t9DqSNACVNRERkQiSntycJ8/LZl1JOZe+NIPKKt0hGu5U1kRERCLM0C6tueeMwUxb\nvolb3pmrO0TDnG4wEBERiUDjhqSTX1jGo5Pz6dWuJRcf3d3rSHKQVNZEREQi1DUn9GZpURl3frCA\n7qkJjOzbzutIchB0GlRERCRCRUUZ95+dyYCOSVz5r1ksWqc7RMORypqIiEgEaxEbw9O/yCYhLoaL\nXpzOhrIKryPJAVJZExERiXAdWjXn6V9kU1RawaX/nEFFVbXXkeQAqKyJiIg0AZkZydx/diY5Kzdz\n0Qs5lJbv8DqS7CeVNRERkSZi9OCO3HdWJt8t28j4p6ZSVKpTouFAZU1ERKQJOXNoJ545P5tlRVs5\n4/FvWb5hq9eRZB88KWtmNsrMFplZvpndWMf3rzGz+WY228w+N7MuXuQUERGJRCP6pPHKJUdQVlHF\nmY9/S97qYq8jyV6EvKyZWTTwGHAS0B+YYGb9d9tsFpDtnBsMvAHcE9qUIiIikS0rI5k3Lj2S5rHR\nTHh6Kl8uLvI6kuyBF0fWDgPynXPLnHOVwKvA2NobOOcmO+e2BRenAp1CnFFERCTidU9tyVuXDaNr\n2wQuemE6b80s8DqS1MGLspYOrK61XBBctycXAR/W9Q0zu8TMcswsp6hI/yMQERE5UGlJ8fz710dw\nWLc2XPNaHk9+uVRzifqMF2XN6lhX598KMzsXyAburev7zrmnnHPZzrns1NTUBowoIiLSdCTGN+P5\nXx7K6MEd+MuHC7njPwuoqVFh8wsv5gYtADJqLXcC1u6+kZkdD9wMDHfO6d5iERGRRhQXE80j44eQ\nmhjHs1OWU1hawX1nDSYuJtrraE2eF2VtOtDLzLoBa4DxwM9qb2BmQ4AngVHOucLQRxQREWl6oqKM\nP47uT7ukeO7+cCGbtlbwxLlDSYxv5nW0Ji3kp0Gdc1XAFcDHwALgNefcPDO73czGBDe7F2gJvG5m\nuWY2MdQ5RUREmiIz49LhPbj/rEymLdvEOU9OpbC03OtYTZpFykWE2dnZLicnx+sYIiIiEeOLRYVc\n9tJMUhJj+ceFh9MtJcHrSBHDzGY457L3Z1vNYCAiIiJ1OjY4eO7WimrO0OC5nlFZExERkT3Kykjm\nzcuGkRAXzfinpvLFIl1KHmoqayIiIrJX3VISePOyYXRLSeDiF3M0eG6IqayJiIjIPqUl/nDw3Cc0\neG7IqKyJiIjIPSBCcwAADL5JREFUftk5eO6pmR25+8OF3P7+fA2eGwJejLMmIiIiYSouJpqHz8ki\ntWUcz32znKLSCu4/O1OD5zYilTURERE5IFFRxi2j+9EuKY6/fLiQTVsrefI8DZ7bWHQaVERERA6Y\nmfHr4T144OxM/rs8OHhuiQbPbQwqayIiInLQTj+kE8+cn82KjVs5/fFvWVZU5nWkiKOyJiIiIvVy\nbJ80XvnVEWyvrObMJ74jV4PnNiiVNREREam3zIxk3ggOnjvhqalM1uC5DUZlTURERBrEzsFzu6cG\nBs99Y4YGz20IKmsiIiLSYNIS43n1kiM4onsbrns9j8e/0OC59aWyJiIiIg0qMb4Zz19wGGMyO/LX\njxZy23saPLc+NM6aiIiINLjYmCgeOieL1MQ4np2ynKKyCh7Q4LkHRWVNREREGkVg8Nz+tEuK464P\nFrKprJInfzGUJA2ee0B0GlREREQa1SXH9ODBczKZvkKD5x4MlTURERFpdKcN6cSzFxzKyuDguUs1\neO5+U1kTERGRkBjeO5VXLwkOnvv4t8xatdnrSGFBZU1ERERCZnCnZN68bBiJ8c342dPTmLxQg+fu\ni8qaiIiIhFTXlATeuOzIwOC5/8jhscn5bK+s9jqWb6msiYiISMilJcbz718fyci+adz78SKOvmcy\nz01ZTvkOlbbdqayJiIiIJ1rGxfD0L7J5/dIj6ZXWktvfn8/weyfzz+9WUFGl0raTRcoUENnZ2S4n\nJ8frGCIiInKQvl26gQc+WUzOys2kJzfnypE9OWNoJ5pFR96xJTOb4ZzL3q9tVdZERETEL5xzfL1k\nA/d/upi81cV0btOCq4/rxdisjsREUGk7kLIWOT+1iIiIhD0z45jeqbzzm2E8e342ifExXPt6Hj99\n6CvezV1DdROcY1RlTURERHzHzDiuXzvev/InPHHuITSLiuLqV3M56eGv+GDO901qYnhPypqZjTKz\nRWaWb2Y31vH9Y8xspplVmdmZXmQUERER75kZowZ24MOrj+ZvE4ZQXeP4zcszOeVvU/h0/noi5XKu\nvQl5WTOzaOAx4CSgPzDBzPrvttkq4ALgX6FNJyIiIn4UFWWcmtmRT343nAfOzmRbZRW/+kcO4x77\nhi8WFUZ0afPiyNphQL5zbplzrhJ4FRhbewPn3Arn3GygxoN8IiIi4lPRUcbph3Tis2uGc88Zg9lQ\nVskFz0/nzCe+45v8DRFZ2rwoa+nA6lrLBcF1B8zMLjGzHDPLKSoqapBwIiIi4n/NoqM4+9AMJl93\nLHeMG8iazdv5+TPTGP/UVP67fJPX8RqUF2XN6lh3UDXYOfeUcy7bOZedmppaz1giIiISbmJjojj3\niC58cf2x3Hpqf5Zt2MrZT37Hec9OY2aETBTvRVkrADJqLXcC1nqQQ0RERCJEfLNoLjiqG19dP4Kb\nT+7HvLUlnP73b/nl8/9lTsEWr+PVixdlbTrQy8y6mVksMB6Y6EEOERERiTDNY6P51THd+fqGEdww\nqg8zVxVz6qNTuOQfOSz4vsTreAfFkxkMzOxk4CEgGnjOOXenmd0O5DjnJprZocDbQGugHFjnnBuw\nt9fUDAYiIiKyu9LyHTw3ZQXPfL2M0ooqThnUgd8e34te7RI9zaXppkRERERq2bJtB09/vYznv1nO\nth3VjM3syNXH96ZbSoIneVTWREREROqwaWslT361lBe/XcGOasfpQ9K56rheZLRpEdIcKmsiIiIi\ne1FUWsHjXyzlpWkrqalxnJWdwZUje9IxuXlI3l9lTURERGQ/rC8p57HJ+bzy31UYxoTDMrh8RE/S\nkuIb9X0PpKxpIncRERFpstolxXP72IF8cf0IzhiazsvTVjHusW+o9tFE8TFeBxARERHxWnpyc/5y\n+mAuG96TZRvKiI6qawx/b6isiYiIiAR1btuCzm1De7PBvug0qIiIiIiPqayJiIiI+JjKmoiIiIiP\nqayJiIiI+JjKmoiIiIiPqayJiIiI+JjKmoiIiIiPqayJiIiI+JjKmoiIiIiPRcxE7mZWBKwMwVul\nABtC8D7hSvtn37SP9k77Z9+0j/ZO+2fftI/2LhT7p4tzLnV/NoyYshYqZpbjnMv2Oodfaf/sm/bR\n3mn/7Jv20d5p/+yb9tHe+W3/6DSoiIiIiI+prImIiIj4mMragXvK6wA+p/2zb9pHe6f9s2/aR3un\n/bNv2kd756v9o2vWRERERHxMR9ZEREREfExlTURERMTHVNb2k5mNMrNFZpZvZjd6ncdvzCzDzCab\n2QIzm2dmV3udyY/MLNrMZpnZ+15n8SMzSzazN8xsYfDv0pFeZ/ITM/td8Pdrrpm9YmbxXmfympk9\nZ2aFZja31ro2ZvapmS0J/tnay4xe2sP+uTf4OzbbzN42s2QvM3qtrn1U63vXmZkzsxQvsu2ksrYf\nzCwaeAw4CegPTDCz/t6m8p0q4FrnXD/gCOBy7aM6XQ0s8DqEjz0MfOSc6wtkon21i5mlA1cB2c65\ngUA0MN7bVL7wAjBqt3U3Ap8753oBnweXm6oX+PH++RQY6JwbDCwGbgp1KJ95gR/vI8wsAzgBWBXq\nQLtTWds/hwH5zrllzrlK4FVgrMeZfMU5971zbmbw61IC/8ime5vKX8ysE3AK8IzXWfzIzJKAY4Bn\nAZxzlc65Ym9T+U4M0NzMYoAWwFqP83jOOfcVsGm31WOBF4NfvwiMC2koH6lr/zjnPnHOVQUXpwKd\nQh7MR/bwdwjgQeAGwPM7MVXW9k86sLrWcgEqIntkZl2BIcA0b5P4zkMEfvFrvA7iU92BIuD54Kni\nZ8wswetQfuGcWwPcR+B/+d8DW5xzn3ibyrfaOee+h8B/JIE0j/P42YXAh16H8BszGwOscc7leZ0F\nVNb2l9WxzvOm7Udm1hJ4E/itc67E6zx+YWajgULn3Ayvs/hYDHAI8LhzbgiwlaZ9+uoHgtddjQW6\nAR2BBDM719tUEs7M7GYCl7C87HUWPzGzFsDNwB+9zrKTytr+KQAyai13QqcffsTMmhEoai87597y\nOo/PHAWMMbMVBE6jjzSzl7yN5DsFQIFzbucR2TcIlDcJOB5Y7pwrcs7tAN4Chnmcya/Wm1kHgOCf\nhR7n8R0zOx8YDfzcacDV3fUg8J+ivOBndidgppm19yqQytr+mQ70MrNuZhZL4KLeiR5n8hUzMwLX\nGi1wzj3gdR6/cc7d5Jzr5JzrSuDvzyTnnI6K1OKcWwesNrM+wVXHAfM9jOQ3q4AjzKxF8PftOHQD\nxp5MBM4Pfn0+8K6HWXzHzEYBvwfGOOe2eZ3Hb5xzc5xzac65rsHP7ALgkOBnlCdU1vZD8ELMK4CP\nCXw4vuacm+dtKt85CjiPwBGj3ODjZK9DSdi5EnjZzGYDWcBdHufxjeARxzeAmcAcAp/fvpoSxwtm\n9grwHdDHzArM7CLgbuAEM1tC4G6+u73M6KU97J9HgUTg0+Bn9ROehvTYHvaRr2i6KREREREf05E1\nERERER9TWRMRERHxMZU1ERERER9TWRMRERHxMZU1ERERER9TWRORsGdmyWb2m+DXx5rZ+43wHheY\n2aMH+JwVZpZSx/pbzey6hksnIpFMZU1EIkEy8JsDeYKZRTdSFhGRBqWyJiKR4G6gh5nlAvcCLc3s\nDTNbaGYvB0f833mk649mNgU4y8x6mNlHZjbDzL42s77B7c4ys7lmlmdmX9V6n47B7ZeY2T07V5rZ\nBDObE3zOX+sKaGY3m9kiM/sM6FPXNiIidYnxOoCISAO4ERjonMsys2MJTC80gMAcvt8QmGFjSnDb\ncufcTwDM7HPgUufcEjM7HPg7MJLABM4nOufWmFlyrffJAoYAFcAiM/sbUA38FRgKbAY+MbNxzrl3\ndj7JzIYSmGZsCIHP3ZnAjIbfDSISiVTWRCQS/dc5VwAQPNrWlf+VtX8H17ckMBH668EDbwBxwT+/\nAV4ws9cITJi+0+fOuS3B588HugBtgS+cc0XB9S8DxwDv1Hre0cDbO+dhNDPNLSwi+01lTUQiUUWt\nr6v54Wfd1uCfUUCxcy5r9yc75y4NHmk7Bcg1s53b1PW6tvvz90Bz+4nIQdE1ayISCUoJTEy935xz\nJcByMzsLwAIyg1/3cM5Nc879EdgAZOzlpaYBw80sJXjTwgTgy922+Qo4zcyam1kicOqBZBWRpk1H\n1kQk7DnnNprZN2Y2F9gOrN/Pp/4ceNzM/gA0A14F8oB7zawXgaNmnwfX/egIXPC9vzezm4DJwe0/\ncM69u9s2M83s30AusBL4+kB/RhFpusw5HZkXERER8SudBhURERHxMZU1ERERER9TWRMRERHxMZU1\nERERER9TWRMRERHxMZU1ERERER9TWRMRERHxsf8H6F6zmsROcJoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe4815c8da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precisions = np.zeros(15)\n",
    "recall = np.zeros(15)\n",
    "fscores = np.zeros(15)\n",
    "for i in range(15):\n",
    "    tprecision, trecall, tfscore = word_length_baseline(training_file, i)\n",
    "    precisions[i] = tprecision\n",
    "    recall[i] = trecall\n",
    "    fscores[i] = tfscore\n",
    "\n",
    "plt.figure(num=1, figsize=(10, 10))\n",
    "pr_plt = plt.subplot(2 ,1, 1, xlabel=\"recall\", ylabel=\"precision\", label=\"pr\")\n",
    "pr_plt.set_title(\"precision/recall\", y=1.08)\n",
    "pr_plt.plot(recall, precisions)\n",
    "\n",
    "fs_plt = plt.subplot(2 ,1, 2, xlabel=\"threshold\", ylabel=\"fscore\", label=\"fs\")\n",
    "fs_plt.set_title(\"fscore measure\", y=1.01)\n",
    "fs_plt.plot(range(15), fscores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that once the recall is big the precision is small and vice versa, that's because the two complement eachother. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go about building the \"ideal\" threshold classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Finds the best length threshold by f-score, and uses this threshold to\n",
    "## classify the training and development set\n",
    "def word_length_threshold(training_file, development_file):\n",
    "    best_tfscore = 0.0\n",
    "    best_i = 1\n",
    "    i = 1\n",
    "    while(True):\n",
    "        tprecision, trecall, tfscore = word_length_baseline(training_file, i)\n",
    "        if(tfscore < best_tfscore):\n",
    "            break\n",
    "        else:\n",
    "            best_i = i\n",
    "            i += 1\n",
    "            best_tfscore = tfscore\n",
    "            \n",
    "    tprecision, trecall, tfscore = word_length_baseline(training_file, best_i)\n",
    "    dprecision, drecall, dfscore = word_length_baseline(development_file, best_i)\n",
    "    training_performance = [tprecision, trecall, tfscore]\n",
    "    development_performance = [dprecision, drecall, dfscore]\n",
    "    return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Precision: 0.6007401315789473 \n",
      "Training Recall: 0.8440207972270364 \n",
      "Training Fscore: 0.7018976699495555\n",
      "\n",
      "Dev Precision: 0.6053511705685619 \n",
      "Dev Recall: 0.8660287081339713 \n",
      "Dev Fscore: 0.7125984251968505\n"
     ]
    }
   ],
   "source": [
    "training_performance, development_performance = word_length_threshold(training_file, development_file)\n",
    "tr_precision, tr_recall, tr_fscore = training_performance\n",
    "dv_precision, dv_recall, dv_fscore = development_performance\n",
    "print(\"Training Precision: {} \\nTraining Recall: {} \\nTraining Fscore: {}\\n\".format(tr_precision, tr_recall, tr_fscore))\n",
    "print(\"Dev Precision: {} \\nDev Recall: {} \\nDev Fscore: {}\".format(dv_precision, dv_recall, dv_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2.3 - Word Frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to implement a method that classifies based on the frequency of a given word in the language, for that we use Google Ngram Counts - so we first have to load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1.2.3: Word frequency thresholding\n",
    "\n",
    "## Loads Google NGram counts\n",
    "def load_ngram_counts(ngram_counts_file): \n",
    "    counts = defaultdict(int) \n",
    "    with gzip.open(ngram_counts_file, 'rt') as f: \n",
    "        for line in f:\n",
    "            token, count = line.strip().split('\\t') \n",
    "            if token[0].islower(): \n",
    "                counts[token] = int(count) \n",
    "    return counts\n",
    "\n",
    "ngram_path = abspath(join(dirname(\"__file__\"), \"data/ngram_counts.txt.gz\"))\n",
    "ngram_counts = load_ngram_counts(ngram_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have it, we can write a code that use thresholding - if the word count of a certain word is lower than the threshold it gives it 0 and otherwise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_frequency_baseline(data_file, counts, threshold):\n",
    "    words, actual_labels = load_file(data_file)\n",
    "    threshold_labels = [(1 if(counts[word] >= threshold) else 0) for word in words]\n",
    "    \n",
    "    precision = get_precision(threshold_labels, actual_labels)\n",
    "    recall = get_recall(threshold_labels, actual_labels)\n",
    "    fscore = get_fscore(threshold_labels, actual_labels)\n",
    "    preformance = [precision, recall, fscore]\n",
    "    return preformance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test this method using different values for the threshold - We used a range of 30 random numbers between 10 to 2000000 (after examining the Google Ngrams Count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAAJwCAYAAADWXSa8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe481b4cba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precisions = np.zeros(30)\n",
    "recall = np.zeros(30)\n",
    "fscores = np.zeros(30)\n",
    "thresholds = np.sort(np.random.randint(10, 2000000, size=30, dtype=int))\n",
    "for i in range(30):\n",
    "    tprecision, trecall, tfscore = word_frequency_baseline(training_file, ngram_counts, thresholds[i])\n",
    "    precisions[i] = tprecision\n",
    "    recall[i] = trecall\n",
    "    fscores[i]=tfscore\n",
    "\n",
    "    \n",
    "plt.figure(num=3, figsize=(10, 10))\n",
    "pr_plt = plt.subplot(2 ,1, 1, xlabel=\"recall\", ylabel=\"precision\", label=\"pr\")\n",
    "pr_plt.set_title(\"precision/recall\", y=1.08)\n",
    "pr_plt.plot(recall, precisions)\n",
    "\n",
    "fs_plt = plt.subplot(2 ,1, 2, xlabel=\"threshold\", ylabel=\"fscore\", label=\"fs\")\n",
    "fs_plt.set_title(\"fscore measure\", y=1.01)\n",
    "fs_plt.plot(thresholds, fscores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see (and it makes sense) that the tresholds can vary the recall and precision - a higher recall will be when the threshold is high, and a lower one when it's low and the exact opposite happens with the precision. We do see once we set the threshold lower we get a better fscore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finds the best frequency threshold by f-score, and uses this threshold to\n",
    "## classify the training and development set\n",
    "def word_frequency_threshold(training_file, development_file, counts):\n",
    "    best_tfscore = 0.0\n",
    "    i = 1\n",
    "    threshold = np.random.randint(10, 2000000, size=1, dtype=int)\n",
    "    while(True):\n",
    "        tprecision, trecall, tfscore = word_frequency_baseline(training_file, counts, threshold)\n",
    "        if(tfscore < best_tfscore):\n",
    "            break\n",
    "        else:\n",
    "            threshold = np.random.randint(10, 2000000, size=1, dtype=int)\n",
    "            best_tfscore = tfscore\n",
    "            \n",
    "    tprecision, trecall, tfscore = word_frequency_baseline(training_file, counts, threshold)\n",
    "    dprecision, drecall, dfscore = word_frequency_baseline(development_file, counts, threshold)\n",
    "    \n",
    "    training_performance = [tprecision, trecall, tfscore]\n",
    "    development_performance = [dprecision, drecall, dfscore]\n",
    "    return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Precision: 0.4063756461803561 \n",
      "Training Recall: 0.8174465626805315 \n",
      "Training Fscore: 0.5428735852676002\n",
      "\n",
      "Dev Precision: 0.3931034482758621 \n",
      "Dev Recall: 0.8181818181818182 \n",
      "Dev Fscore: 0.531055900621118\n"
     ]
    }
   ],
   "source": [
    "training_performance, development_performance = word_frequency_threshold(training_file, development_file, ngram_counts)\n",
    "tr_precision, tr_recall, tr_fscore = training_performance\n",
    "dv_precision, dv_recall, dv_fscore = development_performance\n",
    "print(\"Training Precision: {} \\nTraining Recall: {} \\nTraining Fscore: {}\\n\".format(tr_precision, tr_recall, tr_fscore))\n",
    "print(\"Dev Precision: {} \\nDev Recall: {} \\nDev Fscore: {}\".format(dv_precision, dv_recall, dv_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3 - Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.3.1 - Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tried different baselines and saw the results from them, that were not that great, it's time to do actual classification.\n",
    "<br>The method we are going to use is called Naive Bayes Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We personally find that explaining this method via an example makes it a bit easier to grasp instead of just overloading people with probability formulas.\n",
    "<br>We shall use our example on the given problem considering it is most fitting, and afterwards we will implement the actual code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our dataset which is composed of two types of labels - 0 for a simple word, and 1 for a complex word.\n",
    "Now lets assume (for the sake of the example, the actual dataset might act differently but we shall see later) someone told us the following - there is a 70% chance of a word being simple, and a 30% chance that a word is complexed.\n",
    "Another thing they tell you is that the mean of the word length of the simple words is 4 and the mean of word length of the complex words is 7.\n",
    "<br>Now we got a new word, and it has a length of 3, what will we guess it's label is? Well, an educated guess will be to say there's a big chance it's a simple word..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>__How did we assume that?__ We saw there is a high chance of being a simple word, and then we also saw the length of the word is 3 and usually shorter words are simple - so by likelihood the chances of our word being simple are pretty high.\n",
    "<br>The _Naive Bayse Classifier_ works pretty much just like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>__How exactly does it work__\n",
    "<br> First thing to note is that the classifier assumes independence among features, this is not always true in real life (actually, it usually isn't...) but it simplifies the model - in our case it will assume there's no relation between the length of a word and it's frequency (although in reallity it's safe to assume a really long word won't be that frequent). \n",
    "Because of how wrong this assumption is, the classifier got the name _naive_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the classifier does is to find the probability of belonging to a class, given a set of features, in our case we can write it down as $P(simple | f_{length}, f_{freq})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have that we can use the Bayse rule to get this probabilty:\n",
    "<br>$P(simple \\vert f_{length}, f_{freq}) = \\frac{P(simple) * P(f_{length}, f_{freq} \\vert simple)}{P(f_{length}, f_{freq})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know $P(simple)=0.7$ and we don't need $P(f_{length}, f_{freq})$ to build a classifier, so all we are left to do is calculate $P(f_{length}, f_{freq} \\vert simple)$ . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply the conditional probability formula we will get:\n",
    "<br>$P(f_{length}, f_{freq} \\vert simple) = P(f_{length} \\vert simple) * P(f_{freq} \\vert simple, f_{length})$\n",
    "<br><br>Or in the general case (not our example) for $n$ features and a label $l$ we get:\n",
    "<br>$P(f_1, f_2, f_3, \\dots , f_n | l) = P(f_1 \\vert l) * P(f_2 \\vert l, f_1) * \\dots * P(f_n \\vert l, f_1, \\dots, f_{n-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can note that if we had n features, this will require a lot of data because we will have to have data for each of these assumption, but we don't actually need it. Why? well, this is where the _naive_ part comes in handy - we assumed all features are independent, so what we actually get is:\n",
    "<br> $P(f_{length}, f_{freq} | simple) = P(f_{length} \\vert simple) * P(f_{freq} \\vert simple)$\n",
    "<br><br>Or in the general case:\n",
    "<br>$P(f_1, f_2, f_3, \\dots , f_n \\vert l) = \\prod{P(f_i|l)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now finally to classify a new vector of features we have to choose the label for it (simple 0 or complex 1), to do so all we have to do is:\n",
    "<br>\n",
    "$Classifier(f_{l'}, f_{f'}) = \\arg\\max_{s\\in{0, 1}}P(f_{l'}, f_{f'} \\vert s)$ \n",
    "<br><br>Or in the general case for $m$ labels and $n$ features:\n",
    "<br>\n",
    "$Classifier(f_{1},\\dots, f_{n}) = \\arg\\max_{s\\in{0,\\dots,m}}P(f_{1},\\dots, f_{n} \\vert s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__implementation__\n",
    "<br>So now that we get the reasoning behind this method, we can go along and implement the classifier.\n",
    "<br>One last thing to note is that we were asked to use the Guassian based classifier, this classifier assumes the features follow a _normal_ distribution (meaning all the features falls on a normal curve)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code results in the classifier (note the code comments for a step by step explenation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1.3.1: Naive Bayes\n",
    "        \n",
    "## Trains a Naive Bayes classifier using length and frequency features\n",
    "def naive_bayes(training_file, development_file, counts):\n",
    "    #import training dataset\n",
    "    t_words, t_labels = load_file(training_file)\n",
    "    t_features = {}\n",
    "    # get length features\n",
    "    t_features[\"length\"] = np.array([len(word) for word in t_words])\n",
    "    # get frequency features\n",
    "    t_features[\"frequency\"] = np.array([counts[word] for word in t_words])\n",
    "    # build features array\n",
    "    X_t_original = np.array([t_features[\"length\"], t_features[\"frequency\"]]).T\n",
    "    \n",
    "    # normalize features\n",
    "    t_mean = np.mean(X_t_original)\n",
    "    t_sd = np.std(X_t_original)\n",
    "    X_t_scaled = (X_t_original - t_mean)/t_sd\n",
    "    \n",
    "    # train the classifier\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_t_scaled, t_labels)\n",
    "    \n",
    "    # extract features for development file\n",
    "    d_words, d_labels = load_file(development_file)\n",
    "    d_features = {}\n",
    "    # get length features\n",
    "    d_features[\"length\"] = np.array([len(word) for word in d_words])\n",
    "    # get frequency features\n",
    "    d_features[\"frequency\"] = np.array([counts[word] for word in d_words])\n",
    "    # build features array\n",
    "    X_d_original = np.array([d_features[\"length\"], d_features[\"frequency\"]]).T\n",
    "    \n",
    "    # normalize development features - note how we use the training mean and sd\n",
    "    X_d_scaled = (X_d_original - t_mean)/t_sd\n",
    "    \n",
    "    dev_pred = clf.predict(X_d_scaled)\n",
    "    train_pred = clf.predict(X_t_scaled)\n",
    "    \n",
    "    tprecision = get_precision(train_pred, t_labels)\n",
    "    trecall = get_recall(train_pred, t_labels)\n",
    "    tfscore = get_fscore(train_pred, t_labels)\n",
    "    \n",
    "    dprecision = get_precision(dev_pred, d_labels)\n",
    "    drecall = get_recall(dev_pred, d_labels)\n",
    "    dfscore = get_fscore(dev_pred, d_labels)\n",
    "    \n",
    "    training_performance = [tprecision, trecall, tfscore]\n",
    "    development_performance = [dprecision, drecall, dfscore]\n",
    "    return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Precision: 0.4853700516351119 \n",
      "Training Recall: 0.9774696707105719 \n",
      "Training Fscore: 0.6486486486486487\n",
      "\n",
      "Dev Precision: 0.4610983981693364 \n",
      "Dev Recall: 0.9641148325358851 \n",
      "Dev Fscore: 0.6238390092879257\n"
     ]
    }
   ],
   "source": [
    "training_performance, development_performance = naive_bayes(training_file, development_file, ngram_counts)\n",
    "tr_precision, tr_recall, tr_fscore = training_performance\n",
    "dv_precision, dv_recall, dv_fscore = development_performance\n",
    "print(\"Training Precision: {} \\nTraining Recall: {} \\nTraining Fscore: {}\\n\".format(tr_precision, tr_recall, tr_fscore))\n",
    "print(\"Dev Precision: {} \\nDev Recall: {} \\nDev Fscore: {}\".format(dv_precision, dv_recall, dv_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__#TODO__ _add comparision between the classifiers, add more info about each one (the results)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.4 - ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases the way a words is classified is based on it's context, meaning sometimes a word can be classified as a simple word and sometimes as a difficult word, all because of the context it appeared in. \n",
    "\n",
    "We would like to show an example for that - in order to do so, we will find two instances of the same word labeled differently in the training set (the code can easily be adapted to any other set, we chose the training one because it made more sense), then we will explore the context trying to infere why they were tagged differently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we'd like to do is to load the context as well as the words, so we can show how it modifies the classification. In order to do so we need to change the file loading code a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_file_with_context(data_file):\n",
    "    words = []\n",
    "    labels = []   \n",
    "    contexts = []\n",
    "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i > 0:\n",
    "                line_split = line[:-1].split(\"\\t\")\n",
    "                words.append(line_split[0].lower())\n",
    "                labels.append(int(line_split[1]))\n",
    "                contexts.append(line_split[3])\n",
    "            i += 1\n",
    "    return words, labels, contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And using the code one the training data we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words, labels, contexts = load_file_with_context(training_file)\n",
    "print(words[0],labels[0],contexts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what we need to do is find a words with multiple instances that differ in how it's labeled. \n",
    "<br>To do so we will find for each word all it's instances in the words list, and then for words with mulitple labels we will look into the labels to see if they differ, if so we will check the context in which they appear to get a better grasp on why the context matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_context_sensitive_words(words, labels, contexts):\n",
    "    words = np.array(words)\n",
    "    differences = []\n",
    "    for word in words:\n",
    "        ii = np.where(words == word)[0]\n",
    "        if(len(ii) > 1):\n",
    "            diff = [(i, j) for i in ii for j in ii if(not labels[i] == labels[j])]\n",
    "            if(len(diff) > 0):\n",
    "                # We only have to iterate it half way because the other half is a mirror of the first\n",
    "                for i in range(int(len(diff)/2)):\n",
    "                    differences.append((words[diff[i][0]], diff[i][0], diff[i][1]))\n",
    "    return differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ambiguity = find_context_sensitive_words(words, labels, contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can show all these words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for couple in ambiguity:\n",
    "    word, index_a, index_b = couple\n",
    "    print(\"Word:\", word)\n",
    "    print(\"Labeled as {} in index {} and as {} in index {}\".format(labels[index_a], index_a, labels[index_b], index_b))\n",
    "    print(\"First context:\\n\", contexts[index_a], \"\\n\")\n",
    "    print( \"Second context:\\n\", contexts[index_b], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at some of the words one can see how and why they were labeled differently:\n",
    "- If we look at the word _sprouts_ it appears in two contexts - firstly as a name of a place \"Sprouts Farmer\" which makes it a very easy identification, but then as a 'sprinkle of sprouts' which is much more complexed (especially if like us you're not a native English speaker and had to google 'sprouts')\n",
    "- If we look at the word _element_ , seemengly a simple word, it can come as an actual element like in the first context(\"the heaviest element\") but it can also appear in as a slang word - \"in his element\" like in the second context, which complicate things. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Remark___ : We did now want to load the entire notebook as part of our notebook because that will just be overloading on our notebook, but we do want to touch upon required information. So we imported the methods we need from the original notebook and implemented methods to fit the requirements of our question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to do is to explore the dataset they used for the classifiers implementation. In order to do that we will first have to load the dataset(note that we used the code they implemented to do so, but we modified it for our need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReutersParser(html_parser.HTMLParser):\n",
    "    \"\"\"Utility class to parse a SGML file and yield documents one at a time.\"\"\"\n",
    "\n",
    "    def __init__(self, encoding='latin-1'):\n",
    "        html_parser.HTMLParser.__init__(self)\n",
    "        self._reset()\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        method = 'start_' + tag\n",
    "        getattr(self, method, lambda x: None)(attrs)\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        method = 'end_' + tag\n",
    "        getattr(self, method, lambda: None)()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.in_title = 0\n",
    "        self.in_body = 0\n",
    "        self.in_topics = 0\n",
    "        self.in_topic_d = 0\n",
    "        self.title = \"\"\n",
    "        self.body = \"\"\n",
    "        self.topics = []\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "    def parse(self, fd):\n",
    "        self.docs = []\n",
    "        for chunk in fd:\n",
    "            self.feed(chunk.decode(self.encoding))\n",
    "            for doc in self.docs:\n",
    "                yield doc\n",
    "            self.docs = []\n",
    "        self.close()\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if self.in_body:\n",
    "            self.body += data\n",
    "        elif self.in_title:\n",
    "            self.title += data\n",
    "        elif self.in_topic_d:\n",
    "            self.topic_d += data\n",
    "\n",
    "    def start_reuters(self, attributes):\n",
    "        pass\n",
    "\n",
    "    def end_reuters(self):\n",
    "        self.body = re.sub(r'\\s+', r' ', self.body)\n",
    "        self.docs.append({'title': self.title,\n",
    "                          'body': self.body,\n",
    "                          'topics': self.topics})\n",
    "        self._reset()\n",
    "\n",
    "    def start_title(self, attributes):\n",
    "        self.in_title = 1\n",
    "\n",
    "    def end_title(self):\n",
    "        self.in_title = 0\n",
    "\n",
    "    def start_body(self, attributes):\n",
    "        self.in_body = 1\n",
    "\n",
    "    def end_body(self):\n",
    "        self.in_body = 0\n",
    "\n",
    "    def start_topics(self, attributes):\n",
    "        self.in_topics = 1\n",
    "\n",
    "    def end_topics(self):\n",
    "        self.in_topics = 0\n",
    "\n",
    "    def start_d(self, attributes):\n",
    "        self.in_topic_d = 1\n",
    "\n",
    "    def end_d(self):\n",
    "        self.in_topic_d = 0\n",
    "        self.topics.append(self.topic_d)\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "\n",
    "def stream_reuters_documents(data_path=None):\n",
    "    DOWNLOAD_URL = ('http://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "                        'reuters21578-mld/reuters21578.tar.gz')\n",
    "    ARCHIVE_FILENAME = 'reuters21578.tar.gz'\n",
    "    data_path = os.path.join(get_data_home(), \"reuters\")\n",
    "    if not os.path.exists(data_path):\n",
    "        \"\"\"Download the dataset.\"\"\"\n",
    "        print(\"downloading dataset (once and for all) into %s\" %\n",
    "              data_path)\n",
    "        os.mkdir(data_path)\n",
    "\n",
    "        def progress(blocknum, bs, size):\n",
    "            total_sz_mb = '%.2f MB' % (size / 1e6)\n",
    "            current_sz_mb = '%.2f MB' % ((blocknum * bs) / 1e6)\n",
    "            if _not_in_sphinx():\n",
    "                sys.stdout.write(\n",
    "                    '\\rdownloaded %s / %s' % (current_sz_mb, total_sz_mb))\n",
    "\n",
    "        archive_path = os.path.join(data_path, ARCHIVE_FILENAME)\n",
    "        urlretrieve(DOWNLOAD_URL, filename=archive_path,\n",
    "                    reporthook=progress)\n",
    "\n",
    "        print(\"untarring Reuters dataset...\")\n",
    "        tarfile.open(archive_path, 'r:gz').extractall(data_path)\n",
    "        print(\"done.\")\n",
    "          \n",
    "    parser = ReutersParser()\n",
    "    doc_counter = 0\n",
    "#     raw_docs = []\n",
    "    docs = {\n",
    "        'title_index': [],\n",
    "        'title': [],\n",
    "        'body': [],\n",
    "        'topic': []\n",
    "    }\n",
    "    for filename in glob(os.path.join(data_path, \"*.sgm\")):\n",
    "        for doc in parser.parse(open(filename, 'rb')):\n",
    "            if(doc['topics']):\n",
    "                for topic in doc['topics']:\n",
    "                    docs['title_index'].append(doc_counter)\n",
    "                    docs['title'].append(doc['title'])\n",
    "                    docs['body'].append(doc['body'])\n",
    "                    docs['topic'].append(topic)\n",
    "            else:\n",
    "                docs['title_index'].append(doc_counter)\n",
    "                docs['title'].append(doc['title'])\n",
    "                docs['body'].append(doc['body'])\n",
    "                docs['topic'].append(None)\n",
    "            doc_counter += 1\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did is to modify the downloader function to return a dictionary of the form 'title','body', 'topic' which we then can feed into the Pandas dataframe module and explore it.\n",
    "Note how we arranged it by topics so it will be easier to handle for our needs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stream = stream_reuters_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 445 topics in the dataset\n",
      "and 20030 unique titles\n",
      "there are 21578 documents in the dataset\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data=data_stream)\n",
    "print(\"there are {} topics in the dataset\".format(df.topic.nunique()))\n",
    "print(\"and {} unique titles\".format(df.title.nunique()))\n",
    "print(\"there are {} documents in the dataset\".format(df.title_index.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each of the topics we can get the count, mean, max index, std, and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    title_index                                                \\\n",
      "                          count          mean          std      min       25%   \n",
      "topic                                                                           \n",
      "acq                      2448.0  11102.989788  6136.448893      4.0   5398.25   \n",
      "adb-africa                 10.0   6504.000000  4112.422428   1086.0   3513.00   \n",
      "adb-asia                   20.0   4519.450000  6073.270622    451.0   1233.75   \n",
      "afghanistan                 3.0  15466.000000  7466.900562   6844.0  13300.00   \n",
      "alfonsin                    2.0   7864.500000  4480.935672   4696.0   6280.25   \n",
      "algeria                    35.0   8099.342857  5928.470535     96.0   3479.00   \n",
      "alum                       63.0  10444.507937  6873.331391    506.0   3321.00   \n",
      "amex                       60.0  10376.416667  5634.302229   1361.0   5927.25   \n",
      "andriessen                  1.0   1365.000000          NaN   1365.0   1365.00   \n",
      "angola                      2.0  16262.500000  3618.265399  13704.0  14983.25   \n",
      "antigua                     1.0   1411.000000          NaN   1411.0   1411.00   \n",
      "aqazadeh                    6.0   7882.500000  4249.519208   2685.0   5669.75   \n",
      "aquino                      4.0   7988.000000  7678.610682    240.0   4280.25   \n",
      "argentina                  90.0  11052.877778  6192.417010    161.0   5331.25   \n",
      "aruba                       1.0  21469.000000          NaN  21469.0  21469.00   \n",
      "ase                        11.0  11773.181818  7401.940648     11.0   5584.50   \n",
      "asean                       2.0   5910.000000  2983.990617   3800.0   4855.00   \n",
      "asx                         5.0  10631.000000  4756.055982   3796.0   8757.00   \n",
      "atpc                        3.0  16182.333333  3286.203635  12394.0  15141.00   \n",
      "austdlr                     4.0   9549.500000  6942.671220   4778.0   4802.00   \n",
      "australia                 270.0  12002.051852  6390.875236    261.0   7553.50   \n",
      "austria                    27.0  11601.407407  5701.876569   2737.0   6694.50   \n",
      "babangida                   2.0   6297.000000  2347.594514   4637.0   5467.00   \n",
      "bahamas                     1.0  11434.000000          NaN  11434.0  11434.00   \n",
      "bahrain                    46.0  11460.217391  6067.117287    450.0   5673.25   \n",
      "balladur                   35.0   9895.257143  6580.533727    251.0   5129.00   \n",
      "bangemann                   6.0   9322.833333  6028.959825   1118.0   4832.25   \n",
      "bangladesh                 34.0   9537.647059  6526.434559    276.0   3948.25   \n",
      "barbados                    3.0   5243.000000  6904.887906   1226.0   1256.50   \n",
      "barley                     54.0   9113.814815  6328.874880     98.0   3234.00   \n",
      "...                         ...           ...          ...      ...       ...   \n",
      "uruguay                     5.0  10429.800000  6073.055343   1540.0   8004.00   \n",
      "us-virgin-islands           2.0  15121.000000  4634.377844  11844.0  13482.50   \n",
      "usa                     12542.0  11019.017780  6083.025813      0.0   5845.25   \n",
      "ussr                      216.0  10720.583333  6220.333254    278.0   5367.75   \n",
      "vanuatu                     1.0   3885.000000          NaN   3885.0   3885.00   \n",
      "veg-oil                   137.0  11256.635036  6106.372857     79.0   5809.00   \n",
      "venezuela                  75.0  10158.920000  7024.072688    176.0   3546.00   \n",
      "verity                      3.0   5480.000000   714.615981   5030.0   5068.00   \n",
      "vietnam                     8.0  10875.500000  8778.538294   1206.0   1343.00   \n",
      "volcker                    79.0  10997.493671  6466.407360    830.0   7646.00   \n",
      "von-weizsaecker             7.0   9813.285714  4766.158261   4696.0   4816.50   \n",
      "wang-bingqian               2.0  20288.500000    28.991378  20268.0  20278.25   \n",
      "west-germany              567.0  10380.947090  6425.665568    237.0   4432.50   \n",
      "wheat                     306.0  10309.568627  6102.795830     93.0   4482.00   \n",
      "wilson                     16.0  10692.000000  3333.444885   5859.0   9687.50   \n",
      "wool                        2.0  13540.500000  6309.513809   9079.0  11309.75   \n",
      "worldbank                  87.0  11283.206897  6511.669425     45.0   5725.50   \n",
      "wpi                        32.0  13049.750000  6957.751444    231.0   7166.75   \n",
      "yemen-arab-republic         8.0  16694.125000  3286.541971  10463.0  15625.00   \n",
      "yemen-demo-republic         3.0  14668.666667  8034.528009   5501.0  11760.50   \n",
      "yen                        69.0  11865.318841  7190.323329    543.0   5541.00   \n",
      "yeutter                    63.0   9469.555556  5247.906802   1201.0   4839.50   \n",
      "young                       1.0   3811.000000          NaN   3811.0   3811.00   \n",
      "yugoslavia                 47.0   9522.574468  6044.465919    212.0   4148.00   \n",
      "zaire                       9.0  13805.000000  6428.403884   1397.0  11422.00   \n",
      "zambia                     23.0  10469.391304  5835.315530    244.0   5709.00   \n",
      "zhao-ziyang                 8.0  19036.750000  1400.069871  16708.0  19025.50   \n",
      "zimbabwe                   16.0  10644.687500  6113.248277   1105.0   4847.50   \n",
      "zinc                       44.0   9203.727273  5178.798987    876.0   6110.25   \n",
      "zse                         4.0   9008.500000  8673.583823   1055.0   4490.75   \n",
      "\n",
      "                                                 \n",
      "                         50%       75%      max  \n",
      "topic                                            \n",
      "acq                  11445.0  16271.25  21558.0  \n",
      "adb-africa            6746.5   7840.75  15143.0  \n",
      "adb-asia              1332.5   3280.50  17542.0  \n",
      "afghanistan          19756.0  19777.00  19798.0  \n",
      "alfonsin              7864.5   9448.75  11033.0  \n",
      "algeria               6749.0  12484.50  19335.0  \n",
      "alum                 10618.0  17112.00  21274.0  \n",
      "amex                  9411.5  15463.00  21465.0  \n",
      "andriessen            1365.0   1365.00   1365.0  \n",
      "angola               16262.5  17541.75  18821.0  \n",
      "antigua               1411.0   1411.00   1411.0  \n",
      "aqazadeh              6546.5  10719.50  14029.0  \n",
      "aquino                6592.0  10299.75  18528.0  \n",
      "argentina            11970.5  16606.50  21565.0  \n",
      "aruba                21469.0  21469.00  21469.0  \n",
      "ase                  12258.0  19039.00  21496.0  \n",
      "asean                 5910.0   6965.00   8020.0  \n",
      "asx                  11951.0  11958.00  16693.0  \n",
      "atpc                 17888.0  18076.50  18265.0  \n",
      "austdlr               6956.0  11703.50  19508.0  \n",
      "australia            12688.0  17912.75  21224.0  \n",
      "austria              11136.0  15895.50  20379.0  \n",
      "babangida             6297.0   7127.00   7957.0  \n",
      "bahamas              11434.0  11434.00  11434.0  \n",
      "bahrain              11966.0  17028.50  20678.0  \n",
      "balladur              6586.0  15693.00  20487.0  \n",
      "bangemann            10840.5  12861.75  16773.0  \n",
      "bangladesh           10255.0  13949.25  21219.0  \n",
      "barbados              1287.0   7251.50  13216.0  \n",
      "barley                8162.5  13210.50  20873.0  \n",
      "...                      ...       ...      ...  \n",
      "uruguay              11323.0  13704.00  17578.0  \n",
      "us-virgin-islands    15121.0  16759.50  18398.0  \n",
      "usa                  11332.0  16258.75  21576.0  \n",
      "ussr                 10855.5  15654.50  21565.0  \n",
      "vanuatu               3885.0   3885.00   3885.0  \n",
      "veg-oil              11444.0  16657.00  21258.0  \n",
      "venezuela            10266.0  16487.00  21186.0  \n",
      "verity                5106.0   5705.00   6304.0  \n",
      "vietnam              12808.0  18089.00  20282.0  \n",
      "volcker               7936.0  17669.50  21174.0  \n",
      "von-weizsaecker      13004.0  13125.50  15109.0  \n",
      "wang-bingqian        20288.5  20298.75  20309.0  \n",
      "west-germany          9486.0  15962.50  21450.0  \n",
      "wheat                10768.0  15528.25  21273.0  \n",
      "wilson               10593.5  11884.25  17640.0  \n",
      "wool                 13540.5  15771.25  18002.0  \n",
      "worldbank            12632.0  16634.00  21219.0  \n",
      "wpi                  16250.0  17452.50  21521.0  \n",
      "yemen-arab-republic  17179.0  18546.75  20485.0  \n",
      "yemen-demo-republic  18020.0  19252.50  20485.0  \n",
      "yen                  12032.0  19273.00  21286.0  \n",
      "yeutter               9181.0  13172.50  21459.0  \n",
      "young                 3811.0   3811.00   3811.0  \n",
      "yugoslavia            8657.0  13847.50  21516.0  \n",
      "zaire                15972.0  18485.00  19595.0  \n",
      "zambia               10850.0  15631.00  19180.0  \n",
      "zhao-ziyang          19765.5  19810.00  19833.0  \n",
      "zimbabwe             10978.5  15380.50  19193.0  \n",
      "zinc                  8791.5  13695.75  19062.0  \n",
      "zse                   6857.5  11375.25  21264.0  \n",
      "\n",
      "[445 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby('topic').describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we'll be looking into the task of classifying emails and SMS messages as 'spam' and 'ham' ('ham' being the opposite of spam - good, meaningful messages). We'll be looking and answering questions about pre-written code. The first part of the code loads the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "def progress(i, end_val, bar_length=50):\n",
    "    '''\n",
    "    Print a progress bar of the form: Percent: [#####      ]\n",
    "    i is the current progress value expected in a range [0..end_val]\n",
    "    bar_length is the width of the progress bar on the screen.\n",
    "    '''\n",
    "    percent = float(i) / end_val\n",
    "    hashes = '#' * int(round(percent * bar_length))\n",
    "    spaces = ' ' * (bar_length - len(hashes))\n",
    "    sys.stdout.write(\"\\rPercent: [{0}] {1}%\".format(hashes + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "NEWLINE = '\\n'\n",
    "HAM = 'ham'\n",
    "SPAM = 'spam'\n",
    "\n",
    "SOURCES = [\n",
    "    ('data/spam',        SPAM),\n",
    "    ('data/easy_ham',    HAM),\n",
    "    ('data/hard_ham',    HAM),\n",
    "    ('data/beck-s',      HAM),\n",
    "    ('data/farmer-d',    HAM),\n",
    "    ('data/kaminski-v',  HAM),\n",
    "    ('data/kitchen-l',   HAM),\n",
    "    ('data/lokay-m',     HAM),\n",
    "    ('data/williams-w3', HAM),\n",
    "    ('data/BG',          SPAM),\n",
    "    ('data/GP',          SPAM),\n",
    "    ('data/SH',          SPAM)\n",
    "]\n",
    "\n",
    "SKIP_FILES = {'cmds'}\n",
    "\n",
    "\n",
    "def read_files(path):\n",
    "    '''\n",
    "    Generator of pairs (filename, filecontent)\n",
    "    for all files below path whose name is not in SKIP_FILES.\n",
    "    The content of the file is of the form:\n",
    "        header....\n",
    "        <emptyline>\n",
    "        body...\n",
    "    This skips the headers and returns body only.\n",
    "    '''\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = open(file_path, encoding=\"latin-1\")\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield file_path, content\n",
    "\n",
    "\n",
    "def build_data_frame(l, path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for i, (file_name, text) in enumerate(read_files(path)):\n",
    "        if ((i+l) % 100 == 0):\n",
    "            progress(i+l, 58910, 50)\n",
    "        rows.append({'text': text, 'class': classification})\n",
    "        index.append(file_name)\n",
    "   \n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame, len(rows)\n",
    "\n",
    "def load_data():\n",
    "    data = DataFrame({'text': [], 'class': []})\n",
    "    l = 0\n",
    "    for path, classification in SOURCES:\n",
    "        data_frame, nrows = build_data_frame(l, path, classification)\n",
    "        data = data.append(data_frame)\n",
    "        l += nrows\n",
    "    data = data.reindex(np.random.permutation(data.index))\n",
    "    return data\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a fancy code, but the imporant thing about it is the data structure: a pandas dataframe in size (D,2) - D being the number of **documents** in the collection. For each document, there are two columns: text (the actual text of the document) and class (classification as ham/spam).\n",
    "\n",
    "This is the training code (no need to run it yet, just understand how it works):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "        ('classifier',         MultinomialNB())\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def train(data = None, n_folds = 6):\n",
    "    if data is None:\n",
    "        print(\"Loading data...\")\n",
    "        data = load_data()\n",
    "        print(\"Data loaded\")\n",
    "    k_fold = KFold(n_splits = n_folds)\n",
    "    pipeline = build_pipeline()\n",
    "    scores = []\n",
    "    confusion = numpy.array([[0, 0], [0, 0]])\n",
    "    print(\"Training with %d folds\" % n_folds)\n",
    "    for i, (train_indices, test_indices) in enumerate(k_fold.split(data)):\n",
    "        train_text = data.iloc[train_indices]['text'].values\n",
    "        train_y = data.iloc[train_indices]['class'].values.astype(str)\n",
    "\n",
    "        test_text = data.iloc[test_indices]['text'].values\n",
    "        test_y = data.iloc[test_indices]['class'].values.astype(str)\n",
    "\n",
    "        print(\"Training for fold %d\" % i)\n",
    "        pipeline.fit(train_text, train_y)\n",
    "        print(\"Testing for fold %d\" % i)\n",
    "        predictions = pipeline.predict(test_text)\n",
    "\n",
    "        confusion += confusion_matrix(test_y, predictions)\n",
    "        score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "        scores.append(score)\n",
    "        print(\"Score for %d: %2.2f\" % (i, score))\n",
    "        print(\"Confusion matrix for %d: \" % i)\n",
    "        print(confusion)\n",
    "\n",
    "    print('Total emails classified:', len(data))\n",
    "    print('Score:', sum(scores)/len(scores))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick overview of how it works:\n",
    "The code uses a technique called k-fold (cross validation). Instead of simply splitting the corpus to a training and test set, it splits the dataset into k equal parts. The code than runs k times, each time selecting a different part of the corpus as the test set (and the rest k-1 parts as training). So this way, we can use the entire corpus as both training and test set. The disadvantage to this is, of course, performance (the whole learning/predicting process runs k times). \n",
    "In this case, we can assume why this technique was used: our corpus is made of several different sources, so simply splitting the corpus would have likely resulted in a test set that isn't similar to the training set.\n",
    "\n",
    "Another new technique introduced is the pipeline: it is used to create a sequence of several transformers (and a final estimator), and package them all as one object. It does not provide new functionality on it's own, but rather serves as a convinient method to pack a workflow of several transformers.\n",
    "\n",
    "The first element in the pipeline is the CountVectorizer. It's basic function is simple - it recieves a list of documents, and creates a matrix of documents X token counts. So if a token j appears 4 times in a document i, the matrix value in cell (i,j) will be 4. A token can be an n-gram as well, and we're able to choose an n-gram range, so the Vectorizer is able to mix tokens of different sizes in one matrix. In this example, our n-range is 1-2, meaning we check unigrams and bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create the CountVectorizer, and use it to transform our data into a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 2))\n",
    "data_x, data_y = data['text'], data['class']\n",
    "X = cv.fit_transform(data_x, data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're using the entire corpus, with no training/test split, because the k-fold technique ensures that the entire corpus will eventually be used as both training and test data.\n",
    "\n",
    "First we'll calculate the number of unigrams and bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bigrams': 3318380, 'unigrams': 697570}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_uni_bi (fnames):\n",
    "    cnt = Counter([len(feature.split()) for feature in fnames])\n",
    "    return {\"unigrams\": cnt.get(1), \"bigrams\": cnt.get(2)}\n",
    "\n",
    "fnames = cv.get_feature_names()\n",
    "count_uni_bi (fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both unigrams and bigrams are used as features (and they're the only features), we could use *get_feature_names* to get the list of all unigrams and bigrams. From here it's just a matter of separating them and counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to get the most frequent unigrams and bigrams. We could work directly with our matrix, summing every column and printing the top 50 biggest numbers of each n-gram. However, because of the matrix size, we found this to be problematic performance-wise. \n",
    "Instead, we would like to recreate the tokens and count ourselves. Luckily, scikit provides easy access to the very same preprocessing methods they use, so we can assure our tokenization methods are identical. We've created a function that splits the data into unigrams and bigrams, and we use the Counter \"most_common\" method on both.\n",
    "Not that the bigrams split is also done within a document (meaning the last word of document and the first word of the next document will **not** create a bigram) - this is consistent with the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_uni_bi (data, analyzer):\n",
    "    uni, bi = [], []\n",
    "    for doc in data:\n",
    "        tokens = analyzer(doc)\n",
    "        for t in tokens:\n",
    "            if len(t.split()) == 1:\n",
    "                uni.append(t)\n",
    "            else:\n",
    "                bi.append(t)\n",
    "    return [uni,bi]\n",
    "\n",
    "analyzer = cv.build_analyzer()\n",
    "uni_bi_list = split_uni_bi(data_x, analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('font', 588020),\n",
       " ('3d', 573645),\n",
       " ('the', 523545),\n",
       " ('br', 506598),\n",
       " ('td', 412043),\n",
       " ('to', 384631),\n",
       " ('and', 304840),\n",
       " ('nbsp', 288463),\n",
       " ('of', 276364),\n",
       " ('http', 240249),\n",
       " ('20', 230871),\n",
       " ('size', 226011),\n",
       " ('tr', 225925),\n",
       " ('in', 206212),\n",
       " ('width', 197032),\n",
       " ('com', 192779),\n",
       " ('you', 159126),\n",
       " ('face', 153450),\n",
       " ('for', 152577),\n",
       " ('border', 148637),\n",
       " ('is', 147840),\n",
       " ('style', 134521),\n",
       " ('this', 127919),\n",
       " ('align', 127774),\n",
       " ('span', 125907),\n",
       " ('href', 122468),\n",
       " ('height', 122384),\n",
       " ('html', 119934),\n",
       " ('color', 119523),\n",
       " ('www', 116191),\n",
       " ('that', 103891),\n",
       " ('on', 101327),\n",
       " ('your', 96299),\n",
       " ('content', 92507),\n",
       " ('table', 87217),\n",
       " ('with', 86005),\n",
       " ('be', 84848),\n",
       " ('div', 84766),\n",
       " ('arial', 80928),\n",
       " ('it', 79414),\n",
       " ('from', 76064),\n",
       " ('we', 74683),\n",
       " ('or', 74405),\n",
       " ('center', 73872),\n",
       " ('img', 72069),\n",
       " ('as', 71809),\n",
       " ('enron', 70007),\n",
       " ('src', 69740),\n",
       " ('are', 69688),\n",
       " ('text', 67392)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"unigrams:\")\n",
    "Counter(uni_bi_list[0]).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('nbsp nbsp', 163936),\n",
       " ('br br', 156754),\n",
       " ('font size', 111788),\n",
       " ('td tr', 108261),\n",
       " ('http www', 105357),\n",
       " ('font face', 104446),\n",
       " ('td td', 98916),\n",
       " ('tr td', 90633),\n",
       " ('3d http', 83221),\n",
       " ('style 3d', 77299),\n",
       " ('font td', 68641),\n",
       " ('tr tr', 66263),\n",
       " ('href 3d', 61747),\n",
       " ('font font', 57538),\n",
       " ('of the', 55143),\n",
       " ('td width', 52193),\n",
       " ('color 3d', 50305),\n",
       " ('font color', 47765),\n",
       " ('img src', 47259),\n",
       " ('href http', 47094),\n",
       " ('in the', 44708),\n",
       " ('arial helvetica', 43463),\n",
       " ('width 3d', 43361),\n",
       " ('sans serif', 38861),\n",
       " ('face 3d', 36805),\n",
       " ('size 3d2', 35974),\n",
       " ('align center', 35433),\n",
       " ('content type', 35306),\n",
       " ('helvetica sans', 35306),\n",
       " ('bgcolor 3d', 35249),\n",
       " ('src http', 33977),\n",
       " ('1px solid', 33420),\n",
       " ('gif width', 33254),\n",
       " ('font family', 33168),\n",
       " ('tr table', 31674),\n",
       " ('size 3d', 31530),\n",
       " ('face 3darial', 31036),\n",
       " ('br font', 30866),\n",
       " ('face 3dverdana', 30531),\n",
       " ('3d font', 30291),\n",
       " ('src 3d', 28006),\n",
       " ('span style', 27985),\n",
       " ('text html', 26424),\n",
       " ('face arial', 26149),\n",
       " ('align 3d', 25481),\n",
       " ('td align', 25429),\n",
       " ('size 3d1', 25116),\n",
       " ('if you', 24335),\n",
       " ('body html', 24299),\n",
       " ('html charset', 23600)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"bigrams:\")\n",
    "Counter(uni_bi_list[1]).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these words look unlikely (they mostly look like HTML documents), but we've verified with the original matrix too. This is a spam dataset, after all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next task, we'll need the same split but for each class. So we'll filter the dataset per class and use the same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham = data.loc[data['class'] == 'ham']\n",
    "spam = data.loc[data['class'] == 'spam']\n",
    "ham_x, spam_x = ham['text'], spam['text']\n",
    "ham_uni_bi_list = split_uni_bi(ham_x, analyzer)\n",
    "spam_uni_bi_list = split_uni_bi(spam_x, analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham unigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 271739),\n",
       " ('to', 185712),\n",
       " ('and', 123494),\n",
       " ('of', 116399),\n",
       " ('in', 90256),\n",
       " ('com', 71121),\n",
       " ('for', 70561),\n",
       " ('enron', 70004),\n",
       " ('is', 60087),\n",
       " ('on', 59190),\n",
       " ('http', 58170),\n",
       " ('that', 55662),\n",
       " ('you', 50956),\n",
       " ('td', 46347),\n",
       " ('this', 43429),\n",
       " ('it', 41292),\n",
       " ('with', 39740),\n",
       " ('be', 38868),\n",
       " ('ect', 38444),\n",
       " ('20', 37356),\n",
       " ('width', 36024),\n",
       " ('have', 33884),\n",
       " ('from', 33265),\n",
       " ('will', 33164),\n",
       " ('www', 32780),\n",
       " ('as', 32712),\n",
       " ('we', 32597),\n",
       " ('at', 31830),\n",
       " ('3d', 29716),\n",
       " ('are', 29348),\n",
       " ('by', 29116),\n",
       " ('font', 27064),\n",
       " ('or', 25012),\n",
       " ('tr', 23382),\n",
       " ('if', 22790),\n",
       " ('not', 22224),\n",
       " ('your', 21613),\n",
       " ('br', 21065),\n",
       " ('height', 20361),\n",
       " ('09', 19971),\n",
       " ('src', 19681),\n",
       " ('gif', 19436),\n",
       " ('img', 19420),\n",
       " ('an', 18535),\n",
       " ('10', 18144),\n",
       " ('hou', 18101),\n",
       " ('has', 17889),\n",
       " ('was', 17650),\n",
       " ('2001', 17145),\n",
       " ('href', 16761)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"ham unigrams:\")\n",
    "Counter(ham_uni_bi_list[0]).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham bigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('http www', 30946),\n",
       " ('of the', 26553),\n",
       " ('in the', 21549),\n",
       " ('img src', 17425),\n",
       " ('hou ect', 17313),\n",
       " ('ect ect', 17154),\n",
       " ('src http', 15448),\n",
       " ('gif width', 14323),\n",
       " ('href http', 13703),\n",
       " ('to the', 12840),\n",
       " ('on the', 12145),\n",
       " ('for the', 11875),\n",
       " ('09 09', 11240),\n",
       " ('td width', 11184),\n",
       " ('td tr', 10618),\n",
       " ('td td', 10294),\n",
       " ('will be', 10226),\n",
       " ('cnet com', 9924),\n",
       " ('com gif', 9873),\n",
       " ('if you', 9428),\n",
       " ('to be', 8192),\n",
       " ('tr td', 7881),\n",
       " ('com click', 7810),\n",
       " ('width height', 7550),\n",
       " ('enron enron', 7479),\n",
       " ('online com', 7399),\n",
       " ('http clickthru', 7398),\n",
       " ('clickthru online', 7354),\n",
       " ('font face', 7349),\n",
       " ('enron com', 6858),\n",
       " ('with the', 6778),\n",
       " ('the company', 6738),\n",
       " ('and the', 6524),\n",
       " ('pm to', 6131),\n",
       " ('at the', 5962),\n",
       " ('3d http', 5788),\n",
       " ('zdnet com', 5605),\n",
       " ('www cnet', 5539),\n",
       " ('you have', 5478),\n",
       " ('tr table', 5447),\n",
       " ('that the', 5418),\n",
       " ('www zdnet', 5253),\n",
       " ('arial helvetica', 5069),\n",
       " ('width 3d', 4999),\n",
       " ('height td', 4917),\n",
       " ('am to', 4798),\n",
       " ('message from', 4758),\n",
       " ('original message', 4748),\n",
       " ('from the', 4720),\n",
       " ('font td', 4636)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"ham bigrams:\")\n",
    "Counter(ham_uni_bi_list[1]).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam unigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('font', 560956),\n",
       " ('3d', 543929),\n",
       " ('br', 485533),\n",
       " ('td', 365696),\n",
       " ('nbsp', 278903),\n",
       " ('the', 251806),\n",
       " ('size', 213214),\n",
       " ('tr', 202543),\n",
       " ('to', 198919),\n",
       " ('20', 193515),\n",
       " ('http', 182079),\n",
       " ('and', 181346),\n",
       " ('width', 161008),\n",
       " ('of', 159965),\n",
       " ('face', 144363),\n",
       " ('border', 133328),\n",
       " ('style', 131069),\n",
       " ('span', 124769),\n",
       " ('align', 123644),\n",
       " ('com', 121658),\n",
       " ('in', 115956),\n",
       " ('color', 113473),\n",
       " ('html', 112620),\n",
       " ('you', 108170),\n",
       " ('href', 105707),\n",
       " ('height', 102023),\n",
       " ('content', 89698),\n",
       " ('is', 87753),\n",
       " ('this', 84490),\n",
       " ('www', 83411),\n",
       " ('div', 82954),\n",
       " ('for', 82016),\n",
       " ('your', 74686),\n",
       " ('arial', 73977),\n",
       " ('table', 73719),\n",
       " ('center', 70018),\n",
       " ('text', 63758),\n",
       " ('img', 52649),\n",
       " ('class', 51690),\n",
       " ('body', 51559),\n",
       " ('src', 50059),\n",
       " ('strong', 50059),\n",
       " ('or', 49393),\n",
       " ('3d2', 49029),\n",
       " ('3d0', 48932),\n",
       " ('that', 48229),\n",
       " ('type', 47829),\n",
       " ('with', 46265),\n",
       " ('be', 45980),\n",
       " ('bgcolor', 45055)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"spam unigrams:\")\n",
    "Counter(spam_uni_bi_list[0]).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam bigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('nbsp nbsp', 161630),\n",
       " ('br br', 155432),\n",
       " ('font size', 107762),\n",
       " ('td tr', 97643),\n",
       " ('font face', 97097),\n",
       " ('td td', 88622),\n",
       " ('tr td', 82752),\n",
       " ('3d http', 77433),\n",
       " ('style 3d', 76837),\n",
       " ('http www', 74411),\n",
       " ('font td', 64005),\n",
       " ('tr tr', 61843),\n",
       " ('href 3d', 59828),\n",
       " ('font font', 55922),\n",
       " ('color 3d', 49814),\n",
       " ('font color', 46183),\n",
       " ('td width', 41009),\n",
       " ('arial helvetica', 38394),\n",
       " ('width 3d', 38362),\n",
       " ('face 3d', 35937),\n",
       " ('size 3d2', 35712),\n",
       " ('sans serif', 35613),\n",
       " ('content type', 34595),\n",
       " ('align center', 34124),\n",
       " ('helvetica sans', 33665),\n",
       " ('1px solid', 33420),\n",
       " ('href http', 33391),\n",
       " ('bgcolor 3d', 33067),\n",
       " ('font family', 32091),\n",
       " ('face 3darial', 30773),\n",
       " ('size 3d', 30654),\n",
       " ('face 3dverdana', 30419),\n",
       " ('3d font', 30038),\n",
       " ('img src', 29834),\n",
       " ('of the', 28590),\n",
       " ('br font', 27848),\n",
       " ('span style', 27816),\n",
       " ('tr table', 26227),\n",
       " ('text html', 26213),\n",
       " ('align 3d', 25160),\n",
       " ('size 3d1', 24888),\n",
       " ('td align', 24565),\n",
       " ('src 3d', 24095),\n",
       " ('body html', 23957),\n",
       " ('html charset', 23429),\n",
       " ('in the', 23159),\n",
       " ('border 3d0', 22882),\n",
       " ('3d border', 22503),\n",
       " ('content 3d', 22260),\n",
       " ('face arial', 22173)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"spam bigrams:\")\n",
    "Counter(spam_uni_bi_list[1]).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to find the top 20 features for each class. Scikit offers a solution for this case - SelectFromModel. However, we ran into two problems when trying to use it:\n",
    "-SelectFromModel does not support sparse matrices (and our matrix is way too big to not be sparse)\n",
    "-More importantly - for binary classification tasks, SelectFromModel can only returns one list of top features (and not a feature for each class).\n",
    "\n",
    "We've had to dig deeper, and when we checked the source code, we discovered that SelectFromModel basically just returns the features with the highest coefficient (more specifically, highest absolute value of coefficient). So we've decided to work with the coefficient ourselves. So first, let's get our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = data_x[:49092]\n",
    "train_y = data_y[:49092]\n",
    "X = cv.fit_transform(train_text)\n",
    "mnb = MultinomialNB().fit (X, train_y)\n",
    "coefs = mnb.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this time we do split to train and testing data, but we only split once in the 'normal' method, without using cross-validation.\n",
    "\n",
    "We have our coefficient list, but we still run into the same problem - we only have a general list for the classifier, and not for each category. At first we thought that each side end of the list represents a different class (so the top 20 features will be class A, and the bottom 20 class B). But upon further inspection, this turned out to be wrong. We looked at the \"feature count\" attribute of each feature - which represents the number of samples encountered for each (class, feature) during fitting.\n",
    "The bottom 20 looked pretty good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 20:\n",
      "span 892.0 103857.0\n",
      "style 2841.0 109185.0\n",
      "border 12274.0 112371.0\n",
      "face 7149.0 120328.0\n",
      "nbsp nbsp 1841.0 126069.0\n",
      "br br 981.0 130330.0\n",
      "of 95416.0 134255.0\n",
      "width 28520.0 134460.0\n",
      "http 48367.0 151916.0\n",
      "and 101123.0 151991.0\n",
      "20 30429.0 160500.0\n",
      "to 152782.0 166070.0\n",
      "tr 18935.0 167658.0\n",
      "size 10019.0 177519.0\n",
      "the 223453.0 210599.0\n",
      "nbsp 7915.0 223793.0\n",
      "td 37049.0 304758.0\n",
      "br 16502.0 404666.0\n",
      "3d 25924.0 453196.0\n",
      "font 21119.0 467026.0\n"
     ]
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "args = np.argsort(coefs)\n",
    "top_20, bottom_20 = args[:20], args[-20:]\n",
    "bottom = [feature_names[i] for i in bottom_20]\n",
    "print (\"bottom 20:\")\n",
    "for i in range(20):\n",
    "    print (bottom[i], mnb.feature_count_[0][bottom_20[i]], mnb.feature_count_[1][bottom_20[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the top 20, on the other hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 20:\n",
      "further improvments 1.0 0.0\n",
      "grew 151 1.0 0.0\n",
      "grew 150 1.0 0.0\n",
      "grew 12 1.0 0.0\n",
      "gretna va 3.0 0.0\n",
      "gretna 3.0 0.0\n",
      "gretchen sprocket 1.0 0.0\n",
      "gretchen oddly 2.0 0.0\n",
      "gretchen morgenson 1.0 0.0\n",
      "gretchen matt 2.0 0.0\n",
      "gretchen marshall 1.0 0.0\n",
      "gretchen mahajan 1.0 0.0\n",
      "gretchen lotz 1.0 0.0\n",
      "gretchen jennings 2.0 0.0\n",
      "gretchen is 1.0 0.0\n",
      "gretchen invited 1.0 0.0\n",
      "gretchen hansen 1.0 0.0\n",
      "gretchen am 1.0 0.0\n",
      "greta tom 2.0 0.0\n",
      "greta toc 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "top = [feature_names[i] for i in top_20]\n",
    "print (\"top 20:\")\n",
    "for i in range(20):\n",
    "    print (top[i], mnb.feature_count_[0][top_20[i]], mnb.feature_count_[1][top_20[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all those features have extremely small feature counts - which means they are extremely rare, and not valuable features. \n",
    "This explains why Scikit used the top **absolute values** of coefficients - all our coefficients are negative numbers, and here we see that the bottom negative numbers are the valueable ones - hence, the one who have the highest absolute value.\n",
    "Also, inspecting Scikit's code and documentation, nowhere does it implies there's any use to the worst coefficients (for example, SelectFromModel throws them away beneath a certain threshold). \n",
    "\n",
    "So, the coefficient list on it's own doesn't solve our problem - we had to dig even deeper. We discovered a curious thing: when we checked coeff for other classification tasks, with more than one class, coeff actually returned a separate list for every class. Only in binary tasks, coeff returns only one list. So we looked into Naive Bayes code to discover what are those coefficient values, and this is what we found:\n",
    "\n",
    "def _get_coef(self):\n",
    "    return (self.feature_log_prob_[1:]\n",
    "            if len(self.classes_) == 2 else self.feature_log_prob_)\n",
    "            \n",
    "(this is a quote from Scikit's code, so we can't run it here).\n",
    "\n",
    "We disover that the coefficient is nothing more than the log probability of every feature. And also, it seems that for some reason, for binary tasks, Scikit decides specifically to only return the coefficients for the first class. But luckily, we can still access the original log probabilities, and get both lists. We just have to check which class is 0, and which is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam'], dtype='<U4')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we just have to access the bottom 20 (or top 20 absolute values) features of every list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top features for ham:\n",
      "20 30429.0\n",
      "be 31798.0\n",
      "with 32565.0\n",
      "ect 32565.0\n",
      "it 33844.0\n",
      "this 35647.0\n",
      "td 37049.0\n",
      "you 42073.0\n",
      "that 46028.0\n",
      "http 48367.0\n",
      "on 48744.0\n",
      "is 49247.0\n",
      "enron 57919.0\n",
      "for 57988.0\n",
      "com 58348.0\n",
      "in 74138.0\n",
      "of 95416.0\n",
      "and 101123.0\n",
      "to 152782.0\n",
      "the 223453.0\n",
      "\n",
      "top features for spam:\n",
      "span 103857.0\n",
      "style 109185.0\n",
      "border 112371.0\n",
      "face 120328.0\n",
      "nbsp nbsp 126069.0\n",
      "br br 130330.0\n",
      "of 134255.0\n",
      "width 134460.0\n",
      "http 151916.0\n",
      "and 151991.0\n",
      "20 160500.0\n",
      "to 166070.0\n",
      "tr 167658.0\n",
      "size 177519.0\n",
      "the 210599.0\n",
      "nbsp 223793.0\n",
      "td 304758.0\n",
      "br 404666.0\n",
      "3d 453196.0\n",
      "font 467026.0\n"
     ]
    }
   ],
   "source": [
    "ham_coeffs = mnb.feature_log_prob_[0]\n",
    "spam_coeffs = mnb.feature_log_prob_[1]\n",
    "\n",
    "def print_bottom_names (coeffs, feature_names,n, class_num):\n",
    "    args = np.argsort(coeffs)\n",
    "    bottom_co = args[-n:]\n",
    "    bottom = [feature_names[i] for i in bottom_co]\n",
    "    for i in range(n):\n",
    "        print (bottom[i], mnb.feature_count_[class_num][bottom_co[i]])\n",
    "        \n",
    "print (\"top features for ham:\")\n",
    "print_bottom_names(ham_coeffs, feature_names, 20, 0)\n",
    "\n",
    "print()\n",
    "print (\"top features for spam:\")\n",
    "print_bottom_names(spam_coeffs, feature_names, 20, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our results know makes perfect sense - we see that \"ham\" words are generally pretty normal (\"be\", \"with\" etc) while \"spam\" words are generally more nonsense. Each word also have very high feature count for its class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we want to add another feature to our model - the length of the text. But we should consider that the new feature is usually on a completely different scale to our normal features. The length of most documents is thousands of characters, While our usual features (number of appearences for a certain word/bigram in a document) is much lower. Is this a problem?\n",
    "\n",
    "Let's return to the way Naive Bayes classifier works, as explained in part 1:\n",
    "-We assume the features are independent\n",
    "-We calculate the probability of belonging to a specific class:\n",
    "<br>$P(f_1, f_2, f_3, \\dots , f_n \\vert l) = \\prod{P(f_i|l)}$\n",
    "\n",
    "As we can see, the probability for each feature is calculated separately and independently, and eventually we multiply all those probabilities. Because of that, the scale of the feature doesn't make a difference. Handling of each feature is independent, and by the time we multiply them, they're all converted to probabilities (which are of course scaled between 0-1).\n",
    "\n",
    "What about Logistic Regression? \n",
    "Well, first we have to understand how it works. Logistic Regression is similar to Linear Regression (like we implemented in homework1, question 2). The main difference is that while we now wish to use it for binary classification tasks (while linear regression predicted the y value, but not a binary value). So instead, we use the Sigmoid Function to normalize our predicted values between 0 and 1.\n",
    "\n",
    "(add sigmoid function here)\n",
    "\n",
    "Logistic regression, on it's own, does not require scaling. The reason is that after repeated iterations, the optimal coefficients for each feature are chosen, and those coefficients take into account the scale of the feature. That's not to say scaling is useless. Many optimization algorithms (such as gradient descent) converges faster on normalized values, meaning that scaling will lead to better performance. But it isn't neccesary.\n",
    "\n",
    "So when is scaling neccesary?\n",
    "If we're using a Logistic Regression model **with regularization**. Certain penalty functions are very sensitive to scaling, meaning that using the same feature but in different scale (for example, using meters or kilometers) will result in drastically different penalties. The Logistic Regression used in the model uses the solver 'lbfgs', which according to Scikit documentation, only works with the L2 penalty function:\n",
    "\n",
    "(add L2 penalty function)\n",
    "\n",
    "This function is sensitive to scaling. So in order to use this length feature in the Logistic Regression model, we'll need to scale it - but not because of the Logistic Regression itself, but because of the regularization method it uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's add the length feature to our model. We will build our own Pipeline, and we'll also use FeatureUnion. FeatureUnion concatenates lists of features from different transformers. Meaning we can \"prepeare\" several feature sets separately (running each one through different transformers), and use the FeatureUnion to combine them to a single feature list.\n",
    "\n",
    "The FeatureUnion and the Pipeline accept objects called transformers. Transformers in Scikit are objects who inherit from general class TransformerMixin, and implement two methods: fit and transform. In our case, we will build a very simple transformer that returns the length of each string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LengthTransformer(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return np.array([len(doc) for doc in X]).reshape(-1,1)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our fit method does nothing (as there's no need to fit the data in any way). Our transform method, at it's core, returns the length of every item in the training set. It also prepears the datat for the scaler - by converting it into a numpy array, and reshaping it (as the scaler does not accept 1D arrays).\n",
    "Now here's our FeatureUnion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "union = FeatureUnion([\n",
    "                (\"count\", CountVectorizer(ngram_range=(1, 2))), \n",
    "                (\"length\", Pipeline([(\"lt\", LengthTransformer()), (\"scaler\", StandardScaler(with_mean=False))]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge two feature sets - the \"count' feature set is the CountVectorizer from eariler, while the \"length\" feature set is our newly written length transformer. We normalize the length, for reasons we explained in the previous question.\n",
    "\n",
    "So here's our complete pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline2():\n",
    "    pipeline = Pipeline([\n",
    "        ('features',   FeatureUnion([\n",
    "                (\"count\", CountVectorizer(ngram_range=(1, 2))), \n",
    "                (\"length\", Pipeline([(\"lt\", LengthTransformer()), (\"scaler\", StandardScaler(with_mean=False))]))])), \n",
    "        ('classifier',         LogisticRegression(solver='lbfgs'))\n",
    "    ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's run the original code, with our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 4 folds\n",
      "Training for fold 0\n",
      "Testing for fold 0\n",
      "Score for 0: 1.00\n",
      "Confusion matrix for 0: \n",
      "[[5822   28]\n",
      " [  53 8825]]\n",
      "Training for fold 1\n",
      "Testing for fold 1\n",
      "Score for 1: 0.99\n",
      "Confusion matrix for 1: \n",
      "[[11625    65]\n",
      " [  106 17660]]\n",
      "Training for fold 2\n",
      "Testing for fold 2\n",
      "Score for 2: 0.99\n",
      "Confusion matrix for 2: \n",
      "[[17446   106]\n",
      " [  167 26464]]\n",
      "Training for fold 3\n",
      "Testing for fold 3\n",
      "Score for 3: 1.00\n",
      "Confusion matrix for 3: \n",
      "[[23397   142]\n",
      " [  208 35163]]\n",
      "Total emails classified: 58910\n",
      "Score: 0.9950496023828033\n",
      "Confusion matrix:\n",
      "[[23397   142]\n",
      " [  208 35163]]\n"
     ]
    }
   ],
   "source": [
    "def train2(data = None, n_folds = 4):\n",
    "    if data is None:\n",
    "        print(\"Loading data...\")\n",
    "        data = load_data()\n",
    "        print(\"Data loaded\")\n",
    "    k_fold = KFold(n_splits = n_folds)\n",
    "    pipeline = build_pipeline2()\n",
    "    scores = []\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    print(\"Training with %d folds\" % n_folds)\n",
    "    for i, (train_indices, test_indices) in enumerate(k_fold.split(data)):\n",
    "        train_text = data.iloc[train_indices]['text'].values\n",
    "        train_y = data.iloc[train_indices]['class'].values.astype(str)\n",
    "        test_text = data.iloc[test_indices]['text'].values\n",
    "        test_y = data.iloc[test_indices]['class'].values.astype(str)\n",
    "        \n",
    "        print(\"Training for fold %d\" % i)\n",
    "        pipeline.fit(train_text, train_y)\n",
    "        print(\"Testing for fold %d\" % i)\n",
    "        predictions = pipeline.predict(test_text)\n",
    "        \n",
    "        confusion += confusion_matrix(test_y, predictions)\n",
    "        score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "        scores.append(score)\n",
    "        \n",
    "        print(\"Score for %d: %2.2f\" % (i, score))\n",
    "        print(\"Confusion matrix for %d: \" % i)\n",
    "        print(confusion)\n",
    "\n",
    "    print('Total emails classified:', len(data))\n",
    "    print('Score:', sum(scores)/len(scores))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion)\n",
    "    return pipeline\n",
    "    confusion = confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "    print(\"Score for %d: %2.2f\" % (i, score))\n",
    "    print(\"Confusion matrix for %d: \" % i)\n",
    "    print(confusion)\n",
    "    print('Total emails classified:', len(test_text))\n",
    "    return pipeline\n",
    "\n",
    "pipeline = train2(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will create a NER (Named Entity Recognition) model that recognizes names of persons, organizations and other entities in text. We will use the CoNLL 2002 dataset. As we build the code, we will use the Spanish version of the dataset. At the end of every segment, we'll test our functions on the Dutch dataset and compare the differences. \n",
    "First, we will import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "etr = conll2002.chunked_sents('esp.train') # In Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our data, as presented in the assignment, comes in the form of **chunked_sents**. This is a tree data structure, that represent the data like this:\n",
    "\n",
    "-A word is represented as a pair (word, POS tagging).\n",
    "-Each sentence is a tree. The root of the tree is always \"S\".\n",
    "-The words of the sentence are the children of the root, depending on their NER tag:\n",
    "For NER tag O, the word itself (represented by a pair) will be a direct children of the root. \n",
    "For other tags, the children will be another subtree. His root will be the category (for example, \"LOCATION\"), and his children will be all the words within the LOCATION tag.\n",
    "\n",
    "This representation is equivalent to a list, and we can easily convert between the two. It does have one advantage over lists - the tree structure \"forces\" legal tag sequences by its nature, and it's impossible to represent an illegal sequence using a tree. Nevertheless, for our questions we will use the list form, for several reasons:\n",
    "\n",
    "-The chunked_sents represnatation doesn't directly include the word tags we need.\n",
    "-Scikit provides a much wider toolkit for lists rather than trees, including the models we were asked to use (such as DictVectorizer).\n",
    "-Working with a list is much more simpler and convinient.\n",
    "We'll refer to the question of illegal sequences later on.\n",
    "\n",
    "So here is our lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = list(conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(conll2002.iob_sents('esp.testa'))\n",
    "d_train_sents = list(conll2002.iob_sents('ned.train'))\n",
    "d_test_sents = list(conll2002.iob_sents('ned.testa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand how our dataset is represented. For example, let's look at an example of the first element (and hence, first sentence) in the first dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Melbourne', 'NP', 'B-LOC'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('Australia', 'NP', 'B-LOC'),\n",
       " (')', 'Fpt', 'O'),\n",
       " (',', 'Fc', 'O'),\n",
       " ('25', 'Z', 'O'),\n",
       " ('may', 'NC', 'O'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('EFE', 'NC', 'B-ORG'),\n",
       " (')', 'Fpt', 'O'),\n",
       " ('.', 'Fp', 'O')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see:\n",
    "\n",
    "-A word is represented by a tuple of three elements: the word itself, it's POS (Part of Speech) tagging, and it's correct NER tagging.\n",
    "-A sentence is represented by a list of words\n",
    "-Each element in the dataset is a sentence.\n",
    "\n",
    "Now that we understand our data, it's time to extract features. We'll be looking for word level features, and in this step we'll be looking at each word separately. Those are the features we've chosen to extract for every word:\n",
    "\n",
    "Form (The actual word), POS tagging, is number, does it contain a number, does it begin with a capital letter, is it all capital letters, is it a punctuation char, the first one, two and three letters of the word, the last one, two and three letters in of the word. \n",
    "Here's the code for feature extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasNumbers(str):\n",
    "    return any(c.isdigit() for c in str)\n",
    "\n",
    "def get_word_features (word):\n",
    "    w = word[0]\n",
    "    features = {\n",
    "     \"form\": w,\n",
    "     \"pos\": word[1],\n",
    "     \"is_number\": w.isdigit(),\n",
    "     \"contains_number\": hasNumbers(w),\n",
    "     \"beginCapital\": w[0].isupper(),\n",
    "     \"allCaps\": w.isupper(),\n",
    "     \"isPunc\": w in string.punctuation,\n",
    "     \"firstLetter\": w[0],\n",
    "     \"first2Letters\": w[0:2],\n",
    "     \"first3Letters\": w[0:3],\n",
    "     \"lastLetter\": w[-1],\n",
    "     \"last2Letters\": w[-2:],\n",
    "     \"last3Letters\": w[-3:]\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's an example on the word 'Melbourne':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'allCaps': False,\n",
       " 'beginCapital': True,\n",
       " 'contains_number': False,\n",
       " 'first2Letters': 'Me',\n",
       " 'first3Letters': 'Mel',\n",
       " 'firstLetter': 'M',\n",
       " 'form': 'Melbourne',\n",
       " 'isPunc': False,\n",
       " 'is_number': False,\n",
       " 'last2Letters': 'ne',\n",
       " 'last3Letters': 'rne',\n",
       " 'lastLetter': 'e',\n",
       " 'pos': 'NP'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_features(train_sents[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have our features, it's time to train our model. We will be using Scikit's DictVectorizer data structure to keep our data, and it's logistic regression implementation for the training. Those methods requires two seperate lists of identical size, where every element represents one word in the corpus. The first list (X) keeps the list of features for every word, and the second list (y) has the NER tagging of the word - which is the answer our model will be trying to guess.\n",
    "\n",
    "Here is the code that creates those lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_features (corpus):\n",
    "#gets a corpus, returns a list of features for every word\n",
    "    X=[]\n",
    "    for sent in corpus:\n",
    "        X+=[get_word_features(w) for w in sent]\n",
    "    return X\n",
    "\n",
    "def get_y (corpus):\n",
    "    y=[]\n",
    "    for sent in corpus:\n",
    "        y+=[w[2] for w in sent]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in addition to the obvious effect of getting features/NER taggings, those functions also transform our data from a list of **sentences** to a list of **words** - so our output is a big list of all the words, not divided to sentences anymore.\n",
    "\n",
    "Now we'll create our DictVectorizer and train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_sents, v, features):\n",
    "    y = get_y (train_sents)\n",
    "    X = v.fit_transform(features)\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X, y)\n",
    "    return clf\n",
    "\n",
    "v = DictVectorizer(sparse=True)\n",
    "features = get_corpus_features(train_sents)\n",
    "clf = train(train_sents, v, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **DictVecotizer** structure and it's fit_transform method, transforms the features list to a matrix. Each row in the matrix represents a word, and each column represents a feature. Each cell M(i,j) includes the corresponding numerical value for feature j in word i.\n",
    "\n",
    "But what if the features are non-numerical? Boolean data easily transforms to 1 (True) or 0 (False), but strings are a bit more complicated to encode. DictVectorizer's solution is to create a separate boolean feature for every string it encounters. For example, for the word \"Melbourne\", rather than the \"firstLetter\" feature, it will create a new boolean feature: \"firstLetter=M\", that will be True for every word that begins in M, and False otherwise. This is similar to a One-Hot Encoding vector.\n",
    "\n",
    "This is a good solution, but it means that for each possible value of a string feature (for example, each word, prefix or suffix of length 1,2,3), a specific feature will be created. In other words, our matrix is getting really big. For the first sentence alone (11 words), 75 features are created. For the entire dataset (264715 words), the matrix will be huge - in fact, too huge for the computer's memory (or at least, *my* computer's memory) to handle.\n",
    "\n",
    "That's why we use a **sparse** version of the matrix. How do this work? The crucial fact is that while we have a lot of data, mthe huge majority of it (typically above 99%) is zeros. For example, the word \"Melbourne\" will recieve 1 in the \"firstLetter=M\" feature, and 0 in every other \"firstLetter\" feature. The same applies for every single string feature (some of them, such as 3 letter combinations, have a lot of possible values) - that's a lot of zeros.\n",
    "So the sparse data structre take advantage of that fact. Rather than actually save all those zeros, it only keeps the non-zero values and their positions, and assumes zero everywhere else. \n",
    "\n",
    "So now we have our sparse matrix X, and our target values y, it's time to create our classifier (clf) using Scikit's logistic regression implementation. It's time to test this model on the our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict (clf, v, test_features):\n",
    "    X2 = v.transform(test_features)\n",
    "    return clf.predict(X2)\n",
    "\n",
    "test_features = get_corpus_features(test_sents)\n",
    "y_predict = predict (clf, v, test_features)\n",
    "y_true = get_y (test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is fairly straight forward, but it's interesting to note the *transform* method used. We create a sparse feature matrix for the test set, but it will not be in the same size as the original matrix for the train data. However, the *predict* method demands that the train and test matrixes will have the same number of features. \n",
    "The transform method transforms the test_features matrix to the same size as the train_features - by adding empty columns for features encountered in the training data, but not the test data. However, this also means there's some information loss - as features that were encountered in the test data but not the train data are \"silently ignored\".\n",
    "\n",
    "So now we have our predictions, it's time to evaluate how well we did. We can do this by several different methods. The easiest of which will be to calculate the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9295391417720084"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy (x,y):\n",
    "    correct = sum([1 if x[i]==y[i] else 0 for i in range(len(x))])\n",
    "    return correct / len(x)\n",
    "\n",
    "accuracy(y_predict, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So almost 93%, not too bad but we will try to do better. We also have more tools to analyze our errors. For a start, we can simply print all the errors and manually look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct=B-LOC    guess=B-MISC   word=K-2                           \n",
      "correct=B-LOC    guess=B-MISC   word=K-2                           \n",
      "correct=B-LOC    guess=B-MISC   word=K-2                           \n",
      "correct=B-LOC    guess=B-MISC   word=K-2                           \n",
      "correct=B-LOC    guess=B-MISC   word=MADRID                        \n",
      "correct=B-LOC    guess=B-MISC   word=Regin                        \n",
      "correct=B-LOC    guess=B-MISC   word=SEVILLA                       \n",
      "correct=B-LOC    guess=B-MISC   word=SEVILLA                       \n",
      "correct=B-LOC    guess=B-MISC   word=Tbet                         \n",
      "correct=B-LOC    guess=B-MISC   word=Yacuiba-Ro                   \n",
      "correct=B-LOC    guess=B-ORG    word=ACEUCHAL                      \n",
      "correct=B-LOC    guess=B-ORG    word=Aceuchal                      \n",
      "correct=B-LOC    guess=B-ORG    word=Auditorio                     \n",
      "correct=B-LOC    guess=B-ORG    word=Austria                       \n",
      "correct=B-LOC    guess=B-ORG    word=Autoridad                     \n",
      "correct=B-LOC    guess=B-ORG    word=Autoridad                     \n",
      "correct=B-LOC    guess=B-ORG    word=BILBAO                        \n",
      "correct=B-LOC    guess=B-ORG    word=Bolivia-Brasil                \n",
      "correct=B-LOC    guess=B-ORG    word=CHILE                         \n",
      "correct=B-LOC    guess=B-ORG    word=Canad                        \n"
     ]
    }
   ],
   "source": [
    "def get_errors (x,y,test_sents):\n",
    "    features = get_corpus_features(test_sents)\n",
    "    errors=[]\n",
    "    for i in range(len(x)):\n",
    "        if x[i]!=y[i]:\n",
    "            errors.append((y[i], x[i], features[i].get(\"form\")))\n",
    "    return sorted(errors)\n",
    "        \n",
    "errors = get_errors (y_predict,y_true,test_sents)\n",
    "for i in range(20):\n",
    "    print('correct=%-8s guess=%-8s word=%-30s' % (errors[i][0],errors[i][1],errors[i][2]))\n",
    "    \n",
    "#we only print the first 20 errors as examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the confusion matrix:\n",
    "(We'll also print the NER tags in order, because the Scikit confusion matrix does not include labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-PER', 'B-ORG', 'I-LOC', 'B-PER', 'B-MISC', 'B-LOC', 'I-MISC', 'O', 'I-ORG'}\n",
      "[[  779    10    77    30     3     5    30    15    35]\n",
      " [   17   214    42     8     0    32    26    14    92]\n",
      " [  186    45  1201    67     1    12    52    53    83]\n",
      " [  107     5    49   739    20    11    25   233    33]\n",
      " [   24     5     9    30   131     5    37    18    78]\n",
      " [   51    33    34    21    12   126    55    29   293]\n",
      " [  159    53   121    41    18    68   382    78   446]\n",
      " [   83     5    36   142    12    12    25   477    67]\n",
      " [    7    35    72    17     1    35    35     9 45145]]\n"
     ]
    }
   ],
   "source": [
    "print (set(y_true))\n",
    "print (metrics.confusion_matrix(y_true, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit also offers an handy tool called a classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.55      0.79      0.65       984\n",
      "     B-MISC       0.53      0.48      0.50       445\n",
      "      B-ORG       0.73      0.71      0.72      1700\n",
      "      B-PER       0.67      0.60      0.64      1222\n",
      "      I-LOC       0.66      0.39      0.49       337\n",
      "     I-MISC       0.41      0.19      0.26       654\n",
      "      I-ORG       0.57      0.28      0.38      1366\n",
      "      I-PER       0.52      0.56      0.53       859\n",
      "          O       0.98      1.00      0.99     45356\n",
      "\n",
      "avg / total       0.92      0.93      0.92     52923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we are doing a very good job with recognizing O's, but we struggle with other tags. In general, B tags seem to have better score than I tags, while MISC category seems to be the most difficult to classify. It also means that our 93% accuracy score is misleading - the score is relatively high because we are good in recognizing O's (which is the majority of the dataset), but when it comes to the classification of other tags, our accuracy is much lower.\n",
    "\n",
    "Let's check our model on the Dutch dataset as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9508053174834824\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.65      0.60      0.62       479\n",
      "     B-MISC       0.74      0.60      0.66       748\n",
      "      B-ORG       0.72      0.45      0.55       686\n",
      "      B-PER       0.55      0.70      0.62       703\n",
      "      I-LOC       0.37      0.23      0.29        64\n",
      "     I-MISC       0.42      0.23      0.30       215\n",
      "      I-ORG       0.69      0.37      0.48       396\n",
      "      I-PER       0.38      0.45      0.41       423\n",
      "          O       0.98      1.00      0.99     33973\n",
      "\n",
      "avg / total       0.95      0.95      0.95     37687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_features = get_corpus_features(d_train_sents)\n",
    "d_clf = train(d_train_sents, v, d_features)\n",
    "d_test_features = get_corpus_features(d_test_sents)\n",
    "d_y_predict = predict (d_clf, v, d_test_features)\n",
    "d_y_true = get_y (d_test_sents)\n",
    "print (\"accuracy:\", accuracy (d_y_predict,d_y_true))\n",
    "print (metrics.classification_report(d_y_true, d_y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems our Dutch model is doing slightly better than the Spanish one, but it's still prone to similar mistakes - once again we recognize very high success rates among O's (which are the big majority of the dataset), I tags are more difficult to predict than B tags, and \"Misc\" category is still problematic. \n",
    "One noticeable difference is that the Dutch model seems to struggle with Location names (0.29 fscore on I-LOC - even lower than I-MISC), which the Spanish model did better on. A possible explanation is insufficient training data - the Dutch dataset only had 64 I-LOC tags, which is much lower than other tags. \n",
    "The support column refers to the test dataset and not the training dataset, but a check on the training dataset confirms that I-LOC tags are indeed relatively rare.\n",
    "\n",
    "So now, we will try to imrove our overall model by using more than just word-specific features - we'll be looking at features of the previous and following word. Here's our updated *get_word_features2* method, which now receives three arguments (the previous and next word, in addition to the current one) and extracts all features for all 3 words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_features2 (word, prev, next):\n",
    "#also includes information about next and previous word\n",
    "    w = word[0]\n",
    "    p = prev[0]\n",
    "    n = next[0]\n",
    "    features = {\n",
    "     \"form\": w,\n",
    "     \"pos\": word[1],\n",
    "     \"is_number\": w.isdigit(),\n",
    "     \"contains_number\": hasNumbers(w),\n",
    "     \"beginCapital\": w[0].isupper(),\n",
    "     \"allCaps\": w.isupper(),\n",
    "     \"isPunc\": w in string.punctuation,\n",
    "     \"firstLetter\": w[0],\n",
    "     \"first2Letters\": w[0:2],\n",
    "     \"first3Letters\": w[0:3],\n",
    "     \"lastLetter\": w[-1],\n",
    "     \"last2Letters\": w[-2:],\n",
    "     \"last3Letters\": w[-3:],\n",
    "     \"p_form\": p,\n",
    "     \"p_pos\": prev[1],\n",
    "     \"p_is_number\": p.isdigit(),\n",
    "     \"p_contains_number\": hasNumbers(p),\n",
    "     \"p_beginCapital\": p[0].isupper(),\n",
    "     \"p_allCaps\": p.isupper(),\n",
    "     \"p_isPunc\": p in string.punctuation,\n",
    "     \"p_firstLetter\": p[0],\n",
    "     \"p_first2Letters\": p[0:2],\n",
    "     \"p_first3Letters\": p[0:3],\n",
    "     \"p_lastLetter\": p[-1],\n",
    "     \"p_last2Letters\": p[-2:],\n",
    "     \"p_last3Letters\": p[-3:],\n",
    "     \"n_form\": n,\n",
    "     \"n_pos\": next[1],\n",
    "     \"n_is_number\": n.isdigit(),\n",
    "     \"n_contains_number\": hasNumbers(n),\n",
    "     \"n_beginCapital\": n[0].isupper(),\n",
    "     \"n_allCaps\": n.isupper(),\n",
    "     \"n_ispunc\": n in string.punctuation,\n",
    "     \"n_firstLetter\": n[0],\n",
    "     \"n_first2Letters\": n[0:2],\n",
    "     \"n_first3Letters\": n[0:3],\n",
    "     \"n_lastLetter\": n[-1],\n",
    "     \"n_last2Letters\": n[-2:],\n",
    "     \"n_last3Letters\": n[-3:]\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def get_corpus_features2 (corpus):\n",
    "#gets a corpus, returns a list of features for every word\n",
    "    flat = [w for sent in corpus for w in sent]\n",
    "    pad = [(\"*\",\"*\",\"*\")]\n",
    "    flat = pad + flat + pad\n",
    "    X=[]\n",
    "    for i in range(1, len(flat)-1):\n",
    "        X.append(get_word_features2(flat[i],flat[i-1],flat[i+1]))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now let's train on the new data, and try to predict and check our new accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9528560361279594\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.59      0.79      0.68       984\n",
      "     B-MISC       0.53      0.50      0.52       445\n",
      "      B-ORG       0.80      0.74      0.77      1700\n",
      "      B-PER       0.86      0.78      0.82      1222\n",
      "      I-LOC       0.65      0.62      0.63       337\n",
      "     I-MISC       0.57      0.41      0.48       654\n",
      "      I-ORG       0.72      0.60      0.66      1366\n",
      "      I-PER       0.85      0.88      0.86       859\n",
      "          O       0.99      1.00      0.99     45356\n",
      "\n",
      "avg / total       0.95      0.95      0.95     52923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features2 = get_corpus_features2(train_sents)\n",
    "clf2 = train(train_sents, v, features2)\n",
    "test_features2 = get_corpus_features2(test_sents)\n",
    "y_predict2 = predict (clf2, v, test_features2)\n",
    "y_true2 = get_y (test_sents)\n",
    "print (\"accuracy:\", accuracy (y_predict2,y_true2))\n",
    "print (metrics.classification_report(y_true2, y_predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the new score is around 95% - an improvement on the previous attempt. Let's check on the Dutch dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.968742537214424\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.77      0.78      0.77       479\n",
      "     B-MISC       0.79      0.71      0.75       748\n",
      "      B-ORG       0.83      0.62      0.71       686\n",
      "      B-PER       0.68      0.82      0.74       703\n",
      "      I-LOC       0.68      0.30      0.41        64\n",
      "     I-MISC       0.56      0.44      0.49       215\n",
      "      I-ORG       0.80      0.53      0.64       396\n",
      "      I-PER       0.74      0.89      0.81       423\n",
      "          O       0.99      1.00      0.99     33973\n",
      "\n",
      "avg / total       0.97      0.97      0.97     37687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_features2 = get_corpus_features2(d_train_sents)\n",
    "d_clf2 = train(d_train_sents, v, d_features2)\n",
    "d_test_features2 = get_corpus_features2(d_test_sents)\n",
    "d_y_predict2 = predict (d_clf2, v, d_test_features2)\n",
    "d_y_true2 = get_y (d_test_sents)\n",
    "print (\"accuracy:\", accuracy (d_y_predict2, d_y_true2))\n",
    "print (metrics.classification_report(d_y_true2, d_y_predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a similar improvement in the Dutch model as well - who is now up to almost 97% accuracy score. We see some improvement in all categories, including the problematic ones. I-LOC for example is up from 0.29 to 0.41, which is an improvement but still not a good score by any means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.1.3 - Finding Illegal Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the tagging method we used was greedy tagging. We did not check the logic of the tagging, and in particular we didn't check if tag sequences were legal or not. Let's write a function that will find all illegal tag sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish: {'O-IX': 210, 'IX-IY': 232, 'BX-IY': 204}\n",
      "Dutch: {'O-IX': 51, 'IX-IY': 47, 'BX-IY': 127}\n"
     ]
    }
   ],
   "source": [
    "def find_illegal_sequences (guess):\n",
    "    OIX, IXIY, BXIY = 0,0,0\n",
    "    for i in range(len(guess)-1):\n",
    "        curr, next = guess[i], guess[i+1]\n",
    "        if curr[0]==\"O\" and next[0]==\"I\":\n",
    "            OIX+=1\n",
    "        elif curr[0]==\"I\" and next[0]==\"I\" and curr[1:] != next[1:]:\n",
    "            IXIY+=1\n",
    "        elif curr[0]==\"B\" and next[0]==\"I\" and curr[1:] != next[1:]:\n",
    "            BXIY+=1\n",
    "    return {\"O-IX\": OIX, \"IX-IY\": IXIY, \"BX-IY\": BXIY}\n",
    "\n",
    "print (\"Spanish:\", find_illegal_sequences(y_predict2))\n",
    "print (\"Dutch:\", find_illegal_sequences(d_y_predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Spanish dataset, all three illegal sequences seem to happen in similar frequency. Our test set includes 52923 words (and hence 52922 sequences of 2), and overall we guessed 646 illegal sequences - that's about 1.2% of our guesses. \n",
    "In the Dutch dataset, illegal sequences are rarer, and only take about 0.5% of our guesses (which is consistent with our results so far, in which our model did better in Dutch than Spanish).\n",
    "\n",
    "How can we use this information?\n",
    "If we were to change our predicting model to prevent it from predicting illegal sequences, we could theoretically improve our accuracy by **up to** 1.2%. Of course, this will be a difficult process. Even if we know that a sequence of 2 tags is illegal, we still face two problems:\n",
    "\n",
    "-We need to determine which of the two tags is wrong (or maybe both are wrong)\n",
    "-We need to correct it to the right answer.\n",
    "\n",
    "If we were to implement such an algorithm, a possible method of doing so would be to use the *predcit_proba* method, which for every word, returns the probability of each tag (a distribution). Using this method, a possible rough algorithm would be:\n",
    "\n",
    "-Predict normally\n",
    "\n",
    "-Look for illegal sequences - similar to *find_illegal_sequences* above, but rather than just counting, for every illegal sequence we find we will perform the following steps:\n",
    "\n",
    "-Determine which of the two tags is more likely to be wrong. We can do it by getting max probability in each word's distribution, and choosing the lower value of the two. \n",
    "For example: let w1,w2 be two words, and t1,t2 the tags with the highest probability. If p(t1)>p(t2), we will conclude that ws is more likely to be wrong.\n",
    "\n",
    "-Look for the second highest probability in w2's distribution - let's call it t2'. \n",
    "\n",
    "-Check if the sequence (t1, t2') is a legal sequence. If it is, change w2 tag to t2' and move to the next sequence.\n",
    "\n",
    "-If it's illegal, we'll try the next most likely tag - it might be the next option on w2's distribution, or the second option in w1's distribution. \n",
    "\n",
    "-Repeat until you get a legal sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So now we would like to further improve our results, by using word embeddings. Word embeddings are the results of mapping from words to numerical vectors, based on words context. \n",
    "The idea behind them is simple: words that often occur together in similar contexts, are likely to have similiar meanings. We can assume that words like 'cat' and 'dog' will frequently appear in similar context, while 'cat' and 'tractor' are less likely to. So we go through a lot of text, and for every word in our vocabulary, we collect data of all the words it appears frequently with. After collecting our data, we can perform various mathematical manipulations to turn this data into a vector for every word, in such a way that words that frequently appear together will have similar vectors. We will use those vector to try and improve our NER tagging.\n",
    "\n",
    "So first, let's import our word embeddings database (in Spanish):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors_file_vec = 'data\\wiki.es.vec'\n",
    "wordvecs = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a lot of cool things with is, such as finding the most similiar words to a given word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reina', 0.9727405905723572),\n",
       " ('consorte', 0.9092537760734558),\n",
       " ('infanta', 0.8831985592842102),\n",
       " ('princesa', 0.8758658766746521),\n",
       " ('consortes', 0.8660687208175659),\n",
       " ('berenguela', 0.8640629053115845),\n",
       " ('esposa', 0.862601637840271),\n",
       " ('hermanastra', 0.8556246161460876),\n",
       " ('regente', 0.8431113958358765),\n",
       " ('hija', 0.8396967053413391)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvecs.most_similar_cosmul(positive=['rey','mujer'],negative=['hombre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the one feature that interests us is the wordvecs feature, which returns a numerical vector of size 300 that represents a word. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.012764 , -0.027881 , -0.12001  ,  0.20901  ,  0.12917  ,\n",
       "        0.11149  ,  0.051667 , -0.019114 ,  0.25278  , -0.27741  ,\n",
       "        0.23895  ,  0.31481  , -0.1641   , -0.39813  , -0.055049 ,\n",
       "        0.061539 , -0.21919  , -0.15581  ,  0.049749 , -0.1363   ,\n",
       "        0.13232  ,  0.1721   , -0.14481  , -0.57531  , -0.22965  ,\n",
       "       -0.19064  ,  0.018659 ,  0.11678  ,  0.12292  ,  0.16606  ,\n",
       "       -0.22923  , -0.082918 ,  0.048586 , -0.3828   ,  0.42561  ,\n",
       "        0.20425  , -0.55707  ,  0.2868   , -0.11763  ,  0.065075 ,\n",
       "       -0.053827 ,  0.29591  , -0.11956  ,  0.27796  , -0.10618  ,\n",
       "       -0.31602  , -0.0011778,  0.15995  , -0.10007  ,  0.20116  ,\n",
       "        0.098252 , -0.34145  , -0.20297  ,  0.0050985, -0.1641   ,\n",
       "       -0.17609  ,  0.021475 , -0.10934  ,  0.036395 , -0.085886 ,\n",
       "       -0.011623 , -0.12768  ,  0.2338   , -0.082441 ,  0.47398  ,\n",
       "       -0.34687  ,  0.13387  , -0.15126  ,  0.11719  , -0.011416 ,\n",
       "       -0.13186  , -0.12157  , -0.43338  , -0.13693  , -0.26417  ,\n",
       "       -0.27996  ,  0.05085  , -0.32172  ,  0.11932  , -0.038279 ,\n",
       "       -0.16833  , -0.051152 ,  0.26825  ,  0.108    , -0.049512 ,\n",
       "       -0.08124  ,  0.11898  ,  0.24841  ,  0.25019  , -0.182    ,\n",
       "        0.16514  ,  0.25242  , -0.18547  , -0.029567 ,  0.016944 ,\n",
       "        0.32061  , -0.49323  , -0.12459  , -0.20812  , -0.088195 ,\n",
       "       -0.21952  , -0.16484  ,  0.057854 , -0.081644 , -0.087067 ,\n",
       "       -0.26222  , -0.013701 ,  0.11328  ,  0.1104   , -0.14473  ,\n",
       "       -0.015439 ,  0.012801 , -0.035237 , -0.090021 , -0.28895  ,\n",
       "        0.35738  , -0.02834  ,  0.065393 , -0.0073692, -0.094336 ,\n",
       "       -0.034635 ,  0.068267 , -0.15868  ,  0.0773   , -0.26494  ,\n",
       "        0.087048 ,  0.017102 ,  0.42795  ,  0.19467  , -0.049432 ,\n",
       "        0.041269 , -0.10648  , -0.10184  ,  0.21704  ,  0.23329  ,\n",
       "        0.027093 , -0.11359  , -0.19198  , -0.25129  ,  0.3256   ,\n",
       "       -0.31707  ,  0.31196  , -0.19876  ,  0.12515  ,  0.29853  ,\n",
       "        0.16416  ,  0.3245   ,  0.17049  ,  0.56965  ,  0.056343 ,\n",
       "        0.060293 ,  0.14712  ,  0.25813  ,  0.14682  , -0.16098  ,\n",
       "        0.17777  , -0.25111  , -0.043135 ,  0.098832 ,  0.045896 ,\n",
       "       -0.082729 ,  0.1662   ,  0.16275  , -0.23233  ,  0.23666  ,\n",
       "       -0.33059  , -0.36751  , -0.15504  , -0.066513 ,  0.13993  ,\n",
       "        0.13398  ,  0.51255  , -0.12053  , -0.24772  , -0.33333  ,\n",
       "       -0.3356   ,  0.0032378,  0.032853 , -0.021507 ,  0.014923 ,\n",
       "        0.35816  ,  0.16087  ,  0.18532  ,  0.031704 , -0.097824 ,\n",
       "       -0.063434 ,  0.17677  , -0.095959 , -0.01966  , -0.079799 ,\n",
       "        0.036589 ,  0.25665  ,  0.012553 , -0.20274  ,  0.31322  ,\n",
       "        0.12857  ,  0.39675  ,  0.1146   ,  0.082355 ,  0.12217  ,\n",
       "        0.28058  ,  0.034219 ,  0.39198  ,  0.089673 , -0.05012  ,\n",
       "        0.024749 ,  0.21839  , -0.29923  , -0.045059 , -0.0035621,\n",
       "       -0.026522 ,  0.13805  , -0.021702 ,  0.14808  , -0.13776  ,\n",
       "       -0.12525  ,  0.15337  ,  0.12509  , -0.10263  , -0.078571 ,\n",
       "       -0.50205  , -0.37766  ,  0.083631 , -0.088833 ,  0.05792  ,\n",
       "        0.034306 ,  0.085338 , -0.033502 , -0.053128 , -0.1136   ,\n",
       "        0.24891  , -0.30433  ,  0.2003   , -0.11869  , -0.13636  ,\n",
       "       -0.33804  ,  0.38657  ,  0.31885  , -0.09698  ,  0.17563  ,\n",
       "       -0.1541   , -0.22207  ,  0.13369  ,  0.15756  ,  0.084866 ,\n",
       "        0.066391 ,  0.24077  ,  0.08359  , -0.081925 ,  0.17856  ,\n",
       "       -0.11945  ,  0.13794  , -0.099836 ,  0.10411  , -0.21885  ,\n",
       "        0.12778  ,  0.17918  , -0.19547  ,  0.23722  , -0.1627   ,\n",
       "       -0.0773   ,  0.21912  , -0.11043  ,  0.10935  ,  0.13387  ,\n",
       "        0.3627   , -0.24971  ,  0.31158  ,  0.047827 , -0.21206  ,\n",
       "        0.13089  ,  0.1263   , -0.10432  ,  0.10302  , -0.086497 ,\n",
       "       -0.020981 , -0.019517 ,  0.0091034, -0.091594 ,  0.42599  ,\n",
       "       -0.28656  , -0.12951  ,  0.047451 , -0.080151 , -0.13998  ,\n",
       "        0.14481  , -0.27666  ,  0.13893  ,  0.10985  , -0.1539   ,\n",
       "        0.023797 ,  0.1153   ,  0.13895  , -0.071689 , -0.4254   ,\n",
       "       -0.17155  , -0.080621 , -0.071226 ,  0.094614 , -0.016933 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvecs.word_vec(\"hombre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it's time to extract our features. For each word we will have 900 features - it's vector, the vector of the previous and the next word. Note: this code returns only the word embeddings, and not the features we've used in the previous questions.\n",
    "\n",
    "Our first naive implementation failed, because we ran into a new problem: some of the words in our training sents were simply missing in the word embeddings dataset, which has caused it to throw an exception. We have to decide, what do we do in that case? \n",
    "Our first idea was to give it a set value, such as a vector of zeros. But we've decided against it, because we feared to create a wrong similarity between all the missing words (they would all have the exact same word embedding vector, so might be interpeted as the same word). This is why we've decided instead to generate random values for each missing word. It's not a perfect solution, and has some fairly noticeable downsides:\n",
    "\n",
    "1) Accidental similarity might occur (a randomly generated vector might be similar to an unrelated word)\n",
    "\n",
    "2) Several occurences of the same word will be assigned different vectors.\n",
    "\n",
    "But we still think it's preferable to giving the exact same vector to all missing words.\n",
    "So here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_emb_features (word, prev, next, wordvecs):\n",
    "    w = word[0]\n",
    "    p = prev[0]\n",
    "    n = next[0]\n",
    "    try: \n",
    "        wordvec = wordvecs.word_vec(w)\n",
    "    except KeyError:\n",
    "        wordvec = np.random.rand(300)*2-1\n",
    "    try: \n",
    "        p_wordvec = wordvecs.word_vec(p)\n",
    "    except KeyError:\n",
    "        p_wordvec = np.random.rand(300)*2-1\n",
    "    try: \n",
    "        n_wordvec = wordvecs.word_vec(n)\n",
    "    except KeyError:\n",
    "        n_wordvec = np.random.rand(300)*2-1\n",
    "    return np.concatenate((wordvec, p_wordvec, n_wordvec))\n",
    "\n",
    "def emb_corpus_features (corpus, wordvec):\n",
    "#gets a corpus, returns a list of features for every word\n",
    "    flat = [w for sent in corpus for w in sent]\n",
    "    pad = [(\"*\",\"*\",\"*\")]\n",
    "    flat = pad + flat + pad\n",
    "    X=[]\n",
    "    for i in range(1, len(flat)-1):\n",
    "        X.append(word_emb_features(flat[i],flat[i-1],flat[i+1], wordvec))\n",
    "    return X\n",
    "\n",
    "emb_features = emb_corpus_features(train_sents, wordvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have our features, and the next step in our workflow is running them into the DictVectorizer. But here we ran into a technical problem: the DictVectorizer (as his name implies...) is expecting to recieve a dictionary, while our word embeddings are in the form of a list of vectors.\n",
    "\n",
    "Of course, converting a vector into a dictionary is logically very easy. But when we tried it, we've ran into serious performance issues. Our dataset is huge - 264715 words with 900 features for each - and converting this into a dictionary simply takes a lot of time. Also logically, it's pretty silly to convert our matrix into a dictionary, only for the DictVectorizer to convert it back into a matrix.\n",
    "\n",
    "So instead we asked ourselves - why did we need the DictVectorizer in the first place?\n",
    "\n",
    "Well, the DictVectorizer has solved 3 problems for us:\n",
    "\n",
    "1) Convert our original features, who were in the form of a dictionary, into a matrix\n",
    "\n",
    "2) Convert string features into numerical one-hot encodings\n",
    "\n",
    "3) Created a sparse matrix for better performance and taking less memory\n",
    "\n",
    "Interestingly enough, all 3 problems aren't relevant anymore!\n",
    "\n",
    "1) Our data is already in the form of vectors list, with all vectors in the same size - which is pretty much identical to a matrix\n",
    "\n",
    "2) Word embeddings are numerical features, so such conversion isn't necessary\n",
    "\n",
    "3) As we recall, sparse matrices were only effective because most of our data was made of zeros. However, unlike one-hot vectors who are mostly zeros by definition, word embeddings are made of many values. Zero is one of those possible values, but it does not carry any special meaning and does not appear more frequently than others.\n",
    "\n",
    "Common confusion about point 3: earlier we said we needed sparse matrix for performance reasons - because my computer simply couldn't handle the non-sparse matrix. In a quick look, our situation is now much worse - instead of using a 13 (or 39) features, we now have 900 of them! But remember point 2 - because word embeddings are numerical features, conversion into one-hot encoding is unneccasary. So our new matrix is actually much **smaller** - it's size is 264715 X 900, as opposed to the one in the previous question - which size was a fairly insane 264715 X 115042!\n",
    "\n",
    "So basically, our DictVectorizer is now redundant! We'll just skip it and send our data straight to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_y = get_y (train_sents)\n",
    "emb_clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(emb_features, emb_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now it's time to evaluate our new exciting method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8686204485762334\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.32      0.27      0.30       984\n",
      "     B-MISC       0.07      0.03      0.04       445\n",
      "      B-ORG       0.46      0.45      0.46      1700\n",
      "      B-PER       0.45      0.32      0.38      1222\n",
      "      I-LOC       0.09      0.02      0.04       337\n",
      "     I-MISC       0.12      0.02      0.04       654\n",
      "      I-ORG       0.23      0.08      0.12      1366\n",
      "      I-PER       0.34      0.31      0.32       859\n",
      "          O       0.92      0.97      0.95     45356\n",
      "\n",
      "avg / total       0.83      0.87      0.85     52923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emb_test_features = emb_corpus_features(test_sents, wordvec)\n",
    "emb_y_predict = emb_clf.predict(emb_test_features)\n",
    "emb_y_true = get_y (test_sents)\n",
    "print (\"accuracy:\", accuracy (emb_y_predict, emb_y_true))\n",
    "print (metrics.classification_report(emb_y_true, emb_y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is, well, fairly disappointing. How can it be that we've used much more features, who are more meaningful, and still got much worse results? Something has to be wrong here...\n",
    "Well, upon serious examination we've found the problem. Apparently we were a bit too hasty with our treatment of missing words in the WE dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.132009897436866"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_missing_percentage (corpus, wordvec):\n",
    "    count = 0\n",
    "    flat = [w for sent in corpus for w in sent]\n",
    "    for w in flat:\n",
    "        w2 = w[0]\n",
    "        try: \n",
    "            wordvec.word_vec(w2)\n",
    "        except KeyError:\n",
    "            count+=1\n",
    "    return (count/len(flat))*100\n",
    "\n",
    "get_missing_percentage (train_sents, wordvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that a whooping 17% of our dataset is missing word embeddings! Which means that 17% of our predictions were based on random, meaningless numbers. Given that information, a 86% accuracy is actually fairly impressive.\n",
    "\n",
    "Of course, the actual situation is a bit more complicated. Because we're also looking at previous or next words, it's possible that for an unknown word, we'll still have correct information on previous and next words, hopefully allowing us to guess by context. This is probably how we could achieve 86% accuracy despite only having word embeddings for 83% of our dataset. \n",
    "But the point remains - a large bit of our training data is just wrong!\n",
    "\n",
    "So we will try to simply skip the missing words. This will make our training matrix smaller - but since the \"data\" we're skipping is actually just random numbers, it's not a great loss. \n",
    "We do have one significant loss though: because we're skipping some words, we now can't calculate information about previous and next words. What if it's missing? We'll have to either generate fake data (which is what we're trying to avoid in the first place), or find the closest non-missing word (which might be several words away, and might be irrelevant for the context of our word).\n",
    "\n",
    "So we'll only use word embeddings for the current word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_corpus_features_skip (corpus, wordvec):\n",
    "#gets a corpus, returns a list of features for every word\n",
    "    flat = [w for sent in corpus for w in sent]\n",
    "    X=[]\n",
    "    for i in range(1, len(flat)-1):\n",
    "        w = flat[i][0]\n",
    "        try: \n",
    "            v = wordvec.word_vec(w)\n",
    "            X.append([v, flat[i][2]])\n",
    "        except KeyError:\n",
    "            pass    \n",
    "    return X\n",
    "\n",
    "emb_skip = emb_corpus_features_skip (train_sents, wordvec)\n",
    "emb_features_skip = [x[0] for x in emb_skip]\n",
    "emb_y_skip = [x[1] for x in emb_skip]\n",
    "emb_skip_clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(emb_features_skip, emb_y_skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our coding approach is a bit different now - we  have to \"remember\" which words we skipped and also skip the relevant y value. So instead of getting features and y values separately, we're now generating them together, and returing a list of (X, y) pairs (with X being a vector of size 300).\n",
    "\n",
    "Let's test this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9784711684370258\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.00      0.00      0.00        13\n",
      "     B-MISC       0.00      0.00      0.00        47\n",
      "      B-ORG       0.00      0.00      0.00        31\n",
      "      B-PER       0.00      0.00      0.00        15\n",
      "      I-LOC       0.00      0.00      0.00        71\n",
      "     I-MISC       0.00      0.00      0.00       249\n",
      "      I-ORG       0.14      0.00      0.00       407\n",
      "      I-PER       0.00      0.00      0.00        60\n",
      "          O       0.98      1.00      0.99     41283\n",
      "\n",
      "avg / total       0.96      0.98      0.97     42176\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yaniv\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "emb_test_skip = emb_corpus_features_skip (test_sents, wordvec)\n",
    "emb_test_features_skip = [x[0] for x in emb_test_skip]\n",
    "emb_true_y_skip = [x[1] for x in emb_test_skip]\n",
    "emb_y_predict_skip = emb_skip_clf.predict(emb_test_features_skip)\n",
    "\n",
    "print (\"accuracy:\", accuracy (emb_y_predict_skip, emb_true_y_skip))\n",
    "print (metrics.classification_report(emb_true_y_skip, emb_y_predict_skip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at first glance, our accuracy is almost 98% now - which is the best we've had so far - but our classification matrix reveal a much more worrying truth. It seems that we we skipped most of the non-O words, which results in a mostly empty classification report. A quick check confirms this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', 97.88268209408194),\n",
       " ('I-ORG', 0.9650037936267071),\n",
       " ('I-MISC', 0.5903831562974203),\n",
       " ('I-PER', 0.14226100151745066),\n",
       " ('I-LOC', 0.16834218512898333),\n",
       " ('B-MISC', 0.11143778452200304),\n",
       " ('B-ORG', 0.07350151745068285),\n",
       " ('B-LOC', 0.030823216995447645),\n",
       " ('B-PER', 0.035565250379362666)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def counter_percentage (c, len):\n",
    "    return [(i, c[i] / len * 100.0) for i in c]\n",
    "\n",
    "c_skip = Counter(emb_true_y_skip)\n",
    "counter_percentage (c_skip, len (emb_true_y_skip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B-LOC', 1.8593050280596337),\n",
       " ('I-LOC', 0.6367741813578217),\n",
       " ('O', 85.70186875271621),\n",
       " ('B-ORG', 3.2122139712412374),\n",
       " ('I-ORG', 2.5811084027738413),\n",
       " ('B-PER', 2.309014984033407),\n",
       " ('I-PER', 1.6231128242918957),\n",
       " ('B-MISC', 0.8408442454131474),\n",
       " ('I-MISC', 1.2357576101128054)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter(emb_y_true)\n",
    "counter_percentage (c, len (emb_y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So almost 98% of our data is now O's, while before skipping, the O percentage was about 86%. Logically, this makes sense - O represents \"normal\" words in the language, while the rest of the values refer to names (of people, locations, organizations, etc). Names are by nature a lot less common and more specific, so it's hardly surprising that names like \"Melbourne\" are more likely to be missing in the WE dataset, than normal Spanish words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
