{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the solution to HW2, written by Yaniv Bin and Tair Hakman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first would like to import all the required modules in order for our code to run properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "import os.path\n",
    "from os.path import abspath, dirname, join\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import gzip, string, nltk, tarfile, itertools, random\n",
    "from glob import glob\n",
    "import re, os, sys\n",
    "from nltk.corpus import conll2002\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.externals.six.moves import html_parser\n",
    "from sklearn.externals.six.moves.urllib.request import urlretrieve\n",
    "from sklearn.datasets import get_data_home\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first task is identifying complex words. In order to do so we will use the data provided. \n",
    "first we need to load the data using the provided functions. \n",
    "<br>(For a better flow we modified the skeleton code within the notebook, but we added the edit into the skeleton file as well) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_file(data_file):\n",
    "    words = []\n",
    "    labels = []   \n",
    "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i > 0:\n",
    "                line_split = line[:-1].split(\"\\t\")\n",
    "                words.append(line_split[0].lower())\n",
    "                labels.append(int(line_split[1]))\n",
    "            i += 1\n",
    "    return words, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_file = abspath(join(dirname(\"__file__\"), \"data/complex_words_training.txt\"))\n",
    "development_file = abspath(join(dirname(\"__file__\"), \"data/complex_words_development.txt\"))\n",
    "test_file = abspath(join(dirname(\"__file__\"), \"data/complex_words_test_unlabeled.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data we may start working on the actual assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 - Evaluation Matrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually in order to evaluate how well a classifcation algorithm works we use three measures - \n",
    "- **Precision**: Measure how 'useful' the results are (how many are hits and how many are miss)\n",
    "- **Recall**: Measure how 'complete' the results are (out off all the actually possible results, how many did we hit)\n",
    "- **Fscore**: Measure the balance between the Precision and Recall\n",
    "\n",
    "We would like to implement functions that compose each of these matrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by calculating the precision - the precision is defined as:\n",
    "<br>$precision = \\frac{tp}{tp + fp}$\n",
    "<br>Where $tp$ stands for true-positive meaning a hit, and $fp$ stands for false-positive meaning the prediction is positive but it's actually a false alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Input: y_pred, a list of length n with the predicted labels,\n",
    "## y_true, a list of length n with the true labels\n",
    "\n",
    "## Calculates the precision of the predicted labels\n",
    "def get_precision(y_pred, y_true):\n",
    "    positive_hits_array = np.array([(y_pred[i] == y_true[i] == 1) for i in range(len(y_pred))])\n",
    "    positive_hits = np.count_nonzero(positive_hits_array == 1)\n",
    "    total_positive = np.count_nonzero(np.array(y_pred) == 1)\n",
    "    precision = float(positive_hits) / float(total_positive)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's make sure it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [0, 0, 1, 1]\n",
    "# Here we have one true positive at position 2, and 1 false positive at position 1\n",
    "assert(get_precision(y_pred, y_true) == (1 / (1 + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second measure we will implement is the recall, calculated as follows\n",
    "<br>$recall = \\frac{tp}{tp + fn}$\n",
    "<br>Where $fn$ stands for false-negative, meaning the prediction is negative also it should have been positive(a miss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Calculates the recall of the predicted labels\n",
    "def get_recall(y_pred, y_true):\n",
    "    positive_hits_array = np.array([(y_pred[i] == y_true[i] == 1) for i in range(len(y_pred))])\n",
    "    positive_hits = np.count_nonzero(positive_hits_array == 1)\n",
    "    false_negative_array = np.array([((not(y_pred[i] == y_true[i] )) and (y_pred[i] == 0)) for i in range(len(y_pred))])\n",
    "    false_negative =  np.count_nonzero(false_negative_array)\n",
    "    recall = float(positive_hits) / float(positive_hits + false_negative)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's again make sure this works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [1, 0, 1, 1]\n",
    "# We have one true positive at position 2, and two false negative at positions 0 and 3\n",
    "assert(get_recall(y_pred, y_true) == (1 / (1 + 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the last measure we want to implement is the fscore(also known as f1) which is calculated as:\n",
    "<br>$F = 2 * \\frac{precision*recall}{precision + recall}$\n",
    "<br> F score measure the relation between the precision and recall, the results vary between 0 and 1, but once it's equal 1 that means the relation is exact meaning the classifier is perfect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Calculates the f-score of the predicted labels\n",
    "def get_fscore(y_pred, y_true):\n",
    "    precision = get_precision(y_pred, y_true)\n",
    "    recall = get_recall(y_pred, y_true)\n",
    "    \n",
    "    # in case we might tr and divide by 0 \n",
    "    if(precision + recall) == 0:\n",
    "        return 0\n",
    "    \n",
    "    fscore = 2 * float(precision * recall) / float(precision + recall)\n",
    "    return fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And making sure it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [1, 0, 1, 1]\n",
    "# precision is 1/2, recall is 1/3\n",
    "assert(get_fscore(y_pred, y_true) == 2 * (((1 / 2) * (1 / 3)) / (1 / 2 + 1 / 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be extra catious and make sure we didn't mess up we also want to compare our results to scikit results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [1, 0, 1, 1]\n",
    "precision = get_precision(y_pred, y_true)\n",
    "recall = get_recall(y_pred, y_true)\n",
    "fscore = get_fscore(y_pred, y_true)\n",
    "\n",
    "sk_precision = metrics.precision_score(y_true, y_pred)\n",
    "sk_recall = metrics.recall_score(y_true, y_pred)\n",
    "sk_fscore = metrics.f1_score(y_true, y_pred)\n",
    "\n",
    "assert(precision == sk_precision)\n",
    "assert(recall == sk_recall)\n",
    "assert(fscore == sk_fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our own good we would also like to implement a function that prints out all the information given the two arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_predictions(y_pred, y_true):\n",
    "    print(\"Precision:\", get_precision(y_pred, y_true))\n",
    "    print(\"Recall:\", get_recall(y_pred, y_true))\n",
    "    print(\"Fscore:\", get_fscore(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5\n",
      "Recall: 0.3333333333333333\n",
      "Fscore: 0.4\n"
     ]
    }
   ],
   "source": [
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [1, 0, 1, 1]\n",
    "test_predictions(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after implementing the functions, we can go on and implement actual classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 - Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2.1 - All complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classifier we will implement is a very simple one that classifies all the words as complex no matter what they actually are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Labels every word complex\n",
    "def all_complex(data_file):\n",
    "    words, actual_labels = load_file(data_file)\n",
    "    all_complex_labels = np.ones((len(words),), dtype=int)\n",
    "    precision = get_precision(all_complex_labels, actual_labels)\n",
    "    recall = get_recall(all_complex_labels, actual_labels)\n",
    "    fscore = get_fscore(all_complex_labels, actual_labels)\n",
    "    performance = [precision, recall, fscore]\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we shall test it with each of the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Precision: 0.43275 \n",
      "Training Recall: 1.0 \n",
      "Training Fscore: 0.604083057058105\n"
     ]
    }
   ],
   "source": [
    "ac_tr_precision, ac_tr_recall, ac_tr_fscore = all_complex(training_file)\n",
    "print(\"Training Precision: {} \\nTraining Recall: {} \\nTraining Fscore: {}\".format(ac_tr_precision, ac_tr_recall, ac_tr_fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Precision: 0.418 \n",
      "Dev Recall: 1.0 \n",
      "Dev Fscore: 0.5895627644569816\n"
     ]
    }
   ],
   "source": [
    "ac_dv_precision, ac_dv_recall, ac_dv_fscore = all_complex(development_file)\n",
    "print(\"Dev Precision: {} \\nDev Recall: {} \\nDev Fscore: {}\".format(ac_dv_precision, ac_dv_recall, ac_dv_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can note how the recall of this model is always 1 - that makes sense because although it probably has a lot of false positives - it never \"miss\" in terms of false negtive because it always has an answer - complex. \n",
    "\n",
    "Another thing to note is that the Fscore is not that high - that's because the precision of this model is not that great, it actually mess up in more than half cases, which makes sense (assuming there aren't that many complex words in the dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2.2 - word length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second baseline we will implement is a word length based one, which gives a positive value to a word if it's length goes past a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1.2.2: Word length thresholding\n",
    "def word_length_baseline(data_file, threshold):\n",
    "    words, actual_labels = load_file(data_file)\n",
    "    threshold_labels = [(len(word) >= threshold) for word in words]\n",
    "    \n",
    "    precision = get_precision(threshold_labels, actual_labels)\n",
    "    recall = get_recall(threshold_labels, actual_labels)\n",
    "    fscore = get_fscore(threshold_labels, actual_labels)\n",
    "    preformance = [precision, recall, fscore]\n",
    "    return preformance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to run this for both the training dataset and the development dataset for different threshold values in order to get a \"feel\" about how it behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAAJwCAYAAADWXSa8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd6f11e0710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precisions = np.zeros(15)\n",
    "recall = np.zeros(15)\n",
    "fscores = np.zeros(15)\n",
    "for i in range(15):\n",
    "    tprecision, trecall, tfscore = word_length_baseline(training_file, i)\n",
    "    precisions[i] = tprecision\n",
    "    recall[i] = trecall\n",
    "    fscores[i] = tfscore\n",
    "\n",
    "plt.figure(num=1, figsize=(10, 10))\n",
    "pr_plt = plt.subplot(2 ,1, 1, xlabel=\"recall\", ylabel=\"precision\", label=\"pr\")\n",
    "pr_plt.set_title(\"precision/recall\", y=1.08)\n",
    "pr_plt.plot(recall, precisions)\n",
    "\n",
    "fs_plt = plt.subplot(2 ,1, 2, xlabel=\"threshold\", ylabel=\"fscore\", label=\"fs\")\n",
    "fs_plt.set_title(\"fscore measure\", y=1.01)\n",
    "fs_plt.plot(range(15), fscores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we explore the first plot we can see how a very high precision here means low recall and vice-versa, the real high recall is when the results mimic the previous results and label all as complex. the low precision is when the opposite happens and we might get a lot of false negative, on the other hand on that case the \"answers\" we do have are more than likely correct, so we get a good share of true positive, hence the high precision. \n",
    "\n",
    "Another thing we can clearly see here is how the Fscore starts low, goes up, and then it starts deteriorating like a slope - this makes sense as well considering how if the length is really short it should behave like our previous result, and then it should better itself until we reach a certaing length from which words just don't really accure (for example, we don't know many reasonable words with a length greater than 20, so although those are for sure complex, the odds of them showing up in our text are pretty small...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after we're done with our exploration we can go about building the \"ideal\" threshold classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Finds the best length threshold by f-score, and uses this threshold to\n",
    "## classify the training and development set\n",
    "def word_length_threshold(training_file, development_file):\n",
    "    best_tfscore = 0.0\n",
    "    best_i = 1\n",
    "    i = 1\n",
    "    while(True):\n",
    "        tprecision, trecall, tfscore = word_length_baseline(training_file, i)\n",
    "        if(tfscore < best_tfscore):\n",
    "            print(\"best threshold is: \", best_i)\n",
    "            break\n",
    "        else:\n",
    "            best_i = i\n",
    "            i += 1\n",
    "            best_tfscore = tfscore\n",
    "            \n",
    "    tprecision, trecall, tfscore = word_length_baseline(training_file, best_i)\n",
    "    dprecision, drecall, dfscore = word_length_baseline(development_file, best_i)\n",
    "    training_performance = [tprecision, trecall, tfscore]\n",
    "    development_performance = [dprecision, drecall, dfscore]\n",
    "    return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold is:  7\n",
      "Training Precision: 0.6007401315789473 \n",
      "Training Recall: 0.8440207972270364 \n",
      "Training Fscore: 0.7018976699495555\n",
      "\n",
      "Dev Precision: 0.6053511705685619 \n",
      "Dev Recall: 0.8660287081339713 \n",
      "Dev Fscore: 0.7125984251968505\n"
     ]
    }
   ],
   "source": [
    "wl_training_performance, wl_development_performance = word_length_threshold(training_file, development_file)\n",
    "wl_tr_precision, wl_tr_recall, wl_tr_fscore = wl_training_performance\n",
    "wl_dv_precision, wl_dv_recall, wl_dv_fscore = wl_development_performance\n",
    "print(\"Training Precision: {} \\nTraining Recall: {} \\nTraining Fscore: {}\\n\".format(wl_tr_precision, wl_tr_recall, wl_tr_fscore))\n",
    "print(\"Dev Precision: {} \\nDev Recall: {} \\nDev Fscore: {}\".format(wl_dv_precision, wl_dv_recall, wl_dv_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the threshold to Fscore graph it somewhat fits what we would expect - words that are long (about 7 characters) are usually considered complexed therefore the classifier was right. If we set the threshold too low we'd get a lot of false positives, so the precision will be high, but we will get a bad recall, meaning our classifier is very insensitive and doesn't preform well. If we take a high threshold, not many words will pass it (considering what we discussed), so we will get a lot of false negatives(which will give us a bad ). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2.3 - Word Frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to implement a method that classifies based on the frequency of a given word in the language, for that we use Google Ngram Counts - so we first have to load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1.2.3: Word frequency thresholding\n",
    "\n",
    "## Loads Google NGram counts\n",
    "def load_ngram_counts(ngram_counts_file): \n",
    "    counts = defaultdict(int) \n",
    "    with gzip.open(ngram_counts_file, 'rt') as f: \n",
    "        for line in f:\n",
    "            token, count = line.strip().split('\\t') \n",
    "            if token[0].islower(): \n",
    "                counts[token] = int(count) \n",
    "    return counts\n",
    "\n",
    "ngram_path = abspath(join(dirname(\"__file__\"), \"data/ngram_counts.txt.gz\"))\n",
    "ngram_counts = load_ngram_counts(ngram_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading it, we need to explore the ngrams and the text to see what is the range we need for our thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximal ('the', 47376829651) ('of', 30684794816) ('and', 22042266408) ('a', 14166008316) ('was', 5475451740)\n",
      "minimal ('zxxiz', 40) ('zvalking', 40) ('zuwiesen', 40) ('zurite', 40) ('zround', 40)\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter(ngram_counts)\n",
    "most_common = counter.most_common(10)\n",
    "least_common = counter.most_common()[:-10-1:-1]\n",
    "print(\"maximal\", most_common[0], most_common[1], most_common[2], most_common[5], most_common[9])\n",
    "print(\"minimal\", least_common[0], least_common[1], least_common[2], least_common[5], least_common[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this might not be the case in our training document, so let's explore it as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max frequencies 47376829651 751979946 591878281 225568038\n",
      "mean 47031121\n",
      "above mean 828\n"
     ]
    }
   ],
   "source": [
    "words, actual_labels = load_file(training_file)\n",
    "counts = []\n",
    "for i in range(0, len(words)):\n",
    "    counts.append(ngram_counts[words[i]])\n",
    "sorted_co = np.sort(np.array(counts))\n",
    "print(\"max frequencies\", sorted_co[3999], sorted_co[3998], sorted_co[3990], sorted_co[3900])\n",
    "print(\"mean %d\" % np.mean(sorted_co))\n",
    "j = 3999\n",
    "mean = np.mean(sorted_co)\n",
    "while j > 0:\n",
    "    if(sorted_co[j] >= mean):\n",
    "        j -= 1\n",
    "    else:\n",
    "        break\n",
    "print(\"above mean\", 3999 - j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see the maximal frequency is $47376829651$ and the minimal is $0-40$ , but the second best is not $30684794816$  like the word'of' so we're guessing 'of' is not part of the train set, and our dataset is not exactly filled with the most frequent words... Now we can use that info for thresholding for the ngrams, but in our training data it's usually around a much lower results, so we need to set a lower bound so we won't keep getting the same prediction all the time, so we went with an upper bout of 225568038 considering there are 100 other frequencies above it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the code that implements this thresholding - if a word is below the threshold frequency it is labeled complexed, and otherwise simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_frequency_baseline(data_file, counts, threshold):\n",
    "    words, actual_labels = load_file(data_file)\n",
    "    threshold_labels = [counts[word] < threshold for word in words]\n",
    "    precision = get_precision(threshold_labels, actual_labels)\n",
    "    recall = get_recall(threshold_labels, actual_labels)\n",
    "    fscore = get_fscore(threshold_labels, actual_labels)\n",
    "    preformance = [precision, recall, fscore]\n",
    "    return preformance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test this method using different values for the threshold - We used a range of 40 random numbers between 40 to 225568038 (after examining the Google Ngrams Count and our file count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresholds: [  8128062  17432678  32101818  33436282  45663413  46817621  61001947\n",
      "  64765005  83883358  89017951  90868143  91858679  94580343 106210919\n",
      " 114761500 116989328 118013217 121467328 125765397 139451431 148628850\n",
      " 150825784 160557908 162840903 165674378 167659089 177433281 179340735\n",
      " 179638296 182854718 183390220 188106533 193060872 202418364 204228780\n",
      " 209767169 216854225 218338798 219925803 221730816]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnEAAAJwCAYAAADm0TedAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAxJHADAkMQBAAxJHADAkMQBAAwFhYYe0isz334AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd6f13b5c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precisions = np.zeros(40)\n",
    "recall = np.zeros(40)\n",
    "fscores = np.zeros(40)\n",
    "thresholds = np.sort(np.random.randint(40, 225568038, size=40, dtype=int))\n",
    "print(\"thresholds:\", thresholds)\n",
    "for i in range(40):\n",
    "    tprecision, trecall, tfscore = word_frequency_baseline(training_file, ngram_counts, thresholds[i])\n",
    "    precisions[i] = tprecision\n",
    "    recall[i] = trecall\n",
    "    fscores[i]=tfscore\n",
    "\n",
    "    \n",
    "plt.figure(num=3, figsize=(10, 10))\n",
    "pr_plt = plt.subplot(2 ,1, 1, xlabel=\"recall\", ylabel=\"precision\", label=\"pr\")\n",
    "pr_plt.set_title(\"precision/recall\", y=1.08)\n",
    "pr_plt.plot(recall, precisions)\n",
    "\n",
    "fs_plt = plt.subplot(2 ,1, 2, xlabel=\"threshold\", ylabel=\"fscore\", label=\"fs\")\n",
    "fs_plt.set_title(\"fscore measure\", y=1.01)\n",
    "fs_plt.plot(thresholds, fscores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see (and it makes sense) that the tresholds can vary the recall and precision - a higher recall will be when the threshold is high, and a lower one when it's low and the exact opposite happens with the precision. We do see once we set the threshold lower we get a better fscore - this might be because of the amount of true positives we'd get that way (increases the amount of true positives), if we set it very low it will just act as all positive(which we've seen earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finds the best frequency threshold by f-score, and uses this threshold to\n",
    "## classify the training and development set\n",
    "def word_frequency_threshold(training_file, development_file, counts):\n",
    "    best_tfscore = 0.0\n",
    "    best_threshold = 0\n",
    "    i = 1\n",
    "    thresholds = np.sort(np.random.randint(40, 225568038, size=30, dtype=int))\n",
    "    print(\"thresholds:\", thresholds)\n",
    "    for threshold in thresholds:\n",
    "        tprecision, trecall, tfscore = word_frequency_baseline(training_file, counts, threshold)\n",
    "        if(tfscore > best_tfscore):\n",
    "            best_tfscore = tfscore\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    print(\"best threshold\", best_threshold)\n",
    "            \n",
    "    tprecision, trecall, tfscore = word_frequency_baseline(training_file, counts, best_threshold)\n",
    "    dprecision, drecall, dfscore = word_frequency_baseline(development_file, counts, best_threshold)\n",
    "    \n",
    "    training_performance = [tprecision, trecall, tfscore]\n",
    "    development_performance = [dprecision, drecall, dfscore]\n",
    "    return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresholds: [  5526777  11949521  19586293  31036274  39276891  41068715  41116522\n",
      "  46791838  47130241  55497391  61488496  61590198  66751807  71963085\n",
      "  72350336  81243287  82949366  85039117  96253292  99053620 117253599\n",
      " 119025546 121864387 129846952 147827082 150878283 168766271 180920236\n",
      " 185418235 220769339]\n",
      "best threshold 19586293\n",
      "Training Precision: 0.5660757453666398 \n",
      "Training Recall: 0.8116695551704217 \n",
      "Training Fscore: 0.666983147400902\n",
      "\n",
      "Dev Precision: 0.5569620253164557 \n",
      "Dev Recall: 0.8421052631578947 \n",
      "Dev Fscore: 0.6704761904761904\n"
     ]
    }
   ],
   "source": [
    "wf_training_performance, wf_development_performance = word_frequency_threshold(training_file, development_file, ngram_counts)\n",
    "wf_tr_precision, wf_tr_recall, wf_tr_fscore = wf_training_performance\n",
    "wf_dv_precision, wf_dv_recall, wf_dv_fscore = wf_development_performance\n",
    "print(\"Training Precision: {} \\nTraining Recall: {} \\nTraining Fscore: {}\\n\".format(wf_tr_precision, wf_tr_recall, wf_tr_fscore))\n",
    "print(\"Dev Precision: {} \\nDev Recall: {} \\nDev Fscore: {}\".format(wf_dv_precision, wf_dv_recall, wf_dv_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this preforms worse than the previous classifier - that might be because although this logic (words that are frequent are usually easier) holds for the real world, it might not hold for our text, we did see the mean of the frequencies in this specific training set is around 47031121 , and there are only 828 words above that mean which is not that many, so there just aren't a lot of very high frequency words in our text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3 - Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.3.1 - Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tried different baselines and saw the results from them, that were not that great, it's time to do actual classification.\n",
    "<br>The method we are going to use is called Naive Bayes Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We personally find that explaining this method via an example makes it a bit easier to grasp instead of just overloading people with probability formulas.\n",
    "<br>We shall use our example on the given problem considering it is most fitting, and afterwards we will implement the actual code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our dataset which is composed of two types of labels - 0 for a simple word, and 1 for a complex word.\n",
    "Now lets assume (for the sake of the example, the actual dataset might act differently but we shall see later) someone told us the following - there is a 70% chance of a word being simple, and a 30% chance that a word is complexed.\n",
    "Another thing we are told is that the mean of the word length of the simple words is 4 and the mean of word length of the complex words is 7.\n",
    "<br>Now we got a new word, and it has a length of 3, what will we guess it's label is? Well, an educated guess will be to say there's a big chance it's a simple word..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>__How did we assume that?__ We saw there is a high chance of being a simple word, and then we also saw the length of the word is 3 and usually shorter words are simple - so by likelihood the chances of our word being simple are pretty high.\n",
    "<br>The _Naive Bayes Classifier_ works pretty much just like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>__How exactly does it work?__\n",
    "<br> First thing to note is that the classifier assumes independence among features, this is not always true in real life (actually, it usually isn't...) but it simplifies the model - in our case it will assume there's no relation between the length of a word and it's frequency (although in reallity it's safe to assume a really long word won't be that frequent). \n",
    "Because of how wrong this assumption is, the classifier got the name _naive_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the classifier does is to find the probability of belonging to a class, given a set of features, in our case we can write it down as $P(simple | f_{length}, f_{freq})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have that we can use the Bayse rule to get this probabilty:\n",
    "<br>$P(simple \\vert f_{length}, f_{freq}) = \\frac{P(simple) * P(f_{length}, f_{freq} \\vert simple)}{P(f_{length}, f_{freq})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know $P(simple)=0.7$ and we don't need $P(f_{length}, f_{freq})$ to build a classifier, so all we are left to do is calculate $P(f_{length}, f_{freq} \\vert simple)$ . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply the conditional probability formula we will get:\n",
    "<br>$P(f_{length}, f_{freq} \\vert simple) = P(f_{length} \\vert simple) * P(f_{freq} \\vert simple, f_{length})$\n",
    "<br><br>Or in the general case (not our example) for $n$ features and a label $l$ we get:\n",
    "<br>$P(f_1, f_2, f_3, \\dots , f_n | l) = P(f_1 \\vert l) * P(f_2 \\vert l, f_1) * \\dots * P(f_n \\vert l, f_1, \\dots, f_{n-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can note that if we had n features, this will require a lot of data because we will have to have data for each of these assumption, but we don't actually need it. Why? well, this is where the _naive_ part comes in handy - we assumed all features are independent, so what we actually get is:\n",
    "<br> $P(f_{length}, f_{freq} | simple) = P(f_{length} \\vert simple) * P(f_{freq} \\vert simple)$\n",
    "<br><br>Or in the general case:\n",
    "<br>$P(f_1, f_2, f_3, \\dots , f_n \\vert l) = \\prod{P(f_i|l)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now finally to classify a new vector of features we have to choose the label for it (simple 0 or complex 1), to do so all we have to do is:\n",
    "<br>\n",
    "$Classifier(f_{l'}, f_{f'}) = \\arg\\max_{s\\in{0, 1}}P(f_{l'}, f_{f'} \\vert s)$ \n",
    "<br><br>Or in the general case for $m$ labels and $n$ features:\n",
    "<br>\n",
    "$Classifier(f_{1},\\dots, f_{n}) = \\arg\\max_{s\\in{0,\\dots,m}}P(f_{1},\\dots, f_{n} \\vert s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__implementation__\n",
    "<br>So now that we get the reasoning behind this method, we can go along and implement the classifier.\n",
    "<br>One last thing to note is that we were asked to use the Guassian based classifier, this classifier assumes the features follow a _normal_ distribution (meaning all the features falls on a normal curve)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code results in the classifier (note the code comments for a step by step explenation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1.3.1: Naive Bayes\n",
    "        \n",
    "## Trains a Naive Bayes classifier using length and frequency features\n",
    "def naive_bayes(training_file, development_file, counts):\n",
    "    #import training dataset\n",
    "    t_words, t_labels = load_file(training_file)\n",
    "    t_features = {}\n",
    "    # get length features\n",
    "    t_features[\"length\"] = np.array([len(word) for word in t_words])\n",
    "    # get frequency features\n",
    "    t_features[\"frequency\"] = np.array([counts[word] for word in t_words])\n",
    "    # build features array\n",
    "    X_t_original = np.array([t_features[\"length\"], t_features[\"frequency\"]]).T\n",
    "    \n",
    "    # normalize features\n",
    "    t_mean = np.mean(X_t_original)\n",
    "    t_sd = np.std(X_t_original)\n",
    "    X_t_scaled = (X_t_original - t_mean)/t_sd\n",
    "    \n",
    "    # train the classifier\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_t_scaled, t_labels)\n",
    "    \n",
    "    # extract features for development file\n",
    "    d_words, d_labels = load_file(development_file)\n",
    "    d_features = {}\n",
    "    # get length features\n",
    "    d_features[\"length\"] = np.array([len(word) for word in d_words])\n",
    "    # get frequency features\n",
    "    d_features[\"frequency\"] = np.array([counts[word] for word in d_words])\n",
    "    # build features array\n",
    "    X_d_original = np.array([d_features[\"length\"], d_features[\"frequency\"]]).T\n",
    "    \n",
    "    # normalize development features - note how we use the training mean and sd\n",
    "    X_d_scaled = (X_d_original - t_mean)/t_sd\n",
    "    \n",
    "    dev_pred = clf.predict(X_d_scaled)\n",
    "    train_pred = clf.predict(X_t_scaled)\n",
    "    \n",
    "    tprecision = get_precision(train_pred, t_labels)\n",
    "    trecall = get_recall(train_pred, t_labels)\n",
    "    tfscore = get_fscore(train_pred, t_labels)\n",
    "    \n",
    "    dprecision = get_precision(dev_pred, d_labels)\n",
    "    drecall = get_recall(dev_pred, d_labels)\n",
    "    dfscore = get_fscore(dev_pred, d_labels)\n",
    "    \n",
    "    training_performance = [tprecision, trecall, tfscore]\n",
    "    development_performance = [dprecision, drecall, dfscore]\n",
    "    return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Precision: 0.4853700516351119 \n",
      "Training Recall: 0.9774696707105719 \n",
      "Training Fscore: 0.6486486486486487\n",
      "\n",
      "Dev Precision: 0.4610983981693364 \n",
      "Dev Recall: 0.9641148325358851 \n",
      "Dev Fscore: 0.6238390092879257\n"
     ]
    }
   ],
   "source": [
    "nb_training_performance, nb_development_performance = naive_bayes(training_file, development_file, ngram_counts)\n",
    "nb_tr_precision, nb_tr_recall, nb_tr_fscore = nb_training_performance\n",
    "nb_dv_precision, nb_dv_recall, nb_dv_fscore = nb_development_performance\n",
    "print(\"Training Precision: {} \\nTraining Recall: {} \\nTraining Fscore: {}\\n\".format(nb_tr_precision, nb_tr_recall, nb_tr_fscore))\n",
    "print(\"Dev Precision: {} \\nDev Recall: {} \\nDev Fscore: {}\".format(nb_dv_precision, nb_dv_recall, nb_dv_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we look at the fscore measure in each of the methods we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAALKCAYAAAAvY6d9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsiRJklTz/wHou/CSTnW0OgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd6f12c3240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(num=5, figsize=(10, 10))\n",
    "types = [\"all complex\", \"word length\",\"word frequency\", \"naive bayse\"]\n",
    "f_scores_tr = [ac_tr_fscore, wl_tr_fscore, wf_tr_fscore, nb_tr_fscore]\n",
    "f_scores_dv = [ac_dv_fscore, wl_dv_fscore, wf_dv_fscore, nb_dv_fscore]\n",
    "bar_colors = ['b', 'g', 'r', 'c']\n",
    "ax_tr = plt.subplot(211)\n",
    "rectangles_tr = plt.bar(range(len(types)), f_scores_tr, width=0.5,\n",
    "                     color=bar_colors)\n",
    "ax_tr.set_xticks(np.linspace(0, len(types) - 1, len(types)))\n",
    "ax_tr.set_xticklabels(types, fontsize=10)\n",
    "ax_tr.set_ylabel('fscore (s)')\n",
    "ax_tr.set_title('Training set Fscore', y=1.08)\n",
    "\n",
    "ax_dv = plt.subplot(212)\n",
    "rectangles_dv = plt.bar(range(len(types)), f_scores_dv, width=0.5,\n",
    "                     color=bar_colors)\n",
    "ax_dv.set_xticks(np.linspace(0, len(types) - 1, len(types)))\n",
    "ax_dv.set_xticklabels(types, fontsize=10)\n",
    "ax_dv.set_ylabel('fscore (s)')\n",
    "ax_dv.set_title('Development set Fscore', y=1.08)\n",
    "\n",
    "\n",
    "def autolabel(ax, rectangles):\n",
    "    \"\"\"attach some text vi autolabel on rectangles.\"\"\"\n",
    "    for rect in rectangles:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width() / 2.,\n",
    "                1.05 * height, '%.4f' % height,\n",
    "                ha='center', va='bottom')\n",
    "        plt.setp(plt.xticks()[1], rotation=30)\n",
    "\n",
    "\n",
    "autolabel(ax_tr, rectangles_tr)\n",
    "autolabel(ax_dv, rectangles_dv)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Naive Bayse does not preform best out of all the classifiers. This might be because of our independence assumption which is really wrong (usually there is a relation between word frequencies and lengths), or because we just didn't choose the right features to work by (as we previously seen, choosing word frequencies as a feature does not preform the best on our dataset). \n",
    "\n",
    "As we can see word length preforms the best, that might be because there is actually a really reasonable correlation between the length of a word and how complexed it is, regardless of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.4 - ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases the way a words is classified is based on it's context, meaning sometimes a word can be classified as a simple word and sometimes as a difficult word, all because of the context it appeared in. \n",
    "\n",
    "We would like to show an example for that - in order to do so, we will find two instances of the same word labeled differently in the training set (the code can easily be adapted to any other set, we chose the training one because it made more sense), then we will explore the context trying to infere why they were tagged differently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we'd like to do is to load the context as well as the words, so we can show how it modifies the classification. In order to do so we need to change the file loading code a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_file_with_context(data_file):\n",
    "    words = []\n",
    "    labels = []   \n",
    "    contexts = []\n",
    "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i > 0:\n",
    "                line_split = line[:-1].split(\"\\t\")\n",
    "                words.append(line_split[0].lower())\n",
    "                labels.append(int(line_split[1]))\n",
    "                contexts.append(line_split[3])\n",
    "            i += 1\n",
    "    return words, labels, contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And using the code one the training data we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string 0 WASHINGTON -- The beleaguered director of the Secret Service resigned Wednesday after President Barack Obama lost confidence in her ability to lead the troubled agency amid a string of high-profile security lapses .\n"
     ]
    }
   ],
   "source": [
    "words, labels, contexts = load_file_with_context(training_file)\n",
    "print(words[0],labels[0],contexts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what we need to do is find a words with multiple instances that differ in how it's labeled. \n",
    "<br>To do so we will find for each word all it's instances in the words list, and then for words with mulitple labels we will look into the labels to see if they differ, if so we will check the context in which they appear to get a better grasp on why the context matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_context_sensitive_words(words, labels, contexts):\n",
    "    words = np.array(words)\n",
    "    differences = set()\n",
    "    for word in words:\n",
    "        ii = np.where(words == word)[0]\n",
    "        if(len(ii) > 1):\n",
    "            diff = [(i, j) for i in ii for j in ii if(not labels[i] == labels[j])]\n",
    "            if(len(diff) > 0):\n",
    "                # We only have to iterate it half way because the other half is a mirror of the first\n",
    "                for i in range(int(len(diff) / 2)):\n",
    "                    differences.add((words[diff[i][0]], diff[i][0], diff[i][1]))\n",
    "    return differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguity = find_context_sensitive_words(words, labels, contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can show all these words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: inspired\n",
      "Labeled as 1 in index 679 and as 0 in index 757\n",
      "First context:\n",
      " Miguel Castaeda started Tech Connect in 2009 , inspired by a similar program in San Francisco . \n",
      "\n",
      "Second context:\n",
      " Inspired by their courage , demonstrators across the South adopted their `` jail not bail '' tactic and filled jail cells . \n",
      "\n",
      "Word: hoverboard\n",
      "Labeled as 0 in index 2341 and as 1 in index 3339\n",
      "First context:\n",
      " `` Hoverboard is on fire , '' he says in a video taken right after he jumped off the burning piece of equipment . \n",
      "\n",
      "Second context:\n",
      " Timothy Cade had only owned his hoverboard for three days when it exploded beneath him . \n",
      "\n",
      "Word: campaign\n",
      "Labeled as 0 in index 400 and as 1 in index 3883\n",
      "First context:\n",
      " `` Given the way the meeting transpired , '' Ray Halbritter , an Oneida representative and leader of the `` Change the Mascot Campaign , '' said Wednesday , `` it became somewhat evident they were defending the continued use of the name . \n",
      "\n",
      "Second context:\n",
      " The campaign around the Maine Bear Hunting Ban Initiative , which will appear on the ballot as Question 1 , has shed light on Maine 's unique status in the wildlife world . \n",
      "\n",
      "Word: district\n",
      "Labeled as 1 in index 1108 and as 0 in index 3566\n",
      "First context:\n",
      " Now 80 and a resident of the Park La Brea complex in the Fairfax district , Prochnik uses that vat of chocolate as a centerpiece during talks about the Holocaust that he gives to schoolchildren . \n",
      "\n",
      "Second context:\n",
      " Before the jurors began their deliberations , District Judge George A. O'Toole Jr. reminded them that they had to decide if Tsarnaev would spend the rest of his life in prison or if he should be executed by the federal government . \n",
      "\n",
      "Word: league\n",
      "Labeled as 1 in index 2087 and as 0 in index 3391\n",
      "First context:\n",
      " Tom Brady has been an NFL golden boy , with movie-star looks , a supermodel wife , four Super Bowl championship rings , and a regular-guy-makes-good back story -- he went from being a sixth-round draft pick to one of the greatest quarterbacks in league history . \n",
      "\n",
      "Second context:\n",
      " One piece of that market , e-sport multiplayer video-gaming competitions , is expected to bring in $ 612 million this year for games such as Dota 2 and League of Legends . \n",
      "\n",
      "Word: slavery\n",
      "Labeled as 0 in index 2690 and as 1 in index 3585\n",
      "First context:\n",
      " Slavery was a `` side issue to the Civil War , '' said Pat Hardy , a Republican board member , when the board adopted the standards in 2010 . \n",
      "\n",
      "Second context:\n",
      " And when it comes to the Civil War , children are supposed to learn that the conflict was caused by `` sectionalism , states ' rights and slavery '' -- written deliberately in that order to telegraph slavery 's secondary role in driving the conflict , according to some members of the State Board of Education . \n",
      "\n",
      "Word: opponents\n",
      "Labeled as 1 in index 2023 and as 0 in index 3831\n",
      "First context:\n",
      " `` I love droppin ' 'em , '' she says about opponents . \n",
      "\n",
      "Second context:\n",
      " Opponents led by a group called Save Maine 's Bear Hunt say the Humane Society 's involvement shows that this is a fight being bankrolled by outside lobby groups with a broader agenda of ending sport hunting nationwide . \n",
      "\n",
      "Word: element\n",
      "Labeled as 0 in index 664 and as 1 in index 2990\n",
      "First context:\n",
      " Element 118 , for example , is the heaviest element to date , with 118 protons alongside 176 neutrons . \n",
      "\n",
      "Second context:\n",
      " Arturo Sanchez is in his element . \n",
      "\n",
      "Word: indy\n",
      "Labeled as 1 in index 3563 and as 0 in index 3696\n",
      "First context:\n",
      " `` If you want to learn how to run Indy cars , you do n't start at Indy . '' \n",
      "\n",
      "Second context:\n",
      " `` If you want to learn how to run Indy cars , you do n't start at Indy . '' \n",
      "\n",
      "Word: commissioner\n",
      "Labeled as 1 in index 1580 and as 0 in index 2663\n",
      "First context:\n",
      " `` As we have indicated earlier , another lawsuit of this type is not unexpected , '' NHL Deputy Commissioner Bill Daly said in an email to The Associated Press . \n",
      "\n",
      "Second context:\n",
      " NFL Commissioner Roger Goodell said he issued the penalties based on `` the critical importance of protecting the integrity of the game . '' \n",
      "\n",
      "Word: sprouts\n",
      "Labeled as 0 in index 2078 and as 1 in index 3026\n",
      "First context:\n",
      " The scientists were initially skeptical about what they would find on the construction site near the new Sprouts Farmers Market less than 100 yards from busy Matlock Road . \n",
      "\n",
      "Second context:\n",
      " An apple can kill , a sprinkle of sprouts can send you to the hospital and your succulent , pan-seared red snapper may actually be tilefish . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for couple in ambiguity:\n",
    "    word, index_a, index_b = couple\n",
    "    print(\"Word:\", word)\n",
    "    print(\"Labeled as {} in index {} and as {} in index {}\".format(labels[index_a], index_a, labels[index_b], index_b))\n",
    "    print(\"First context:\\n\", contexts[index_a], \"\\n\")\n",
    "    print( \"Second context:\\n\", contexts[index_b], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at some of the words one can see how and why they were labeled differently:\n",
    "- If we look at the word _sprouts_ it appears in two contexts - firstly as a name of a place \"Sprouts Farmer\" which makes it a very easy identification, but then as a 'sprinkle of sprouts' which is much more complexed (especially if like us you're not a native English speaker and had to google 'sprouts')\n",
    "- If we look at the word _element_ , seemengly a simple word, it can come as an actual element like in the first context(\"the heaviest element\") but it can also appear in as a slang word - \"in his element\" like in the second context, which complicate things. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Remark___ : We did now want to load the entire notebook as part of our notebook because that will just be overloading on our notebook, but we do want to touch upon required information so we imported the methods we need from the original notebook and implemented methods to fit the requirements of our question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to do is to explore the dataset they used for the classifiers implementation. In order to do that we will first have to load the dataset(note that we used the code they implemented to do so, but we modified it for our need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReutersParser(html_parser.HTMLParser):\n",
    "    \"\"\"Utility class to parse a SGML file and yield documents one at a time.\"\"\"\n",
    "\n",
    "    def __init__(self, encoding='latin-1'):\n",
    "        html_parser.HTMLParser.__init__(self)\n",
    "        self._reset()\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        method = 'start_' + tag\n",
    "        getattr(self, method, lambda x: None)(attrs)\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        method = 'end_' + tag\n",
    "        getattr(self, method, lambda: None)()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.in_title = 0\n",
    "        self.in_body = 0\n",
    "        self.in_topics = 0\n",
    "        self.in_topic_d = 0\n",
    "        self.title = \"\"\n",
    "        self.body = \"\"\n",
    "        self.topics = []\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "    def parse(self, fd):\n",
    "        self.docs = []\n",
    "        for chunk in fd:\n",
    "            self.feed(chunk.decode(self.encoding))\n",
    "            for doc in self.docs:\n",
    "                yield doc\n",
    "            self.docs = []\n",
    "        self.close()\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if self.in_body:\n",
    "            self.body += data\n",
    "        elif self.in_title:\n",
    "            self.title += data\n",
    "        elif self.in_topic_d:\n",
    "            self.topic_d += data\n",
    "\n",
    "    def start_reuters(self, attributes):\n",
    "        pass\n",
    "\n",
    "    def end_reuters(self):\n",
    "        self.body = re.sub(r'\\s+', r' ', self.body)\n",
    "        self.docs.append({'title': self.title,\n",
    "                          'body': self.body,\n",
    "                          'topics': self.topics})\n",
    "        self._reset()\n",
    "\n",
    "    def start_title(self, attributes):\n",
    "        self.in_title = 1\n",
    "\n",
    "    def end_title(self):\n",
    "        self.in_title = 0\n",
    "\n",
    "    def start_body(self, attributes):\n",
    "        self.in_body = 1\n",
    "\n",
    "    def end_body(self):\n",
    "        self.in_body = 0\n",
    "\n",
    "    def start_topics(self, attributes):\n",
    "        self.in_topics = 1\n",
    "\n",
    "    def end_topics(self):\n",
    "        self.in_topics = 0\n",
    "\n",
    "    def start_d(self, attributes):\n",
    "        self.in_topic_d = 1\n",
    "\n",
    "    def end_d(self):\n",
    "        self.in_topic_d = 0\n",
    "        self.topics.append(self.topic_d)\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "\n",
    "def stream_reuters_documents(data_path=None):\n",
    "    DOWNLOAD_URL = ('http://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "                        'reuters21578-mld/reuters21578.tar.gz')\n",
    "    ARCHIVE_FILENAME = 'reuters21578.tar.gz'\n",
    "    data_path = os.path.join(get_data_home(), \"reuters\")\n",
    "    if not os.path.exists(data_path):\n",
    "        \"\"\"Download the dataset.\"\"\"\n",
    "        print(\"downloading dataset (once and for all) into %s\" %\n",
    "              data_path)\n",
    "        os.mkdir(data_path)\n",
    "\n",
    "        def progress(blocknum, bs, size):\n",
    "            total_sz_mb = '%.2f MB' % (size / 1e6)\n",
    "            current_sz_mb = '%.2f MB' % ((blocknum * bs) / 1e6)\n",
    "            if _not_in_sphinx():\n",
    "                sys.stdout.write(\n",
    "                    '\\rdownloaded %s / %s' % (current_sz_mb, total_sz_mb))\n",
    "\n",
    "        archive_path = os.path.join(data_path, ARCHIVE_FILENAME)\n",
    "        urlretrieve(DOWNLOAD_URL, filename=archive_path,\n",
    "                    reporthook=progress)\n",
    "\n",
    "        print(\"untarring Reuters dataset...\")\n",
    "        tarfile.open(archive_path, 'r:gz').extractall(data_path)\n",
    "        print(\"done.\")\n",
    "          \n",
    "    parser = ReutersParser()\n",
    "    doc_counter = 0\n",
    "    docs = {\n",
    "        'title_index': [],\n",
    "        'title': [],\n",
    "        'body': [],\n",
    "        'topic': []\n",
    "    }\n",
    "    for filename in glob(os.path.join(data_path, \"*.sgm\")):\n",
    "        for doc in parser.parse(open(filename, 'rb')):\n",
    "            if(doc['topics']):\n",
    "                for topic in doc['topics']:\n",
    "                    docs['title_index'].append(doc_counter)\n",
    "                    docs['title'].append(doc['title'])\n",
    "                    docs['body'].append(doc['body'])\n",
    "                    docs['topic'].append(topic)\n",
    "            else:\n",
    "                docs['title_index'].append(doc_counter)\n",
    "                docs['title'].append(doc['title'])\n",
    "                docs['body'].append(doc['body'])\n",
    "                docs['topic'].append(None)\n",
    "            doc_counter += 1\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did is to modify the downloader function to return a dictionary of the form 'title','body', 'topic' which we then can feed into the Pandas dataframe module and explore it.\n",
    "Note how we arranged it by topics so it will be easier to handle for our needs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_stream = stream_reuters_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 445 topics in the dataset\n",
      "and 20030 unique titles\n",
      "there are 21578 documents in the dataset\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data=data_stream)\n",
    "print(\"there are {} topics in the dataset\".format(df.topic.nunique()))\n",
    "print(\"and {} unique titles\".format(df.title.nunique()))\n",
    "print(\"there are {} documents in the dataset\".format(df.title_index.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each of the topics we can get the count, mean, max index, std, and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    title_index                                                \\\n",
      "                          count          mean          std      min       25%   \n",
      "topic                                                                           \n",
      "acq                      2448.0  11102.989788  6136.448893      4.0   5398.25   \n",
      "adb-africa                 10.0   6504.000000  4112.422428   1086.0   3513.00   \n",
      "adb-asia                   20.0   4519.450000  6073.270622    451.0   1233.75   \n",
      "afghanistan                 3.0  15466.000000  7466.900562   6844.0  13300.00   \n",
      "alfonsin                    2.0   7864.500000  4480.935672   4696.0   6280.25   \n",
      "algeria                    35.0   8099.342857  5928.470535     96.0   3479.00   \n",
      "alum                       63.0  10444.507937  6873.331391    506.0   3321.00   \n",
      "amex                       60.0  10376.416667  5634.302229   1361.0   5927.25   \n",
      "andriessen                  1.0   1365.000000          NaN   1365.0   1365.00   \n",
      "angola                      2.0  16262.500000  3618.265399  13704.0  14983.25   \n",
      "antigua                     1.0   1411.000000          NaN   1411.0   1411.00   \n",
      "aqazadeh                    6.0   7882.500000  4249.519208   2685.0   5669.75   \n",
      "aquino                      4.0   7988.000000  7678.610682    240.0   4280.25   \n",
      "argentina                  90.0  11052.877778  6192.417010    161.0   5331.25   \n",
      "aruba                       1.0  21469.000000          NaN  21469.0  21469.00   \n",
      "ase                        11.0  11773.181818  7401.940648     11.0   5584.50   \n",
      "asean                       2.0   5910.000000  2983.990617   3800.0   4855.00   \n",
      "asx                         5.0  10631.000000  4756.055982   3796.0   8757.00   \n",
      "atpc                        3.0  16182.333333  3286.203635  12394.0  15141.00   \n",
      "austdlr                     4.0   9549.500000  6942.671220   4778.0   4802.00   \n",
      "australia                 270.0  12002.051852  6390.875236    261.0   7553.50   \n",
      "austria                    27.0  11601.407407  5701.876569   2737.0   6694.50   \n",
      "babangida                   2.0   6297.000000  2347.594514   4637.0   5467.00   \n",
      "bahamas                     1.0  11434.000000          NaN  11434.0  11434.00   \n",
      "bahrain                    46.0  11460.217391  6067.117287    450.0   5673.25   \n",
      "balladur                   35.0   9895.257143  6580.533727    251.0   5129.00   \n",
      "bangemann                   6.0   9322.833333  6028.959825   1118.0   4832.25   \n",
      "bangladesh                 34.0   9537.647059  6526.434559    276.0   3948.25   \n",
      "barbados                    3.0   5243.000000  6904.887906   1226.0   1256.50   \n",
      "barley                     54.0   9113.814815  6328.874880     98.0   3234.00   \n",
      "...                         ...           ...          ...      ...       ...   \n",
      "uruguay                     5.0  10429.800000  6073.055343   1540.0   8004.00   \n",
      "us-virgin-islands           2.0  15121.000000  4634.377844  11844.0  13482.50   \n",
      "usa                     12542.0  11019.017780  6083.025813      0.0   5845.25   \n",
      "ussr                      216.0  10720.583333  6220.333254    278.0   5367.75   \n",
      "vanuatu                     1.0   3885.000000          NaN   3885.0   3885.00   \n",
      "veg-oil                   137.0  11256.635036  6106.372857     79.0   5809.00   \n",
      "venezuela                  75.0  10158.920000  7024.072688    176.0   3546.00   \n",
      "verity                      3.0   5480.000000   714.615981   5030.0   5068.00   \n",
      "vietnam                     8.0  10875.500000  8778.538294   1206.0   1343.00   \n",
      "volcker                    79.0  10997.493671  6466.407360    830.0   7646.00   \n",
      "von-weizsaecker             7.0   9813.285714  4766.158261   4696.0   4816.50   \n",
      "wang-bingqian               2.0  20288.500000    28.991378  20268.0  20278.25   \n",
      "west-germany              567.0  10380.947090  6425.665568    237.0   4432.50   \n",
      "wheat                     306.0  10309.568627  6102.795830     93.0   4482.00   \n",
      "wilson                     16.0  10692.000000  3333.444885   5859.0   9687.50   \n",
      "wool                        2.0  13540.500000  6309.513809   9079.0  11309.75   \n",
      "worldbank                  87.0  11283.206897  6511.669425     45.0   5725.50   \n",
      "wpi                        32.0  13049.750000  6957.751444    231.0   7166.75   \n",
      "yemen-arab-republic         8.0  16694.125000  3286.541971  10463.0  15625.00   \n",
      "yemen-demo-republic         3.0  14668.666667  8034.528009   5501.0  11760.50   \n",
      "yen                        69.0  11865.318841  7190.323329    543.0   5541.00   \n",
      "yeutter                    63.0   9469.555556  5247.906802   1201.0   4839.50   \n",
      "young                       1.0   3811.000000          NaN   3811.0   3811.00   \n",
      "yugoslavia                 47.0   9522.574468  6044.465919    212.0   4148.00   \n",
      "zaire                       9.0  13805.000000  6428.403884   1397.0  11422.00   \n",
      "zambia                     23.0  10469.391304  5835.315530    244.0   5709.00   \n",
      "zhao-ziyang                 8.0  19036.750000  1400.069871  16708.0  19025.50   \n",
      "zimbabwe                   16.0  10644.687500  6113.248277   1105.0   4847.50   \n",
      "zinc                       44.0   9203.727273  5178.798987    876.0   6110.25   \n",
      "zse                         4.0   9008.500000  8673.583823   1055.0   4490.75   \n",
      "\n",
      "                                                 \n",
      "                         50%       75%      max  \n",
      "topic                                            \n",
      "acq                  11445.0  16271.25  21558.0  \n",
      "adb-africa            6746.5   7840.75  15143.0  \n",
      "adb-asia              1332.5   3280.50  17542.0  \n",
      "afghanistan          19756.0  19777.00  19798.0  \n",
      "alfonsin              7864.5   9448.75  11033.0  \n",
      "algeria               6749.0  12484.50  19335.0  \n",
      "alum                 10618.0  17112.00  21274.0  \n",
      "amex                  9411.5  15463.00  21465.0  \n",
      "andriessen            1365.0   1365.00   1365.0  \n",
      "angola               16262.5  17541.75  18821.0  \n",
      "antigua               1411.0   1411.00   1411.0  \n",
      "aqazadeh              6546.5  10719.50  14029.0  \n",
      "aquino                6592.0  10299.75  18528.0  \n",
      "argentina            11970.5  16606.50  21565.0  \n",
      "aruba                21469.0  21469.00  21469.0  \n",
      "ase                  12258.0  19039.00  21496.0  \n",
      "asean                 5910.0   6965.00   8020.0  \n",
      "asx                  11951.0  11958.00  16693.0  \n",
      "atpc                 17888.0  18076.50  18265.0  \n",
      "austdlr               6956.0  11703.50  19508.0  \n",
      "australia            12688.0  17912.75  21224.0  \n",
      "austria              11136.0  15895.50  20379.0  \n",
      "babangida             6297.0   7127.00   7957.0  \n",
      "bahamas              11434.0  11434.00  11434.0  \n",
      "bahrain              11966.0  17028.50  20678.0  \n",
      "balladur              6586.0  15693.00  20487.0  \n",
      "bangemann            10840.5  12861.75  16773.0  \n",
      "bangladesh           10255.0  13949.25  21219.0  \n",
      "barbados              1287.0   7251.50  13216.0  \n",
      "barley                8162.5  13210.50  20873.0  \n",
      "...                      ...       ...      ...  \n",
      "uruguay              11323.0  13704.00  17578.0  \n",
      "us-virgin-islands    15121.0  16759.50  18398.0  \n",
      "usa                  11332.0  16258.75  21576.0  \n",
      "ussr                 10855.5  15654.50  21565.0  \n",
      "vanuatu               3885.0   3885.00   3885.0  \n",
      "veg-oil              11444.0  16657.00  21258.0  \n",
      "venezuela            10266.0  16487.00  21186.0  \n",
      "verity                5106.0   5705.00   6304.0  \n",
      "vietnam              12808.0  18089.00  20282.0  \n",
      "volcker               7936.0  17669.50  21174.0  \n",
      "von-weizsaecker      13004.0  13125.50  15109.0  \n",
      "wang-bingqian        20288.5  20298.75  20309.0  \n",
      "west-germany          9486.0  15962.50  21450.0  \n",
      "wheat                10768.0  15528.25  21273.0  \n",
      "wilson               10593.5  11884.25  17640.0  \n",
      "wool                 13540.5  15771.25  18002.0  \n",
      "worldbank            12632.0  16634.00  21219.0  \n",
      "wpi                  16250.0  17452.50  21521.0  \n",
      "yemen-arab-republic  17179.0  18546.75  20485.0  \n",
      "yemen-demo-republic  18020.0  19252.50  20485.0  \n",
      "yen                  12032.0  19273.00  21286.0  \n",
      "yeutter               9181.0  13172.50  21459.0  \n",
      "young                 3811.0   3811.00   3811.0  \n",
      "yugoslavia            8657.0  13847.50  21516.0  \n",
      "zaire                15972.0  18485.00  19595.0  \n",
      "zambia               10850.0  15631.00  19180.0  \n",
      "zhao-ziyang          19765.5  19810.00  19833.0  \n",
      "zimbabwe             10978.5  15380.50  19193.0  \n",
      "zinc                  8791.5  13695.75  19062.0  \n",
      "zse                   6857.5  11375.25  21264.0  \n",
      "\n",
      "[445 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby('topic').describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring the dataset in terms of the documents it contains and the topics they are about, we want to explore the dataset content in terms of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_vocab_statistics(dataframe):    \n",
    "    print(\"statistics about the characters in each of the document's body:\\n\")\n",
    "    print(dataframe['body_length_chars'].describe(), \"\\n\")\n",
    "    \n",
    "    print(\"statistics about the words in each of the document's body:\\n\")    \n",
    "    print(dataframe['body_length_words'].describe(), \"\\n\")\n",
    "    \n",
    "    print(\"total amount of words in all the documents(body): {}\\n\".format(dataframe['body_length_words'].sum()))\n",
    "    print(\"total amount of characters in all the documents(body): {}\\n\".format(dataframe['body_length_chars'].sum()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we first want to modify the dataframe to have all the required fields for the statistics\n",
    "mod = df.drop(['topic'], axis=1)\n",
    "mod = mod.drop_duplicates(subset=['title_index'], keep='first')\n",
    "mod['body_length_chars'] = mod['body'].map(lambda x: len(x))\n",
    "mod['body_length_words'] = mod['body'].map(lambda x: len(word_tokenize(x)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics about the characters in each of the document's body:\n",
      "\n",
      "count    21578.000000\n",
      "mean       718.527621\n",
      "std        819.414178\n",
      "min          0.000000\n",
      "25%        235.000000\n",
      "50%        491.000000\n",
      "75%        904.000000\n",
      "max       9875.000000\n",
      "Name: body_length_chars, dtype: float64 \n",
      "\n",
      "statistics about the words in each of the document's body:\n",
      "\n",
      "count    21578.000000\n",
      "mean       132.294884\n",
      "std        151.179044\n",
      "min          0.000000\n",
      "25%         44.000000\n",
      "50%         89.000000\n",
      "75%        165.000000\n",
      "max       1857.000000\n",
      "Name: body_length_words, dtype: float64 \n",
      "\n",
      "total amount of words in all the documents(body): 2854659\n",
      "\n",
      "total amount of characters in all the documents(body): 15504389\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_vocab_statistics(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reporting and gathering all the information regarding the dataset, it is time for us to start analyzing the code and what's going on in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main principle the author of the code is trying to show is the idea of 'out of core' learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What is out of core learning?__ \n",
    "<br>Sometimes, especially when dealing with text documents, the dataset we get to work with is much greater than the actual RAM we have on a decent computer, so we wouldn't want to load all of it in one go (that will just result in our computer slowing down until it can't go on...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of out of core learning is this - we will load a certain amount of data each time and learn from it (batches), so generally speaking, most of our learning is done from things we don't hold in our memory anymore (therfore out of core)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, Scikit-learn implemented an API called \"partial_fit\" - a classifier that implements a partial fit works as follows: \n",
    "<br>The classifier will see a batch and then will incremently update whatever it is learning based on that batch, that way whenever a new batch arrives it learns what it needs from it and \"throws it away\" instead of holding it in memory or trying to learn everything in one go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, as we've seen just now the dataset we work with has over $2000000$ words, that is a very large dataset to work with in one batch (for example, in the last assignment we only had $978726$ and that took a while to process as well...), \n",
    "so to try and infere information from it, let alone actual learning, is a nearly impossible task if you use a regular computer. What we should do instead is 'chunk' the dataset (in the notebook they used a 1000 documents each time, and considering there are $21578$ documents, it means $\\frac{2854659}{21578} = 132.3$ (approx) words per document, so we only work with $132300$ words each time which is much more reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formal name for the learning process we just discussed is called _online setting_ - lets farther explain it (we'll focus on classification, but there are also online settings for regression problems).\n",
    "<br>On each round we receive an instance $x_t$ and extend a prediction using our current hypothesis $w_t$(we estimate weights). \n",
    "We then receive the true target $y_t$ and suffer an instantaneous loss based on the discrepancy between $y_t$ and our prediction.\n",
    "<br>Our goal is to make the cumulative loss that we suffer small.\n",
    "Finally, we update the hypothesis according to the previous hypothesis and the current example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more formally - we can describe this using math as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given $x_t \\in R^n$ , we predict $sign(w_t * x_t)$\n",
    "- We get the true target and suffer loss - $\\max({\\epsilon - y_{t}w_{t}*x_{t}})$\n",
    "- We update the hypothesis - $w_{t+1} = g(w_t, z_t)$\n",
    "\n",
    "Usually in classification $z_t$ is set to be the distance between our results and the true prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code they implemented test and compare a few classifiers that implement a partial fit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ___SGD___ - SGD stands for Stochastic Gradient Descent. This methood is really similar to the Gradient Desecent method we discussed in class, only this time it is stochastics, meaning the samples are selected randomly. Each sample at a time the gradient of the loss is estimated and the model is updated with a learning rate. Just like regular gradient descent we optimize $w$ by minimizing the loss function. In the begining we choose an initial guess for $w$ and a learning rate $\\eta$, we then randomly shuffle the examples in the training set, and then for each sample we update $w$ to be  $w = w - \\eta \\nabla Q_i(w)$ , we keep doing so until the algorithm converge. \n",
    "\n",
    "-  ___Perceptron___ - this is an algorithm for supervised learning of a binary classifier (a classifier which determines if an inputed represented by a vector of numbers fits a class or doesn't). the algorithm is based on a linear-predictor function, which is a function of the form $g(x) = wx + b$ where $x$ is our input, $w$ is a set of weights, and $b$ is the bias. what the perceptron does if to return $1$ if $g(x) > 0$ and $0$ otherwise. The algorithm learns the set of weights(tunes the weights) based on the training data, and then once it has them and it gets a vector x from the test data it feeds it to the function $g$ with the learned weights.\n",
    "\n",
    "-  ___NB Multinomial___ - Naive Bayes Multinomial Classifier is a Naive Bayes Classifier (see Part 1 of the assignment for  explenation) for multinomial models, that means that the feature vectors represent the frequencies with which certain events have been generated by a multinomial with probabilities $p_1, \\dots, p_n$ such that $p_i$ is the probability that event $i$ occures. A feature vector in this model is therefor a histogram with $x_i$ counting the number of times event i was observed in a particular instance. In document classification this  classifier is usually used with events representing the occurrence of a word in a single document (aka bag-of-words). \n",
    "\n",
    "-  ___Passive-Aggressive___ - this type of classifier uses a measure called _hinge-loss_ in it's training process. The hinge loss is defined as \n",
    "<br>$\\ell(w; (x,y)) = \\max(0, \\delta(w,(x,y))-\\epsilon)$\n",
    "<br> where $\\delta(w,(x,y))$ is the distance between $y$ and $xw$ : $\\delta(w,(x,y)) = |y_t-(x_{t}w_{t}|$\n",
    "<br> and $\\epsilon$ is the insensitivity parameter.\n",
    "<br> The algorithm goal is to make the hinge loss as small as possible at each round.\n",
    "<br> On each round of the algorithm we update the weights based on -\n",
    "<br> $w_{t+1} = \\arg\\min_{w \\in R^n} \\frac{||w-w_t||^2}{2} \\\\ s.t \\\\ \\ell(w; (x_t,y_t)) = 0$\n",
    "<br> Which is basically a projection of $w_t$ on the space of vectors that has a 0 loss.\n",
    "<br> This algorithm is _passive_ when the hinge loss is 0, because then we get $w_{t+1} = w_t$, but it's _aggressive_ when that's not the case, because when it's not 0 it aggressively forces $\\ell(w_{t+1}; (x_t,y_t)) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After explaining each of the Algorithms we only have one thing to address - the _hashing-vectorized_.\n",
    "<br>The method converts a collection of documents into a matrix of token occurences, and it uses scipy sparse matrix (which means that, considering this probably is a very sparse matrix, all the values that are assigned 0 are not kept in memory). The idea in this implementation is to use hashing to find the token string name to feature integer index mapping.\n",
    "<br>We need to use this method here for a few reasons:\n",
    "- It is very low in memory, we don't need to store an entire dictionary in memory. \n",
    "- There is no state computed during fit (in partial fit). \n",
    "- It holds no state besides the constructor parameters. \n",
    "\n",
    "As we note in the code, they used this method with $n_{features} = 2^{18}$, that is because if they don't do so, there might be collisions (different tokens mapped to same place)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we'll be looking into the task of classifying emails and SMS messages as 'spam' and 'ham' ('ham' being the opposite of spam - good, meaningful messages). We'll be looking and answering questions about pre-written code. The first part of the code loads the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "def progress(i, end_val, bar_length=50):\n",
    "    '''\n",
    "    Print a progress bar of the form: Percent: [#####      ]\n",
    "    i is the current progress value expected in a range [0..end_val]\n",
    "    bar_length is the width of the progress bar on the screen.\n",
    "    '''\n",
    "    percent = float(i) / end_val\n",
    "    hashes = '#' * int(round(percent * bar_length))\n",
    "    spaces = ' ' * (bar_length - len(hashes))\n",
    "    sys.stdout.write(\"\\rPercent: [{0}] {1}%\".format(hashes + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "NEWLINE = '\\n'\n",
    "HAM = 'ham'\n",
    "SPAM = 'spam'\n",
    "\n",
    "SOURCES = [\n",
    "    ('data/spam',        SPAM),\n",
    "    ('data/easy_ham',    HAM),\n",
    "    ('data/hard_ham',    HAM),\n",
    "    ('data/beck-s',      HAM),\n",
    "    ('data/farmer-d',    HAM),\n",
    "    ('data/kaminski-v',  HAM),\n",
    "    ('data/kitchen-l',   HAM),\n",
    "    ('data/lokay-m',     HAM),\n",
    "    ('data/williams-w3', HAM),\n",
    "    ('data/BG',          SPAM),\n",
    "    ('data/GP',          SPAM),\n",
    "    ('data/SH',          SPAM)\n",
    "]\n",
    "\n",
    "SKIP_FILES = {'cmds'}\n",
    "\n",
    "\n",
    "def read_files(path):\n",
    "    '''\n",
    "    Generator of pairs (filename, filecontent)\n",
    "    for all files below path whose name is not in SKIP_FILES.\n",
    "    The content of the file is of the form:\n",
    "        header....\n",
    "        <emptyline>\n",
    "        body...\n",
    "    This skips the headers and returns body only.\n",
    "    '''\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = open(file_path, encoding=\"latin-1\")\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield file_path, content\n",
    "\n",
    "\n",
    "def build_data_frame(l, path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for i, (file_name, text) in enumerate(read_files(path)):\n",
    "        if ((i + l) % 100 == 0):\n",
    "            progress(i + l, 58910, 50)\n",
    "        rows.append({'text': text, 'class': classification})\n",
    "        index.append(file_name)\n",
    "   \n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame, len(rows)\n",
    "\n",
    "def load_data():\n",
    "    data = DataFrame({'text': [], 'class': []})\n",
    "    l = 0\n",
    "    for path, classification in SOURCES:\n",
    "        data_frame, nrows = build_data_frame(l, path, classification)\n",
    "        data = data.append(data_frame)\n",
    "        l += nrows\n",
    "    data = data.reindex(np.random.permutation(data.index))\n",
    "    return data\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a fancy code, but the imporant thing about it is the data structure: a pandas dataframe in size (D,2) - D being the number of **documents** in the collection.<br>For each document, there are two columns: <br>text (the actual text of the document) and class (classification as ham/spam).\n",
    "\n",
    "This is the training code (no need to run it yet, just understand how it works):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "        ('classifier',         MultinomialNB())\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def train(data=None, n_folds=6):\n",
    "    if data is None:\n",
    "        print(\"Loading data...\")\n",
    "        data = load_data()\n",
    "        print(\"Data loaded\")\n",
    "    k_fold = KFold(n_splits = n_folds)\n",
    "    pipeline = build_pipeline()\n",
    "    scores = []\n",
    "    confusion = numpy.array([[0, 0], [0, 0]])\n",
    "    print(\"Training with %d folds\" % n_folds)\n",
    "    for i, (train_indices, test_indices) in enumerate(k_fold.split(data)):\n",
    "        train_text = data.iloc[train_indices]['text'].values\n",
    "        train_y = data.iloc[train_indices]['class'].values.astype(str)\n",
    "\n",
    "        test_text = data.iloc[test_indices]['text'].values\n",
    "        test_y = data.iloc[test_indices]['class'].values.astype(str)\n",
    "\n",
    "        print(\"Training for fold %d\" % i)\n",
    "        pipeline.fit(train_text, train_y)\n",
    "        print(\"Testing for fold %d\" % i)\n",
    "        predictions = pipeline.predict(test_text)\n",
    "\n",
    "        confusion += confusion_matrix(test_y, predictions)\n",
    "        score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "        scores.append(score)\n",
    "        print(\"Score for %d: %2.2f\" % (i, score))\n",
    "        print(\"Confusion matrix for %d: \" % i)\n",
    "        print(confusion)\n",
    "\n",
    "    print('Total emails classified:', len(data))\n",
    "    print('Score:', sum(scores)/len(scores))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick overview of how it works:\n",
    "The code uses a technique called k-fold (cross validation). Instead of simply splitting the corpus to a training and test set, it splits the dataset into k equal parts. The code than runs k times, each time selecting a different part of the corpus as the test set (and the rest k-1 parts as training). So this way, we can use the entire corpus as both training and test set. The disadvantage to this is, of course, performance (the whole learning/predicting process runs k times). \n",
    "In this case, we can assume why this technique was used: our corpus is made of several different sources, so simply splitting the corpus would have likely resulted in a test set that isn't similar to the training set.\n",
    "\n",
    "Another new technique introduced is the pipeline: it is used to create a sequence of several transformers (and a final estimator), and package them all as one object. It does not provide new functionality on it's own, but rather serves as a convinient method to pack a workflow of several transformers.\n",
    "\n",
    "The first element in the pipeline is the CountVectorizer. It's basic function is simple - it recieves a list of documents, and creates a matrix of documents X token counts. So if a token j appears 4 times in a document i, the matrix value in cell (i,j) will be 4. A token can be an n-gram as well, and we're able to choose an n-gram range, so the Vectorizer is able to mix tokens of different sizes in one matrix. In this example, our n-range is 1-2, meaning we check unigrams and bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create the CountVectorizer, and use it to transform our data into a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 2))\n",
    "data_x, data_y = data['text'], data['class']\n",
    "X = cv.fit_transform(data_x, data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're using the entire corpus, with no training/test split, because the k-fold technique ensures that the entire corpus will eventually be used as both training and test data.\n",
    "\n",
    "First we'll calculate the number of unigrams and bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bigrams': 3318380, 'unigrams': 697570}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_uni_bi (fnames):\n",
    "    cnt = Counter([len(feature.split()) for feature in fnames])\n",
    "    return {\"unigrams\": cnt.get(1), \"bigrams\": cnt.get(2)}\n",
    "\n",
    "fnames = cv.get_feature_names()\n",
    "count_uni_bi (fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both unigrams and bigrams are used as features (and they're the only features), we could use *get_feature_names* to get the list of all unigrams and bigrams. From here it's just a matter of separating them and counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to get the most frequent unigrams and bigrams. We could work directly with our matrix, summing every column and printing the top 50 biggest numbers of each n-gram. However, because of the matrix size, we found this to be problematic performance-wise. \n",
    "Instead, we would like to recreate the tokens and count ourselves. Luckily, scikit provides easy access to the very same preprocessing methods they use, so we can assure our tokenization methods are identical. We've created a function that splits the data into unigrams and bigrams, and we use the Counter \"most_common\" method on both.\n",
    "Not that the bigrams split is also done within a document (meaning the last word of document and the first word of the next document will **not** create a bigram) - this is consistent with the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_uni_bi (data, analyzer):\n",
    "    uni, bi = [], []\n",
    "    for doc in data:\n",
    "        tokens = analyzer(doc)\n",
    "        for t in tokens:\n",
    "            if len(t.split()) == 1:\n",
    "                uni.append(t)\n",
    "            else:\n",
    "                bi.append(t)\n",
    "    return [uni,bi]\n",
    "\n",
    "analyzer = cv.build_analyzer()\n",
    "uni_bi_list = split_uni_bi(data_x, analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('font', 588020),\n",
       " ('3d', 573645),\n",
       " ('the', 523545),\n",
       " ('br', 506598),\n",
       " ('td', 412043),\n",
       " ('to', 384631),\n",
       " ('and', 304840),\n",
       " ('nbsp', 288463),\n",
       " ('of', 276364),\n",
       " ('http', 240249),\n",
       " ('20', 230871),\n",
       " ('size', 226011),\n",
       " ('tr', 225925),\n",
       " ('in', 206212),\n",
       " ('width', 197032),\n",
       " ('com', 192779),\n",
       " ('you', 159126),\n",
       " ('face', 153450),\n",
       " ('for', 152577),\n",
       " ('border', 148637),\n",
       " ('is', 147840),\n",
       " ('style', 134521),\n",
       " ('this', 127919),\n",
       " ('align', 127774),\n",
       " ('span', 125907),\n",
       " ('href', 122468),\n",
       " ('height', 122384),\n",
       " ('html', 119934),\n",
       " ('color', 119523),\n",
       " ('www', 116191),\n",
       " ('that', 103891),\n",
       " ('on', 101327),\n",
       " ('your', 96299),\n",
       " ('content', 92507),\n",
       " ('table', 87217),\n",
       " ('with', 86005),\n",
       " ('be', 84848),\n",
       " ('div', 84766),\n",
       " ('arial', 80928),\n",
       " ('it', 79414),\n",
       " ('from', 76064),\n",
       " ('we', 74683),\n",
       " ('or', 74405),\n",
       " ('center', 73872),\n",
       " ('img', 72069),\n",
       " ('as', 71809),\n",
       " ('enron', 70007),\n",
       " ('src', 69740),\n",
       " ('are', 69688),\n",
       " ('text', 67392)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"unigrams:\")\n",
    "Counter(uni_bi_list[0]).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('nbsp nbsp', 163936),\n",
       " ('br br', 156754),\n",
       " ('font size', 111788),\n",
       " ('td tr', 108261),\n",
       " ('http www', 105357),\n",
       " ('font face', 104446),\n",
       " ('td td', 98916),\n",
       " ('tr td', 90633),\n",
       " ('3d http', 83221),\n",
       " ('style 3d', 77299),\n",
       " ('font td', 68641),\n",
       " ('tr tr', 66263),\n",
       " ('href 3d', 61747),\n",
       " ('font font', 57538),\n",
       " ('of the', 55143),\n",
       " ('td width', 52193),\n",
       " ('color 3d', 50305),\n",
       " ('font color', 47765),\n",
       " ('img src', 47259),\n",
       " ('href http', 47094),\n",
       " ('in the', 44708),\n",
       " ('arial helvetica', 43463),\n",
       " ('width 3d', 43361),\n",
       " ('sans serif', 38861),\n",
       " ('face 3d', 36805),\n",
       " ('size 3d2', 35974),\n",
       " ('align center', 35433),\n",
       " ('content type', 35306),\n",
       " ('helvetica sans', 35306),\n",
       " ('bgcolor 3d', 35249),\n",
       " ('src http', 33977),\n",
       " ('1px solid', 33420),\n",
       " ('gif width', 33254),\n",
       " ('font family', 33168),\n",
       " ('tr table', 31674),\n",
       " ('size 3d', 31530),\n",
       " ('face 3darial', 31036),\n",
       " ('br font', 30866),\n",
       " ('face 3dverdana', 30531),\n",
       " ('3d font', 30291),\n",
       " ('src 3d', 28006),\n",
       " ('span style', 27985),\n",
       " ('text html', 26424),\n",
       " ('face arial', 26149),\n",
       " ('align 3d', 25481),\n",
       " ('td align', 25429),\n",
       " ('size 3d1', 25116),\n",
       " ('if you', 24335),\n",
       " ('body html', 24299),\n",
       " ('html charset', 23600)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"bigrams:\")\n",
    "Counter(uni_bi_list[1]).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these words look unlikely (they mostly look like HTML documents), but we've verified with the original matrix too. This is a spam dataset, after all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next task, we'll need the same split but for each class. So we'll filter the dataset per class and use the same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ham = data.loc[data['class'] == 'ham']\n",
    "spam = data.loc[data['class'] == 'spam']\n",
    "ham_x, spam_x = ham['text'], spam['text']\n",
    "ham_uni_bi_list = split_uni_bi(ham_x, analyzer)\n",
    "spam_uni_bi_list = split_uni_bi(spam_x, analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham unigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 271739),\n",
       " ('to', 185712),\n",
       " ('and', 123494),\n",
       " ('of', 116399),\n",
       " ('in', 90256),\n",
       " ('com', 71121),\n",
       " ('for', 70561),\n",
       " ('enron', 70004),\n",
       " ('is', 60087),\n",
       " ('on', 59190),\n",
       " ('http', 58170),\n",
       " ('that', 55662),\n",
       " ('you', 50956),\n",
       " ('td', 46347),\n",
       " ('this', 43429),\n",
       " ('it', 41292),\n",
       " ('with', 39740),\n",
       " ('be', 38868),\n",
       " ('ect', 38444),\n",
       " ('20', 37356),\n",
       " ('width', 36024),\n",
       " ('have', 33884),\n",
       " ('from', 33265),\n",
       " ('will', 33164),\n",
       " ('www', 32780),\n",
       " ('as', 32712),\n",
       " ('we', 32597),\n",
       " ('at', 31830),\n",
       " ('3d', 29716),\n",
       " ('are', 29348),\n",
       " ('by', 29116),\n",
       " ('font', 27064),\n",
       " ('or', 25012),\n",
       " ('tr', 23382),\n",
       " ('if', 22790),\n",
       " ('not', 22224),\n",
       " ('your', 21613),\n",
       " ('br', 21065),\n",
       " ('height', 20361),\n",
       " ('09', 19971),\n",
       " ('src', 19681),\n",
       " ('gif', 19436),\n",
       " ('img', 19420),\n",
       " ('an', 18535),\n",
       " ('10', 18144),\n",
       " ('hou', 18101),\n",
       " ('has', 17889),\n",
       " ('was', 17650),\n",
       " ('2001', 17145),\n",
       " ('href', 16761)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"ham unigrams:\")\n",
    "Counter(ham_uni_bi_list[0]).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham bigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('http www', 30946),\n",
       " ('of the', 26553),\n",
       " ('in the', 21549),\n",
       " ('img src', 17425),\n",
       " ('hou ect', 17313),\n",
       " ('ect ect', 17154),\n",
       " ('src http', 15448),\n",
       " ('gif width', 14323),\n",
       " ('href http', 13703),\n",
       " ('to the', 12840),\n",
       " ('on the', 12145),\n",
       " ('for the', 11875),\n",
       " ('09 09', 11240),\n",
       " ('td width', 11184),\n",
       " ('td tr', 10618),\n",
       " ('td td', 10294),\n",
       " ('will be', 10226),\n",
       " ('cnet com', 9924),\n",
       " ('com gif', 9873),\n",
       " ('if you', 9428),\n",
       " ('to be', 8192),\n",
       " ('tr td', 7881),\n",
       " ('com click', 7810),\n",
       " ('width height', 7550),\n",
       " ('enron enron', 7479),\n",
       " ('online com', 7399),\n",
       " ('http clickthru', 7398),\n",
       " ('clickthru online', 7354),\n",
       " ('font face', 7349),\n",
       " ('enron com', 6858),\n",
       " ('with the', 6778),\n",
       " ('the company', 6738),\n",
       " ('and the', 6524),\n",
       " ('pm to', 6131),\n",
       " ('at the', 5962),\n",
       " ('3d http', 5788),\n",
       " ('zdnet com', 5605),\n",
       " ('www cnet', 5539),\n",
       " ('you have', 5478),\n",
       " ('tr table', 5447),\n",
       " ('that the', 5418),\n",
       " ('www zdnet', 5253),\n",
       " ('arial helvetica', 5069),\n",
       " ('width 3d', 4999),\n",
       " ('height td', 4917),\n",
       " ('am to', 4798),\n",
       " ('message from', 4758),\n",
       " ('original message', 4748),\n",
       " ('from the', 4720),\n",
       " ('font td', 4636)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"ham bigrams:\")\n",
    "Counter(ham_uni_bi_list[1]).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam unigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('font', 560956),\n",
       " ('3d', 543929),\n",
       " ('br', 485533),\n",
       " ('td', 365696),\n",
       " ('nbsp', 278903),\n",
       " ('the', 251806),\n",
       " ('size', 213214),\n",
       " ('tr', 202543),\n",
       " ('to', 198919),\n",
       " ('20', 193515),\n",
       " ('http', 182079),\n",
       " ('and', 181346),\n",
       " ('width', 161008),\n",
       " ('of', 159965),\n",
       " ('face', 144363),\n",
       " ('border', 133328),\n",
       " ('style', 131069),\n",
       " ('span', 124769),\n",
       " ('align', 123644),\n",
       " ('com', 121658),\n",
       " ('in', 115956),\n",
       " ('color', 113473),\n",
       " ('html', 112620),\n",
       " ('you', 108170),\n",
       " ('href', 105707),\n",
       " ('height', 102023),\n",
       " ('content', 89698),\n",
       " ('is', 87753),\n",
       " ('this', 84490),\n",
       " ('www', 83411),\n",
       " ('div', 82954),\n",
       " ('for', 82016),\n",
       " ('your', 74686),\n",
       " ('arial', 73977),\n",
       " ('table', 73719),\n",
       " ('center', 70018),\n",
       " ('text', 63758),\n",
       " ('img', 52649),\n",
       " ('class', 51690),\n",
       " ('body', 51559),\n",
       " ('src', 50059),\n",
       " ('strong', 50059),\n",
       " ('or', 49393),\n",
       " ('3d2', 49029),\n",
       " ('3d0', 48932),\n",
       " ('that', 48229),\n",
       " ('type', 47829),\n",
       " ('with', 46265),\n",
       " ('be', 45980),\n",
       " ('bgcolor', 45055)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"spam unigrams:\")\n",
    "Counter(spam_uni_bi_list[0]).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam bigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('nbsp nbsp', 161630),\n",
       " ('br br', 155432),\n",
       " ('font size', 107762),\n",
       " ('td tr', 97643),\n",
       " ('font face', 97097),\n",
       " ('td td', 88622),\n",
       " ('tr td', 82752),\n",
       " ('3d http', 77433),\n",
       " ('style 3d', 76837),\n",
       " ('http www', 74411),\n",
       " ('font td', 64005),\n",
       " ('tr tr', 61843),\n",
       " ('href 3d', 59828),\n",
       " ('font font', 55922),\n",
       " ('color 3d', 49814),\n",
       " ('font color', 46183),\n",
       " ('td width', 41009),\n",
       " ('arial helvetica', 38394),\n",
       " ('width 3d', 38362),\n",
       " ('face 3d', 35937),\n",
       " ('size 3d2', 35712),\n",
       " ('sans serif', 35613),\n",
       " ('content type', 34595),\n",
       " ('align center', 34124),\n",
       " ('helvetica sans', 33665),\n",
       " ('1px solid', 33420),\n",
       " ('href http', 33391),\n",
       " ('bgcolor 3d', 33067),\n",
       " ('font family', 32091),\n",
       " ('face 3darial', 30773),\n",
       " ('size 3d', 30654),\n",
       " ('face 3dverdana', 30419),\n",
       " ('3d font', 30038),\n",
       " ('img src', 29834),\n",
       " ('of the', 28590),\n",
       " ('br font', 27848),\n",
       " ('span style', 27816),\n",
       " ('tr table', 26227),\n",
       " ('text html', 26213),\n",
       " ('align 3d', 25160),\n",
       " ('size 3d1', 24888),\n",
       " ('td align', 24565),\n",
       " ('src 3d', 24095),\n",
       " ('body html', 23957),\n",
       " ('html charset', 23429),\n",
       " ('in the', 23159),\n",
       " ('border 3d0', 22882),\n",
       " ('3d border', 22503),\n",
       " ('content 3d', 22260),\n",
       " ('face arial', 22173)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"spam bigrams:\")\n",
    "Counter(spam_uni_bi_list[1]).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to find the top 20 features for each class. Scikit offers a solution for this case - SelectFromModel. However, we ran into two problems when trying to use it:\n",
    "\n",
    "- SelectFromModel does not support sparse matrices (and our matrix is way too big to not be sparse)\n",
    "\n",
    "- More importantly - for binary classification tasks, SelectFromModel can only returns one list of top features (and not a feature for each class).\n",
    "\n",
    "We've had to dig deeper, and when we checked the source code, we discovered that SelectFromModel basically just returns the features with the highest coefficient (more specifically, highest absolute value of coefficient). So we've decided to work with the coefficient ourselves. So first, let's get our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = data_x[:49092]\n",
    "train_y = data_y[:49092]\n",
    "X = cv.fit_transform(train_text)\n",
    "mnb = MultinomialNB().fit(X, train_y)\n",
    "coefs = mnb.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this time we do split to train and testing data, but we only split once in the 'normal' method, without using cross-validation.\n",
    "\n",
    "We have our coefficient list, but we still run into the same problem - we only have a general list for the classifier, and not for each category. At first we thought that each side end of the list represents a different class (so the top 20 features will be class A, and the bottom 20 class B). But upon further inspection, this turned out to be wrong. We looked at the \"feature count\" attribute of each feature - which represents the number of samples encountered for each (class, feature) during fitting.\n",
    "The bottom 20 looked pretty good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 20:\n",
      "span 911.0 103775.0\n",
      "style 2918.0 109775.0\n",
      "border 12591.0 113789.0\n",
      "face 7763.0 121431.0\n",
      "br br 1110.0 129477.0\n",
      "of 98276.0 133201.0\n",
      "width 29719.0 134165.0\n",
      "nbsp nbsp 1940.0 134805.0\n",
      "and 104232.0 151114.0\n",
      "http 49440.0 152449.0\n",
      "20 31982.0 164036.0\n",
      "to 156345.0 165854.0\n",
      "tr 19507.0 168473.0\n",
      "size 10971.0 178949.0\n",
      "the 229124.0 209795.0\n",
      "nbsp 7996.0 232885.0\n",
      "td 38705.0 303903.0\n",
      "br 17513.0 404872.0\n",
      "3d 23097.0 460158.0\n",
      "font 23201.0 469529.0\n"
     ]
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "args = np.argsort(coefs)\n",
    "top_20, bottom_20 = args[:20], args[-20:]\n",
    "bottom = [feature_names[i] for i in bottom_20]\n",
    "print (\"bottom 20:\")\n",
    "for i in range(20):\n",
    "    print (bottom[i], mnb.feature_count_[0][bottom_20[i]], mnb.feature_count_[1][bottom_20[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the top 20, on the other hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 20:\n",
      "had instituted 1.0 0.0\n",
      "warner enron 8.0 0.0\n",
      "warner director 1.0 0.0\n",
      "warner coo 2.0 0.0\n",
      "warner chief 2.0 0.0\n",
      "warner cable 7.0 0.0\n",
      "oeno6lwysulh4ppy8dza2fdw8obl5qaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaach5baaa 2.0 0.0\n",
      "oeno6lwysulh4ppy8dza2fdw8obl5qaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaach5baaa aaaalaaaaaagaayaaaueylradhbujvi4xjmncirq1dqsb0m9vzqgdijeqgkbads 2.0 0.0\n",
      "warner enw 1.0 0.0\n",
      "copenhagen you 2.0 0.0\n",
      "copenhagen online 1.0 0.0\n",
      "copenhagen get 1.0 0.0\n",
      "copenhagen for 1.0 0.0\n",
      "copenhagen dow 1.0 0.0\n",
      "copenhagen br 1.0 0.0\n",
      "copene petroquemica 2.0 0.0\n",
      "copene elektro 4.0 0.0\n",
      "copenhagen said 1.0 0.0\n",
      "warner expects 2.0 0.0\n",
      "warner font 2.0 0.0\n"
     ]
    }
   ],
   "source": [
    "top = [feature_names[i] for i in top_20]\n",
    "print (\"top 20:\")\n",
    "for i in range(20):\n",
    "    print (top[i], mnb.feature_count_[0][top_20[i]], mnb.feature_count_[1][top_20[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all those features have extremely small feature counts - which means they are extremely rare, and not valuable features. \n",
    "This explains why Scikit used the top **absolute values** of coefficients - all our coefficients are negative numbers, and here we see that the bottom negative numbers are the valueable ones - hence, the one who have the highest absolute value.\n",
    "Also, inspecting Scikit's code and documentation, nowhere does it implies there's any use to the worst coefficients (for example, SelectFromModel throws them away beneath a certain threshold). \n",
    "\n",
    "So, the coefficient list on it's own doesn't solve our problem - we had to dig even deeper. We discovered a curious thing: when we checked coeff for other classification tasks, with more than one class, coeff actually returned a separate list for every class. Only in binary tasks, coeff returns only one list. So we looked into Naive Bayes code to discover what are those coefficient values, and this is what we found:\n",
    "\n",
    "def _get_coef(self):\n",
    "\n",
    "    return (self.feature_log_prob_[1:]\n",
    "    \n",
    "            if len(self.classes_) == 2 else self.feature_log_prob_)\n",
    "            \n",
    "(this is a quote from Scikit's code, so we can't run it here).\n",
    "\n",
    "We disover that the coefficient is nothing more than the log probability of every feature. And also, it seems that for some reason, for binary tasks, Scikit decides specifically to only return the coefficients for the first class. But luckily, we can still access the original log probabilities, and get both lists. We just have to check which class is 0, and which is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam'], dtype='<U4')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we just have to access the bottom 20 (or top 20 absolute values) features of every list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top features for ham:\n",
      "20 31982.0\n",
      "ect 32186.0\n",
      "be 32697.0\n",
      "with 33375.0\n",
      "it 34800.0\n",
      "this 36660.0\n",
      "td 38705.0\n",
      "you 42716.0\n",
      "that 46872.0\n",
      "http 49440.0\n",
      "on 49774.0\n",
      "is 50865.0\n",
      "enron 58591.0\n",
      "for 59593.0\n",
      "com 60661.0\n",
      "in 75896.0\n",
      "of 98276.0\n",
      "and 104232.0\n",
      "to 156345.0\n",
      "the 229124.0\n",
      "\n",
      "top features for spam:\n",
      "span 103775.0\n",
      "style 109775.0\n",
      "border 113789.0\n",
      "face 121431.0\n",
      "br br 129477.0\n",
      "of 133201.0\n",
      "width 134165.0\n",
      "nbsp nbsp 134805.0\n",
      "and 151114.0\n",
      "http 152449.0\n",
      "20 164036.0\n",
      "to 165854.0\n",
      "tr 168473.0\n",
      "size 178949.0\n",
      "the 209795.0\n",
      "nbsp 232885.0\n",
      "td 303903.0\n",
      "br 404872.0\n",
      "3d 460158.0\n",
      "font 469529.0\n"
     ]
    }
   ],
   "source": [
    "ham_coeffs = mnb.feature_log_prob_[0]\n",
    "spam_coeffs = mnb.feature_log_prob_[1]\n",
    "\n",
    "def print_bottom_names (coeffs, feature_names, n, class_num):\n",
    "    args = np.argsort(coeffs)\n",
    "    bottom_co = args[-n:]\n",
    "    bottom = [feature_names[i] for i in bottom_co]\n",
    "    for i in range(n):\n",
    "        print (bottom[i], mnb.feature_count_[class_num][bottom_co[i]])\n",
    "        \n",
    "print (\"top features for ham:\")\n",
    "print_bottom_names(ham_coeffs, feature_names, 20, 0)\n",
    "\n",
    "print()\n",
    "print (\"top features for spam:\")\n",
    "print_bottom_names(spam_coeffs, feature_names, 20, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our results know makes perfect sense - we see that \"ham\" words are generally pretty normal (\"be\", \"with\" etc) while \"spam\" words are generally more nonsense. Each word also have very high feature count for its class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we want to add another feature to our model - the length of the text. But we should consider that the new feature is usually on a completely different scale to our normal features. The length of most documents is thousands of characters, While our usual features (number of appearences for a certain word/bigram in a document) is much lower. Is this a problem?\n",
    "\n",
    "Let's return to the way Naive Bayes classifier works, as explained in part 1:\n",
    "- We assume the features are independent\n",
    "- We calculate the probability of belonging to a specific class:\n",
    "<br>$P(f_1, f_2, f_3, \\dots , f_n \\vert l) = \\prod{P(f_i|l)}$\n",
    "\n",
    "As we can see, the probability for each feature is calculated separately and independently. A feature with very large values might have a bias in it's probability - because that feature is likely to have a large standard deviation. However, since we're using Naive Bayes for a **classification** task, this have a very minimal effect on our predictions (we essentialy only care about which class has a higher probability - not the probability itself). By the time we multiply the different features, they're all converted to probabilities (which are of course scaled between 0-1), so there's no significant effect on the result.\n",
    "\n",
    "If we were to use Naive Bayes for estimating probability, we would need to scale the features. However, this isn't recommended, because Naive Bayes is a bad estimator for probabilities (mostly because it's naive assumption, of all features being independent, is almost always untrue). But Naive Bayes works well for classification tasks, and the common experience and opinion is that scaling isn't neccasary for those tasks.\n",
    "\n",
    "What about Logistic Regression? \n",
    "<br>Well, first we have to understand how it works. Logistic Regression is similar to Linear Regression (like we implemented in homework1, question 2). The main difference is that while we now wish to use it for binary classification tasks (while linear regression predicted the y value, but not a binary value). So instead, we use the Sigmoid Function to normalize our predicted values between 0 and 1.\n",
    "\n",
    "$$ h_ \\theta (x) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-z} }  $$ \n",
    "\n",
    "z = the features polynom\n",
    "\n",
    "Logistic regression, on it's own, does not require scaling. The reason is that after repeated iterations, the model will choose the optimal coefficients for each feature, and effectively scale the features using the coefficients. That's not to say scaling is useless. Many optimization algorithms (such as gradient descent) converges faster on normalized values, meaning that scaling will lead to better performance. But it isn't neccesary.\n",
    "\n",
    "So when is scaling neccesary?\n",
    "<br>If we're using a Logistic Regression model **with regularization**. Because naturally, features with smaller values are likely to have bigger coefficients (scaling them up to have a similar effect to the bigger values), and vice versa. As we remember from homework1, regularization penalizes the size of the coefficients. So effectively, regularization will penalize features with small values (and hence, large coefficients) more than large ones.\n",
    "\n",
    "Back to our specific model. The Logistic Regression used in the model uses the solver 'lbfgs'. According to Scikit documentation, this solver works with the L2 penalty function, which is sensitive to scaling. So in order to use this length feature in the Logistic Regression model, we'll need to scale it - but not because of the Logistic Regression itself, but because of the regularization method it uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's add the length feature to our model. We will build our own Pipeline, and we'll also use FeatureUnion. FeatureUnion concatenates lists of features from different transformers. Meaning we can \"prepeare\" several feature sets separately (running each one through different transformers), and use the FeatureUnion to combine them to a single feature list.\n",
    "\n",
    "The FeatureUnion and the Pipeline accept objects called transformers. Transformers in Scikit are objects who inherit from general class TransformerMixin, and implement two methods: fit and transform. In our case, we will build a very simple transformer that returns the length of each string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LengthTransformer(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return np.array([len(doc) for doc in X]).reshape(-1,1)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our fit method does nothing (as there's no need to fit the data in any way). Our transform method, at it's core, returns the length of every item in the training set. It also prepears the datat for the scaler - by converting it into a numpy array, and reshaping it (as the scaler does not accept 1D arrays).\n",
    "Now here's our FeatureUnion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "union = FeatureUnion([\n",
    "                (\"count\", CountVectorizer(ngram_range=(1, 2))), \n",
    "                (\"length\", Pipeline([(\"lt\", LengthTransformer()), (\"scaler\", StandardScaler(with_mean=False))]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge two feature sets - the \"count' feature set is the CountVectorizer from eariler, while the \"length\" feature set is our newly written length transformer. We normalize the length, for reasons we explained in the previous question.\n",
    "\n",
    "So here's our complete pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_pipeline2():\n",
    "    pipeline = Pipeline([\n",
    "        ('features',   FeatureUnion([\n",
    "                (\"count\", CountVectorizer(ngram_range=(1, 2))), \n",
    "                (\"length\", Pipeline([(\"lt\", LengthTransformer()), (\"scaler\", StandardScaler(with_mean=False))]))])), \n",
    "        ('classifier',         LogisticRegression(solver='lbfgs'))\n",
    "    ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's run the original code, with our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 4 folds\n",
      "Training for fold 0\n",
      "Testing for fold 0\n",
      "Score for 0: 1.00\n",
      "Confusion matrix for 0: \n",
      "[[5853   32]\n",
      " [  52 8791]]\n",
      "Training for fold 1\n",
      "Testing for fold 1\n",
      "Score for 1: 0.99\n",
      "Confusion matrix for 1: \n",
      "[[11760    65]\n",
      " [  112 17519]]\n",
      "Training for fold 2\n",
      "Testing for fold 2\n",
      "Score for 2: 0.99\n",
      "Confusion matrix for 2: \n",
      "[[17614    91]\n",
      " [  175 26303]]\n",
      "Training for fold 3\n",
      "Testing for fold 3\n",
      "Score for 3: 1.00\n",
      "Confusion matrix for 3: \n",
      "[[23415   124]\n",
      " [  227 35144]]\n",
      "Total emails classified: 58910\n",
      "Score: 0.9950302530201399\n",
      "Confusion matrix:\n",
      "[[23415   124]\n",
      " [  227 35144]]\n"
     ]
    }
   ],
   "source": [
    "def train2(data = None, n_folds = 4):\n",
    "    if data is None:\n",
    "        print(\"Loading data...\")\n",
    "        data = load_data()\n",
    "        print(\"Data loaded\")\n",
    "    k_fold = KFold(n_splits = n_folds)\n",
    "    pipeline = build_pipeline2()\n",
    "    scores = []\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    print(\"Training with %d folds\" % n_folds)\n",
    "    for i, (train_indices, test_indices) in enumerate(k_fold.split(data)):\n",
    "        train_text = data.iloc[train_indices]['text'].values\n",
    "        train_y = data.iloc[train_indices]['class'].values.astype(str)\n",
    "        test_text = data.iloc[test_indices]['text'].values\n",
    "        test_y = data.iloc[test_indices]['class'].values.astype(str)\n",
    "        \n",
    "        print(\"Training for fold %d\" % i)\n",
    "        pipeline.fit(train_text, train_y)\n",
    "        print(\"Testing for fold %d\" % i)\n",
    "        predictions = pipeline.predict(test_text)\n",
    "        \n",
    "        confusion += confusion_matrix(test_y, predictions)\n",
    "        score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "        scores.append(score)\n",
    "        \n",
    "        print(\"Score for %d: %2.2f\" % (i, score))\n",
    "        print(\"Confusion matrix for %d: \" % i)\n",
    "        print(confusion)\n",
    "\n",
    "    print('Total emails classified:', len(data))\n",
    "    print('Score:', sum(scores)/len(scores))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion)\n",
    "    return pipeline\n",
    "    confusion = confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "    print(\"Score for %d: %2.2f\" % (i, score))\n",
    "    print(\"Confusion matrix for %d: \" % i)\n",
    "    print(confusion)\n",
    "    print('Total emails classified:', len(test_text))\n",
    "    return pipeline\n",
    "\n",
    "pipeline = train2(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will create a NER (Named Entity Recognition) model that recognizes names of persons, organizations and other entities in text. We will use the CoNLL 2002 dataset. As we build the code, we will use the Spanish version of the dataset. At the end of every segment, we'll test our functions on the Dutch dataset and compare the differences. \n",
    "First, we will import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "etr = conll2002.chunked_sents('esp.train') # In Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our data, as presented in the assignment, comes in the form of **chunked_sents**. This is a tree data structure, that represent the data like this:\n",
    "\n",
    "- A word is represented as a pair (word, POS tagging).\n",
    "\n",
    "- Each sentence is a tree. The root of the tree is always \"S\".\n",
    "\n",
    "- The words of the sentence are the children of the root, depending on their NER tag:\n",
    "\n",
    "For NER tag O, the word itself (represented by a pair) will be a direct children of the root. \n",
    "\n",
    "For other tags, the children will be another subtree. His root will be the category (for example, \"LOCATION\"), and his children will be all the words within the LOCATION tag.\n",
    "\n",
    "This representation is equivalent to a list, and we can easily convert between the two. It does have one advantage over lists - the tree structure \"forces\" legal tag sequences by its nature, and it's impossible to represent an illegal sequence using a tree. Nevertheless, for our questions we will use the list form, for several reasons:\n",
    "\n",
    "- The chunked_sents represnatation doesn't directly include the word tags we need.\n",
    "\n",
    "- Scikit provides a much wider toolkit for lists rather than trees, including the models we were asked to use (such as DictVectorizer).\n",
    "\n",
    "- Working with a list is much more simpler and convinient.\n",
    "\n",
    "We'll refer to the question of illegal sequences later on.\n",
    "\n",
    "So here is our lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sents = list(conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(conll2002.iob_sents('esp.testa'))\n",
    "d_train_sents = list(conll2002.iob_sents('ned.train'))\n",
    "d_test_sents = list(conll2002.iob_sents('ned.testa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand how our dataset is represented. For example, let's look at an example of the first element (and hence, first sentence) in the first dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Melbourne', 'NP', 'B-LOC'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('Australia', 'NP', 'B-LOC'),\n",
       " (')', 'Fpt', 'O'),\n",
       " (',', 'Fc', 'O'),\n",
       " ('25', 'Z', 'O'),\n",
       " ('may', 'NC', 'O'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('EFE', 'NC', 'B-ORG'),\n",
       " (')', 'Fpt', 'O'),\n",
       " ('.', 'Fp', 'O')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see:\n",
    "\n",
    "- A word is represented by a tuple of three elements: the word itself, it's POS (Part of Speech) tagging, and it's correct NER tagging.\n",
    "- A sentence is represented by a list of words\n",
    "- Each element in the dataset is a sentence.\n",
    "\n",
    "Now that we understand our data, it's time to extract features. We'll be looking for word level features, and in this step we'll be looking at each word separately. Those are the features we've chosen to extract for every word:\n",
    "\n",
    "Form (The actual word), POS tagging, is number, does it contain a number, does it begin with a capital letter, is it all capital letters, is it a punctuation char, the first one, two and three letters of the word, the last one, two and three letters in of the word. \n",
    "Here's the code for feature extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hasNumbers(str):\n",
    "    return any(c.isdigit() for c in str)\n",
    "\n",
    "def get_word_features (word):\n",
    "    w = word[0]\n",
    "    features = {\n",
    "     \"form\": w,\n",
    "     \"pos\": word[1],\n",
    "     \"is_number\": w.isdigit(),\n",
    "     \"contains_number\": hasNumbers(w),\n",
    "     \"beginCapital\": w[0].isupper(),\n",
    "     \"allCaps\": w.isupper(),\n",
    "     \"isPunc\": w in string.punctuation,\n",
    "     \"firstLetter\": w[0],\n",
    "     \"first2Letters\": w[0:2],\n",
    "     \"first3Letters\": w[0:3],\n",
    "     \"lastLetter\": w[-1],\n",
    "     \"last2Letters\": w[-2:],\n",
    "     \"last3Letters\": w[-3:]\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's an example on the word 'Melbourne':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'allCaps': False,\n",
       " 'beginCapital': True,\n",
       " 'contains_number': False,\n",
       " 'first2Letters': 'Me',\n",
       " 'first3Letters': 'Mel',\n",
       " 'firstLetter': 'M',\n",
       " 'form': 'Melbourne',\n",
       " 'isPunc': False,\n",
       " 'is_number': False,\n",
       " 'last2Letters': 'ne',\n",
       " 'last3Letters': 'rne',\n",
       " 'lastLetter': 'e',\n",
       " 'pos': 'NP'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_features(train_sents[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have our features, it's time to train our model. We will be using Scikit's DictVectorizer data structure to keep our data, and it's logistic regression implementation for the training. Those methods requires two seperate lists of identical size, where every element represents one word in the corpus. The first list (X) keeps the list of features for every word, and the second list (y) has the NER tagging of the word - which is the answer our model will be trying to guess.\n",
    "\n",
    "Here is the code that creates those lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_corpus_features (corpus):\n",
    "#gets a corpus, returns a list of features for every word\n",
    "    X = []\n",
    "    for sent in corpus:\n",
    "        X += [get_word_features(w) for w in sent]\n",
    "    return X\n",
    "\n",
    "def get_y (corpus):\n",
    "    y = []\n",
    "    for sent in corpus:\n",
    "        y += [w[2] for w in sent]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in addition to the obvious effect of getting features/NER taggings, those functions also transform our data from a list of **sentences** to a list of **words** - so our output is a big list of all the words, not divided to sentences anymore.\n",
    "\n",
    "Now we'll create our DictVectorizer and train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_sents, v, features):\n",
    "    y = get_y(train_sents)\n",
    "    X = v.fit_transform(features)\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X, y)\n",
    "    return clf\n",
    "\n",
    "v = DictVectorizer(sparse=True)\n",
    "features = get_corpus_features(train_sents)\n",
    "clf = train(train_sents, v, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **DictVecotizer** structure and it's fit_transform method, transforms the features list to a matrix. Each row in the matrix represents a word, and each column represents a feature. Each cell M(i,j) includes the corresponding numerical value for feature j in word i.\n",
    "\n",
    "But what if the features are non-numerical? Boolean data easily transforms to 1 (True) or 0 (False), but strings are a bit more complicated to encode. DictVectorizer's solution is to create a separate boolean feature for every string it encounters. For example, for the word \"Melbourne\", rather than the \"firstLetter\" feature, it will create a new boolean feature: \"firstLetter=M\", that will be True for every word that begins in M, and False otherwise. This is similar to a One-Hot Encoding vector.\n",
    "\n",
    "This is a good solution, but it means that for each possible value of a string feature (for example, each word, prefix or suffix of length 1,2,3), a specific feature will be created. In other words, our matrix is getting really big. For the first sentence alone (11 words), 75 features are created. For the entire dataset (264715 words), the matrix will be huge - in fact, too huge for the computer's memory (or at least, *our* computer's memory) to handle.\n",
    "\n",
    "That's why we use a **sparse** version of the matrix. How do this work? The crucial fact is that while we have a lot of data, the huge majority of it (typically above 99%) is zeros. For example, the word \"Melbourne\" will recieve 1 in the \"firstLetter=M\" feature, and 0 in every other \"firstLetter\" feature. The same applies for every single string feature (some of them, such as 3 letter combinations, have a lot of possible values) - that's a lot of zeros.\n",
    "So the sparse data structure take advantage of that fact. Rather than actually save all those zeros, it only keeps the non-zero values and their positions, and assumes zero everywhere else. \n",
    "\n",
    "So now we have our sparse matrix X, and our target values y, it's time to create our classifier (clf) using Scikit's logistic regression implementation. It's time to test this model on the our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict (clf, v, test_features):\n",
    "    X2 = v.transform(test_features)\n",
    "    return clf.predict(X2)\n",
    "\n",
    "test_features = get_corpus_features(test_sents)\n",
    "y_predict = predict(clf, v, test_features)\n",
    "y_true = get_y(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is fairly straight forward, but it's interesting to note the *transform* method used. We create a sparse feature matrix for the test set, but it will not be in the same size as the original matrix for the train data. However, the *predict* method demands that the train and test matrixes will have the same number of features. \n",
    "The transform method transforms the test_features matrix to the same size as the train_features - by adding empty columns for features encountered in the training data, but not the test data. However, this also means there's some information loss - as features that were encountered in the test data but not the train data are \"silently ignored\".\n",
    "\n",
    "So now we have our predictions, it's time to evaluate how well we did. We can do this by several different methods. The easiest of which will be to calculate the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9295391417720084"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy (x,y):\n",
    "    correct = sum([1 if x[i] == y[i] else 0 for i in range(len(x))])\n",
    "    return correct / len(x)\n",
    "\n",
    "accuracy(y_predict, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So almost 93%, not too bad but we will try to do better. We also have more tools to analyze our errors. For a start, we can simply print all the errors and manually look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct = B-LOC    guess = B-MISC   word = K-2                           \n",
      "correct = B-LOC    guess = B-MISC   word = K-2                           \n",
      "correct = B-LOC    guess = B-MISC   word = K-2                           \n",
      "correct = B-LOC    guess = B-MISC   word = K-2                           \n",
      "correct = B-LOC    guess = B-MISC   word = MADRID                        \n",
      "correct = B-LOC    guess = B-MISC   word = Regin                        \n",
      "correct = B-LOC    guess = B-MISC   word = SEVILLA                       \n",
      "correct = B-LOC    guess = B-MISC   word = SEVILLA                       \n",
      "correct = B-LOC    guess = B-MISC   word = Tbet                         \n",
      "correct = B-LOC    guess = B-MISC   word = Yacuiba-Ro                   \n",
      "correct = B-LOC    guess = B-ORG    word = ACEUCHAL                      \n",
      "correct = B-LOC    guess = B-ORG    word = Aceuchal                      \n",
      "correct = B-LOC    guess = B-ORG    word = Auditorio                     \n",
      "correct = B-LOC    guess = B-ORG    word = Austria                       \n",
      "correct = B-LOC    guess = B-ORG    word = Autoridad                     \n",
      "correct = B-LOC    guess = B-ORG    word = Autoridad                     \n",
      "correct = B-LOC    guess = B-ORG    word = BILBAO                        \n",
      "correct = B-LOC    guess = B-ORG    word = Bolivia-Brasil                \n",
      "correct = B-LOC    guess = B-ORG    word = CHILE                         \n",
      "correct = B-LOC    guess = B-ORG    word = Canad                        \n"
     ]
    }
   ],
   "source": [
    "def get_errors (x,y,test_sents):\n",
    "    features = get_corpus_features(test_sents)\n",
    "    errors = []\n",
    "    for i in range(len(x)):\n",
    "        if x[i] != y[i]:\n",
    "            errors.append((y[i], x[i], features[i].get(\"form\")))\n",
    "    return sorted(errors)\n",
    "        \n",
    "errors = get_errors(y_predict, y_true, test_sents)\n",
    "for i in range(20):\n",
    "    print('correct = %-8s guess = %-8s word = %-30s' % (errors[i][0], errors[i][1], errors[i][2]))\n",
    "    \n",
    "#we only print the first 20 errors as examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the confusion matrix:\n",
    "<br>(We'll also print the NER tags in order, because the Scikit confusion matrix does not include labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-LOC', 'I-MISC', 'B-LOC', 'I-ORG', 'B-ORG', 'O', 'B-PER', 'B-MISC', 'I-PER'}\n",
      "[[  779    10    77    30     3     5    30    15    35]\n",
      " [   17   214    42     8     0    32    26    14    92]\n",
      " [  186    45  1201    67     1    12    52    53    83]\n",
      " [  107     5    49   739    20    11    25   233    33]\n",
      " [   24     5     9    30   131     5    37    18    78]\n",
      " [   51    33    34    21    12   126    55    29   293]\n",
      " [  159    53   121    41    18    68   382    78   446]\n",
      " [   83     5    36   142    12    12    25   477    67]\n",
      " [    7    35    72    17     1    35    35     9 45145]]\n"
     ]
    }
   ],
   "source": [
    "print(set(y_true))\n",
    "print(metrics.confusion_matrix(y_true, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit also offers an handy tool called a classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.55      0.79      0.65       984\n",
      "     B-MISC       0.53      0.48      0.50       445\n",
      "      B-ORG       0.73      0.71      0.72      1700\n",
      "      B-PER       0.67      0.60      0.64      1222\n",
      "      I-LOC       0.66      0.39      0.49       337\n",
      "     I-MISC       0.41      0.19      0.26       654\n",
      "      I-ORG       0.57      0.28      0.38      1366\n",
      "      I-PER       0.52      0.56      0.53       859\n",
      "          O       0.98      1.00      0.99     45356\n",
      "\n",
      "avg / total       0.92      0.93      0.92     52923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we are doing a very good job with recognizing O's, but we struggle with other tags. In general, B tags seem to have better score than I tags, while MISC category seems to be the most difficult to classify. It also means that our 93% accuracy score is misleading - the score is relatively high because we are good in recognizing O's (which is the majority of the dataset), but when it comes to the classification of other tags, our accuracy is much lower.\n",
    "\n",
    "Let's check our model on the Dutch dataset as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9508053174834824\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.65      0.60      0.62       479\n",
      "     B-MISC       0.74      0.60      0.66       748\n",
      "      B-ORG       0.72      0.45      0.55       686\n",
      "      B-PER       0.55      0.70      0.62       703\n",
      "      I-LOC       0.37      0.23      0.29        64\n",
      "     I-MISC       0.42      0.23      0.30       215\n",
      "      I-ORG       0.69      0.37      0.48       396\n",
      "      I-PER       0.38      0.45      0.41       423\n",
      "          O       0.98      1.00      0.99     33973\n",
      "\n",
      "avg / total       0.95      0.95      0.95     37687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_features = get_corpus_features(d_train_sents)\n",
    "d_clf = train(d_train_sents, v, d_features)\n",
    "d_test_features = get_corpus_features(d_test_sents)\n",
    "d_y_predict = predict (d_clf, v, d_test_features)\n",
    "d_y_true = get_y (d_test_sents)\n",
    "print (\"accuracy:\", accuracy (d_y_predict,d_y_true))\n",
    "print (metrics.classification_report(d_y_true, d_y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems our Dutch model is doing slightly better than the Spanish one, but it's still prone to similar mistakes - once again we recognize very high success rates among O's (which are the big majority of the dataset), I tags are more difficult to predict than B tags, and \"Misc\" category is still problematic. \n",
    "One noticeable difference is that the Dutch model seems to struggle with Location names (0.29 fscore on I-LOC - even lower than I-MISC), which the Spanish model did better on. A possible explanation is insufficient training data - the Dutch dataset only had 64 I-LOC tags, which is much lower than other tags. \n",
    "The support column refers to the test dataset and not the training dataset, but a check on the training dataset confirms that I-LOC tags are indeed relatively rare.\n",
    "\n",
    "So now, we will try to imrove our overall model by using more than just word-specific features - we'll be looking at features of the previous and following word. Here's our updated *get_word_features2* method, which now receives three arguments (the previous and next word, in addition to the current one) and extracts all features for all 3 words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_features2 (word, prev, next):\n",
    "#also includes information about next and previous word\n",
    "    w = word[0]\n",
    "    p = prev[0]\n",
    "    n = next[0]\n",
    "    features = {\n",
    "     \"form\": w,\n",
    "     \"pos\": word[1],\n",
    "     \"is_number\": w.isdigit(),\n",
    "     \"contains_number\": hasNumbers(w),\n",
    "     \"beginCapital\": w[0].isupper(),\n",
    "     \"allCaps\": w.isupper(),\n",
    "     \"isPunc\": w in string.punctuation,\n",
    "     \"firstLetter\": w[0],\n",
    "     \"first2Letters\": w[0:2],\n",
    "     \"first3Letters\": w[0:3],\n",
    "     \"lastLetter\": w[-1],\n",
    "     \"last2Letters\": w[-2:],\n",
    "     \"last3Letters\": w[-3:],\n",
    "     \"p_form\": p,\n",
    "     \"p_pos\": prev[1],\n",
    "     \"p_is_number\": p.isdigit(),\n",
    "     \"p_contains_number\": hasNumbers(p),\n",
    "     \"p_beginCapital\": p[0].isupper(),\n",
    "     \"p_allCaps\": p.isupper(),\n",
    "     \"p_isPunc\": p in string.punctuation,\n",
    "     \"p_firstLetter\": p[0],\n",
    "     \"p_first2Letters\": p[0:2],\n",
    "     \"p_first3Letters\": p[0:3],\n",
    "     \"p_lastLetter\": p[-1],\n",
    "     \"p_last2Letters\": p[-2:],\n",
    "     \"p_last3Letters\": p[-3:],\n",
    "     \"n_form\": n,\n",
    "     \"n_pos\": next[1],\n",
    "     \"n_is_number\": n.isdigit(),\n",
    "     \"n_contains_number\": hasNumbers(n),\n",
    "     \"n_beginCapital\": n[0].isupper(),\n",
    "     \"n_allCaps\": n.isupper(),\n",
    "     \"n_ispunc\": n in string.punctuation,\n",
    "     \"n_firstLetter\": n[0],\n",
    "     \"n_first2Letters\": n[0:2],\n",
    "     \"n_first3Letters\": n[0:3],\n",
    "     \"n_lastLetter\": n[-1],\n",
    "     \"n_last2Letters\": n[-2:],\n",
    "     \"n_last3Letters\": n[-3:]\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def get_corpus_features2 (corpus):\n",
    "#gets a corpus, returns a list of features for every word\n",
    "    flat = [w for sent in corpus for w in sent]\n",
    "    pad = [(\"*\",\"*\",\"*\")]\n",
    "    flat = pad + flat + pad\n",
    "    X = []\n",
    "    for i in range(1, len(flat) - 1):\n",
    "        X.append(get_word_features2(flat[i], flat[i - 1], flat[i + 1]))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now let's train on the new data, and try to predict and check our new accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9528560361279594\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.59      0.79      0.68       984\n",
      "     B-MISC       0.53      0.50      0.52       445\n",
      "      B-ORG       0.80      0.74      0.77      1700\n",
      "      B-PER       0.86      0.78      0.82      1222\n",
      "      I-LOC       0.65      0.62      0.63       337\n",
      "     I-MISC       0.57      0.41      0.48       654\n",
      "      I-ORG       0.72      0.60      0.66      1366\n",
      "      I-PER       0.85      0.88      0.86       859\n",
      "          O       0.99      1.00      0.99     45356\n",
      "\n",
      "avg / total       0.95      0.95      0.95     52923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features2 = get_corpus_features2(train_sents)\n",
    "clf2 = train(train_sents, v, features2)\n",
    "test_features2 = get_corpus_features2(test_sents)\n",
    "y_predict2 = predict (clf2, v, test_features2)\n",
    "print(\"accuracy:\", accuracy (y_predict2, y_true))\n",
    "print(metrics.classification_report(y_true, y_predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the new score is around 95% - an improvement on the previous attempt. Let's check on the Dutch dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.968742537214424\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.77      0.78      0.77       479\n",
      "     B-MISC       0.79      0.71      0.75       748\n",
      "      B-ORG       0.83      0.62      0.71       686\n",
      "      B-PER       0.68      0.82      0.74       703\n",
      "      I-LOC       0.68      0.30      0.41        64\n",
      "     I-MISC       0.56      0.44      0.49       215\n",
      "      I-ORG       0.80      0.53      0.64       396\n",
      "      I-PER       0.74      0.89      0.81       423\n",
      "          O       0.99      1.00      0.99     33973\n",
      "\n",
      "avg / total       0.97      0.97      0.97     37687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_features2 = get_corpus_features2(d_train_sents)\n",
    "d_clf2 = train(d_train_sents, v, d_features2)\n",
    "d_test_features2 = get_corpus_features2(d_test_sents)\n",
    "d_y_predict2 = predict (d_clf2, v, d_test_features2)\n",
    "print(\"accuracy:\", accuracy (d_y_predict2, d_y_true))\n",
    "print(metrics.classification_report(d_y_true, d_y_predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a similar improvement in the Dutch model as well - who is now up to almost 97% accuracy score. We see some improvement in all categories, including the problematic ones. I-LOC for example is up from 0.29 to 0.41, which is an improvement but still not a good score by any means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.1.3 - Finding Illegal Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the tagging method we used was greedy tagging. We did not check the logic of the tagging, and in particular we didn't check if tag sequences were legal or not. Let's write a function that will find all illegal tag sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish: {'O-IX': 210, 'IX-IY': 232, 'BX-IY': 204}\n",
      "Dutch: {'O-IX': 51, 'IX-IY': 47, 'BX-IY': 127}\n"
     ]
    }
   ],
   "source": [
    "def find_illegal_sequences (guess):\n",
    "    OIX, IXIY, BXIY = 0, 0, 0\n",
    "    for i in range(len(guess) - 1):\n",
    "        curr, next = guess[i], guess[i + 1]\n",
    "        if curr[0] == \"O\" and next[0] == \"I\":\n",
    "            OIX += 1\n",
    "        elif curr[0] == \"I\" and next[0] == \"I\" and curr[1:] != next[1:]:\n",
    "            IXIY+=1\n",
    "        elif curr[0] == \"B\" and next[0] ==\"I\" and curr[1:] != next[1:]:\n",
    "            BXIY += 1\n",
    "    return {\"O-IX\": OIX, \"IX-IY\": IXIY, \"BX-IY\": BXIY}\n",
    "\n",
    "print(\"Spanish:\", find_illegal_sequences(y_predict2))\n",
    "print(\"Dutch:\", find_illegal_sequences(d_y_predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Spanish dataset, all three illegal sequences seem to happen in similar frequency. Our test set includes 52923 words (and hence 52922 sequences of 2), and overall we guessed 646 illegal sequences - that's about 1.2% of our guesses. \n",
    "In the Dutch dataset, illegal sequences are rarer, and only take about 0.5% of our guesses (which is consistent with our results so far, in which our model did better in Dutch than Spanish).\n",
    "\n",
    "How can we use this information?\n",
    "<br>If we were to change our predicting model to prevent it from predicting illegal sequences, we could theoretically improve our accuracy by **up to** 1.2%. Of course, this will be a difficult process. Even if we know that a sequence of 2 tags is illegal, we still face two problems:\n",
    "\n",
    "- We need to determine which of the two tags is wrong (or maybe both are wrong)\n",
    "- We need to correct it to the right answer.\n",
    "\n",
    "If we were to implement such an algorithm, a possible method of doing so would be to use the *predcit_proba* method, which for every word, returns the probability of each tag (a distribution). Using this method, a possible rough algorithm would be:\n",
    "\n",
    "- Predict normally\n",
    "\n",
    "- Look for illegal sequences - similar to *find_illegal_sequences* above, but rather than just counting, for every illegal sequence we find we will perform the following steps:\n",
    "\n",
    "- Determine which of the two tags is more likely to be wrong. We can do it by getting max probability in each word's distribution, and choosing the lower value of the two. \n",
    "For example: let w1,w2 be two words, and t1,t2 the tags with the highest probability. If p(t1)>p(t2), we will conclude that t2 is more likely to be wrong.\n",
    "\n",
    "- Look for the second highest probability in w2's distribution - let's call it t2'. \n",
    "\n",
    "- Check if the sequence (t1, t2') is a legal sequence. If it is, change w2 tag to t2' and move to the next sequence.\n",
    "\n",
    "- If it's illegal, we'll try the next most likely tag - it might be the next option on w2's distribution, or the second option in w1's distribution. \n",
    "\n",
    "- Repeat until you get a legal sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we would like to further improve our results, by using word embeddings. Word embeddings are the results of mapping from words to numerical vectors, based on words context. \n",
    "The idea behind them is simple: words that often occur together in similar contexts, are likely to have similiar meanings. We can assume that words like 'cat' and 'dog' will frequently appear in similar context, while 'cat' and 'tractor' are less likely to. So we go through a lot of text, and for every word in our vocabulary, we collect data of all the words it appears frequently with. After collecting our data, we can perform various mathematical manipulations to turn this data into a vector for every word, in such a way that words that frequently appear together will have similar vectors. We will use those vector to try and improve our NER tagging.\n",
    "\n",
    "So first, let's import our word embeddings database (in Spanish):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordvectors_file_vec = 'data\\wiki.es.vec'\n",
    "wordvecs = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a lot of cool things with this, such as finding the most similiar words to a given word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reina', 0.9727405905723572),\n",
       " ('consorte', 0.9092537760734558),\n",
       " ('infanta', 0.8831985592842102),\n",
       " ('princesa', 0.8758658766746521),\n",
       " ('consortes', 0.8660687208175659),\n",
       " ('berenguela', 0.8640629053115845),\n",
       " ('esposa', 0.862601637840271),\n",
       " ('hermanastra', 0.8556246161460876),\n",
       " ('regente', 0.8431113958358765),\n",
       " ('hija', 0.8396967053413391)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvecs.most_similar_cosmul(positive=['rey','mujer'],negative=['hombre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the one feature that interests us is the wordvecs feature, which returns a numerical vector of size 300 that represents a word. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.012764 , -0.027881 , -0.12001  ,  0.20901  ,  0.12917  ,\n",
       "        0.11149  ,  0.051667 , -0.019114 ,  0.25278  , -0.27741  ,\n",
       "        0.23895  ,  0.31481  , -0.1641   , -0.39813  , -0.055049 ,\n",
       "        0.061539 , -0.21919  , -0.15581  ,  0.049749 , -0.1363   ,\n",
       "        0.13232  ,  0.1721   , -0.14481  , -0.57531  , -0.22965  ,\n",
       "       -0.19064  ,  0.018659 ,  0.11678  ,  0.12292  ,  0.16606  ,\n",
       "       -0.22923  , -0.082918 ,  0.048586 , -0.3828   ,  0.42561  ,\n",
       "        0.20425  , -0.55707  ,  0.2868   , -0.11763  ,  0.065075 ,\n",
       "       -0.053827 ,  0.29591  , -0.11956  ,  0.27796  , -0.10618  ,\n",
       "       -0.31602  , -0.0011778,  0.15995  , -0.10007  ,  0.20116  ,\n",
       "        0.098252 , -0.34145  , -0.20297  ,  0.0050985, -0.1641   ,\n",
       "       -0.17609  ,  0.021475 , -0.10934  ,  0.036395 , -0.085886 ,\n",
       "       -0.011623 , -0.12768  ,  0.2338   , -0.082441 ,  0.47398  ,\n",
       "       -0.34687  ,  0.13387  , -0.15126  ,  0.11719  , -0.011416 ,\n",
       "       -0.13186  , -0.12157  , -0.43338  , -0.13693  , -0.26417  ,\n",
       "       -0.27996  ,  0.05085  , -0.32172  ,  0.11932  , -0.038279 ,\n",
       "       -0.16833  , -0.051152 ,  0.26825  ,  0.108    , -0.049512 ,\n",
       "       -0.08124  ,  0.11898  ,  0.24841  ,  0.25019  , -0.182    ,\n",
       "        0.16514  ,  0.25242  , -0.18547  , -0.029567 ,  0.016944 ,\n",
       "        0.32061  , -0.49323  , -0.12459  , -0.20812  , -0.088195 ,\n",
       "       -0.21952  , -0.16484  ,  0.057854 , -0.081644 , -0.087067 ,\n",
       "       -0.26222  , -0.013701 ,  0.11328  ,  0.1104   , -0.14473  ,\n",
       "       -0.015439 ,  0.012801 , -0.035237 , -0.090021 , -0.28895  ,\n",
       "        0.35738  , -0.02834  ,  0.065393 , -0.0073692, -0.094336 ,\n",
       "       -0.034635 ,  0.068267 , -0.15868  ,  0.0773   , -0.26494  ,\n",
       "        0.087048 ,  0.017102 ,  0.42795  ,  0.19467  , -0.049432 ,\n",
       "        0.041269 , -0.10648  , -0.10184  ,  0.21704  ,  0.23329  ,\n",
       "        0.027093 , -0.11359  , -0.19198  , -0.25129  ,  0.3256   ,\n",
       "       -0.31707  ,  0.31196  , -0.19876  ,  0.12515  ,  0.29853  ,\n",
       "        0.16416  ,  0.3245   ,  0.17049  ,  0.56965  ,  0.056343 ,\n",
       "        0.060293 ,  0.14712  ,  0.25813  ,  0.14682  , -0.16098  ,\n",
       "        0.17777  , -0.25111  , -0.043135 ,  0.098832 ,  0.045896 ,\n",
       "       -0.082729 ,  0.1662   ,  0.16275  , -0.23233  ,  0.23666  ,\n",
       "       -0.33059  , -0.36751  , -0.15504  , -0.066513 ,  0.13993  ,\n",
       "        0.13398  ,  0.51255  , -0.12053  , -0.24772  , -0.33333  ,\n",
       "       -0.3356   ,  0.0032378,  0.032853 , -0.021507 ,  0.014923 ,\n",
       "        0.35816  ,  0.16087  ,  0.18532  ,  0.031704 , -0.097824 ,\n",
       "       -0.063434 ,  0.17677  , -0.095959 , -0.01966  , -0.079799 ,\n",
       "        0.036589 ,  0.25665  ,  0.012553 , -0.20274  ,  0.31322  ,\n",
       "        0.12857  ,  0.39675  ,  0.1146   ,  0.082355 ,  0.12217  ,\n",
       "        0.28058  ,  0.034219 ,  0.39198  ,  0.089673 , -0.05012  ,\n",
       "        0.024749 ,  0.21839  , -0.29923  , -0.045059 , -0.0035621,\n",
       "       -0.026522 ,  0.13805  , -0.021702 ,  0.14808  , -0.13776  ,\n",
       "       -0.12525  ,  0.15337  ,  0.12509  , -0.10263  , -0.078571 ,\n",
       "       -0.50205  , -0.37766  ,  0.083631 , -0.088833 ,  0.05792  ,\n",
       "        0.034306 ,  0.085338 , -0.033502 , -0.053128 , -0.1136   ,\n",
       "        0.24891  , -0.30433  ,  0.2003   , -0.11869  , -0.13636  ,\n",
       "       -0.33804  ,  0.38657  ,  0.31885  , -0.09698  ,  0.17563  ,\n",
       "       -0.1541   , -0.22207  ,  0.13369  ,  0.15756  ,  0.084866 ,\n",
       "        0.066391 ,  0.24077  ,  0.08359  , -0.081925 ,  0.17856  ,\n",
       "       -0.11945  ,  0.13794  , -0.099836 ,  0.10411  , -0.21885  ,\n",
       "        0.12778  ,  0.17918  , -0.19547  ,  0.23722  , -0.1627   ,\n",
       "       -0.0773   ,  0.21912  , -0.11043  ,  0.10935  ,  0.13387  ,\n",
       "        0.3627   , -0.24971  ,  0.31158  ,  0.047827 , -0.21206  ,\n",
       "        0.13089  ,  0.1263   , -0.10432  ,  0.10302  , -0.086497 ,\n",
       "       -0.020981 , -0.019517 ,  0.0091034, -0.091594 ,  0.42599  ,\n",
       "       -0.28656  , -0.12951  ,  0.047451 , -0.080151 , -0.13998  ,\n",
       "        0.14481  , -0.27666  ,  0.13893  ,  0.10985  , -0.1539   ,\n",
       "        0.023797 ,  0.1153   ,  0.13895  , -0.071689 , -0.4254   ,\n",
       "       -0.17155  , -0.080621 , -0.071226 ,  0.094614 , -0.016933 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvecs.word_vec(\"hombre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it's time to extract our features. For each word we will have 900 features - it's vector, the vector of the previous and the next word. Note: this code returns only the word embeddings, and not the features we've used in the previous questions.\n",
    "\n",
    "Our first naive implementation failed, because we ran into a new problem: some of the words in our training sents were simply missing in the word embeddings dataset, which has caused it to throw an exception. We have to decide, what do we do in that case? \n",
    "Our first idea was to give it a set value, such as a vector of zeros. But we've decided against it, because we feared to create a wrong similarity between all the missing words (they would all have the exact same word embedding vector, so might be interpeted as the same word). This is why we've decided instead to generate random values for each missing word. It's not a perfect solution, and has some fairly noticeable downsides:\n",
    "\n",
    "1) Accidental similarity might occur (a randomly generated vector might be similar to an unrelated word)\n",
    "<br>2) Several occurences of the same word will be assigned different vectors.\n",
    "\n",
    "But we still think it's preferable to giving the exact same vector to all missing words.\n",
    "So here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_emb_features (word, prev, next, wordvecs):\n",
    "    w = word[0]\n",
    "    p = prev[0]\n",
    "    n = next[0]\n",
    "    try: \n",
    "        wordvec = wordvecs.word_vec(w)\n",
    "    except KeyError:\n",
    "        wordvec = np.random.rand(300) * 2 - 1\n",
    "    try: \n",
    "        p_wordvec = wordvecs.word_vec(p)\n",
    "    except KeyError:\n",
    "        p_wordvec = np.random.rand(300) * 2 - 1\n",
    "    try: \n",
    "        n_wordvec = wordvecs.word_vec(n)\n",
    "    except KeyError:\n",
    "        n_wordvec = np.random.rand(300) * 2 - 1\n",
    "    return np.concatenate((wordvec, p_wordvec, n_wordvec))\n",
    "\n",
    "def emb_corpus_features (corpus, wordvec):\n",
    "#gets a corpus, returns a list of features for every word\n",
    "    flat = [w for sent in corpus for w in sent]\n",
    "    pad = [(\"*\", \"*\", \"*\")]\n",
    "    flat = pad + flat + pad\n",
    "    X = []\n",
    "    for i in range(1, len(flat) - 1):\n",
    "        X.append(word_emb_features(flat[i], flat[i - 1], flat[i + 1], wordvec))\n",
    "    return X\n",
    "\n",
    "emb_features = emb_corpus_features(train_sents, wordvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have our features, and the next step in our workflow is running them into the DictVectorizer. But here we ran into a technical problem: <br>the DictVectorizer (as his name implies...) is expecting to recieve a dictionary, while our word embeddings are in the form of a list of vectors.\n",
    "\n",
    "Of course, converting a vector into a dictionary is logically very easy. But when we tried it, we've ran into serious performance issues. Our dataset is huge - 264715 words with 900 features for each - and converting this into a dictionary simply takes a lot of time. Also logically, it's pretty silly to convert our matrix into a dictionary, only for the DictVectorizer to convert it back into a matrix.\n",
    "\n",
    "So instead we asked ourselves - why did we need the DictVectorizer in the first place?\n",
    "\n",
    "Well, the DictVectorizer has solved 3 problems for us:\n",
    "\n",
    "1) Convert our original features, who were in the form of a dictionary, into a matrix\n",
    "<br>2) Convert string features into numerical one-hot encodings\n",
    "<br>3) Created a sparse matrix for better performance and taking less memory\n",
    "\n",
    "Interestingly enough, all 3 problems aren't relevant anymore!\n",
    "\n",
    "1) Our data is already in the form of vectors list, with all vectors in the same size - which is pretty much identical to a matrix\n",
    "<br>2) Word embeddings are numerical features, so such conversion isn't necessary\n",
    "<br>3) As we recall, sparse matrices were only effective because most of our data was made of zeros. However, unlike one-hot vectors who are mostly zeros by definition, word embeddings are made of many values. Zero is one of those possible values, but it does not carry any special meaning and does not appear more frequently than others.\n",
    "\n",
    "Common confusion about point 3: earlier we said we needed sparse matrix for performance reasons - because my computer simply couldn't handle the non-sparse matrix. In a quick look, our situation is now much worse - instead of using a 13 (or 39) features, we now have 900 of them! But remember point 2 - because word embeddings are numerical features, conversion into one-hot encoding is unneccasary. So our new matrix is actually much **smaller** - it's size is 264715 X 900, as opposed to the one in the previous question - which size was a fairly insane 264715 X 115042!\n",
    "\n",
    "So basically, our DictVectorizer is now redundant! We'll just skip it and send our data straight to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = get_y(train_sents)\n",
    "emb_clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(emb_features, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now it's time to evaluate our new exciting method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8685637624473291\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.33      0.27      0.29       984\n",
      "     B-MISC       0.12      0.04      0.07       445\n",
      "      B-ORG       0.45      0.44      0.45      1700\n",
      "      B-PER       0.43      0.31      0.36      1222\n",
      "      I-LOC       0.06      0.01      0.02       337\n",
      "     I-MISC       0.08      0.02      0.03       654\n",
      "      I-ORG       0.25      0.09      0.13      1366\n",
      "      I-PER       0.33      0.31      0.32       859\n",
      "          O       0.92      0.97      0.95     45356\n",
      "\n",
      "avg / total       0.83      0.87      0.85     52923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emb_test_features = emb_corpus_features(test_sents, wordvecs)\n",
    "emb_y_predict = emb_clf.predict(emb_test_features)\n",
    "print(\"accuracy:\", accuracy(emb_y_predict, y_true))\n",
    "print(metrics.classification_report(y_true, emb_y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is, well, fairly disappointing. How can it be that we've used much more features, who are more meaningful, and still got much worse results? Something has to be wrong here...\n",
    "Well, upon serious examination we've found the problem. Apparently we were a bit too hasty with our treatment of missing words in the WE dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.132009897436866"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_missing_percentage (corpus, wordvec):\n",
    "    count = 0\n",
    "    flat = [w for sent in corpus for w in sent]\n",
    "    for w in flat:\n",
    "        w2 = w[0] #get the word itself\n",
    "        try: \n",
    "            wordvec.word_vec(w2)\n",
    "        except KeyError:\n",
    "            count += 1\n",
    "    return (count / len(flat)) * 100\n",
    "\n",
    "get_missing_percentage(train_sents, wordvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that a whooping 17% of our dataset is missing word embeddings! Which means that 17% of our predictions were based on random, meaningless numbers. Given that information, a 86% accuracy is actually fairly impressive.\n",
    "\n",
    "Of course, the actual situation is a bit more complicated. Because we're also looking at previous or next words, it's possible that for an unknown word, we'll still have correct information on previous and next words, hopefully allowing us to guess by context. This is probably how we could achieve 86% accuracy despite only having word embeddings for 83% of our dataset. \n",
    "But the point remains - a large bit of our training data is just wrong!\n",
    "\n",
    "So it seems that we have no choice but to use both base features and word embeddings. If we combine both, for every word without word embeddings, we will at least have our basic features. For that, we'll use the FeatureUnion that we've met in part 2. Since we'll now use base features (so we won't have a word with entirely random values), we'll set words without word embeddings to a vector of zeros:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_emb_features_zeros (word, prev, next, wordvecs):\n",
    "    w = word[0]\n",
    "    p = prev[0]\n",
    "    n = next[0]\n",
    "    try: \n",
    "        wordvec = wordvecs.word_vec(w)\n",
    "    except KeyError:\n",
    "        wordvec = np.zeros(300)\n",
    "    try: \n",
    "        p_wordvec = wordvecs.word_vec(p)\n",
    "    except KeyError:\n",
    "        p_wordvec = np.zeros(300)\n",
    "    try: \n",
    "        n_wordvec = wordvecs.word_vec(n)\n",
    "    except KeyError:\n",
    "        n_wordvec = np.zeros(300)\n",
    "    return np.concatenate((wordvec, p_wordvec, n_wordvec))\n",
    "\n",
    "def emb_corpus_features_zeros (corpus):\n",
    "#gets a corpus, returns a list of features for every word\n",
    "    wordvectors_file_vec = 'data\\wiki.es.vec'\n",
    "    wordvec = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=100000)\n",
    "    flat = [w for sent in corpus for w in sent]\n",
    "    pad = [(\"*\",\"*\",\"*\")]\n",
    "    flat = pad + flat + pad\n",
    "    X=[]\n",
    "    for i in range(1, len(flat)-1):\n",
    "        X.append(word_emb_features_zeros(flat[i], flat[i-1], flat[i+1], wordvec))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the relevant transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GetNormalFeatures(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return get_corpus_features2(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self \n",
    "\n",
    "class WordEmbeddings(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return emb_corpus_features_zeros(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the entire pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "                (\"normal\", Pipeline([(\"get\", GetNormalFeatures()), (\"dictVec\", DictVectorizer(sparse=True))])), \n",
    "                (\"embeddings\", WordEmbeddings())])),\n",
    "    ('classifier', LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to run it: (this might take a while...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = pipeline.fit(train_sents, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluate the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9519679534417927\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.60      0.78      0.68       984\n",
      "     B-MISC       0.52      0.50      0.51       445\n",
      "      B-ORG       0.79      0.74      0.77      1700\n",
      "      B-PER       0.86      0.77      0.81      1222\n",
      "      I-LOC       0.61      0.61      0.61       337\n",
      "     I-MISC       0.55      0.39      0.46       654\n",
      "      I-ORG       0.73      0.60      0.66      1366\n",
      "      I-PER       0.84      0.87      0.85       859\n",
      "          O       0.99      1.00      0.99     45356\n",
      "\n",
      "avg / total       0.95      0.95      0.95     52923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline.predict(test_sents)\n",
    "print(\"accuracy:\", accuracy (predictions, y_true))\n",
    "print(metrics.classification_report(y_true, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than the 86% WEs achieved on their own, but still not an imporvement on the regular features. In order to find out why, let's investigate the words that don't have WEs. Instead of showing the general percent of missing words, we'll calculate the missing percent for every classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-MISC: word embeddings percent is 7.086976530142664%\n",
      "I-ORG: word embeddings percent is 31.29006410256411%\n",
      "B-ORG: word embeddings percent is 1.0960757780784869%\n",
      "I-PER: word embeddings percent is 6.021009479887263%\n",
      "I-LOC: word embeddings percent is 31.78212585933369%\n",
      "O: word embeddings percent is 92.87556053811659%\n",
      "B-LOC: word embeddings percent is 1.9743537553429724%\n",
      "I-MISC: word embeddings percent is 37.57783312577833%\n",
      "B-PER: word embeddings percent is 0.6942837306179173%\n"
     ]
    }
   ],
   "source": [
    "def collect_missing_words (corpus, wordvec):\n",
    "    missing = []\n",
    "    flat = [w for sent in corpus for w in sent]\n",
    "    for w in flat:\n",
    "        w2 = w[0] #get the word itself\n",
    "        try: \n",
    "            wordvec.word_vec(w2)\n",
    "        except KeyError:\n",
    "            missing.append(w)\n",
    "    return missing\n",
    "\n",
    "def percentage_per_class (c1, c2):\n",
    "#recieve 2 counters: c1 is a subset of c2. \n",
    "#Prints for each class, the percent of words in c2 that are missing in c1.\n",
    "    tags = set(c1.elements())\n",
    "    for tag in tags:\n",
    "        print(\"{}: word embeddings percent is {}%\".format(tag, (1-c1[tag]/c2[tag])*100))\n",
    "\n",
    "missing_words = collect_missing_words(train_sents, wordvecs)\n",
    "missing_words_tag = get_y([missing_words]) \n",
    "\n",
    "percentage_per_class(Counter(missing_words_tag), Counter(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results clearly reveal our problem. Class O have a decent percentage of word embeddings (almost 93%) - but we already had great results with that class before. But the other classes, which we struggled with, have very poor word embeddings percentage - sometimes even around one percent! \n",
    "\n",
    "Logically, this makes sense - O represents \"regular\" words in the language (common nouns, verbs etc), while the rest of the values refer to names (of people, locations, organizations, etc). Names are by nature a lot less common and more specific, so it's hardly surprising that names like \"Melbourne\" are more likely to be missing in the WE dataset, than regular Spanish words.\n",
    "\n",
    "For some classes, our WEs got even worse results than regular features. This makes sense - if the word doesn't have WE, we've basically introduced hundreds of meaningless features that confuse the algorithm. Also, we should note that I-classes (I-LOC, I-PER etc) will always come ofter B-classes. Since both categories have poor word embeddings percentage, it's likely that **both** the previous and the current words will not have WEs - which already amounts to 600 meaningless features.\n",
    "\n",
    "So we've learned that Word Embeddings are a powerful tool. We've managed to reach 86% accuracy using WEs on their own, despite only having WE for 83% of our dataset - that illustrates their potential. However, for this specific task, we found our WE dataset to be lacking - our task was identifying \"rare\" words such as locations and people names, and the majority of those words did not have WEs. So if we were to keep pursuing this task, our next logical step would be to look for a better WE dataset - or create one ourselves."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
