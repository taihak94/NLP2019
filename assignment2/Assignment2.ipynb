{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the solution to HW2, written by Yaniv Bin and Tair Hakman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first would like to import all the required modules in order for our code to run properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import os.path\n",
    "from os.path import abspath, dirname, join\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import random\n",
    "import gzip, string, nltk\n",
    "import tarfile\n",
    "import itertools\n",
    "from glob import glob\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from nltk.corpus import conll2002\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.externals.six.moves import html_parser\n",
    "from sklearn.externals.six.moves.urllib.request import urlretrieve\n",
    "from sklearn.datasets import get_data_home\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first task is identifying complex words. In order to do so we will use the data provided. \n",
    "first we need to load the data using the provided functions. \n",
    "<br>(For a better flow we modified the skeleton code within the notebook, but we added the edit into the skeleton file as well) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_file(data_file):\n",
    "    words = []\n",
    "    labels = []   \n",
    "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i > 0:\n",
    "                line_split = line[:-1].split(\"\\t\")\n",
    "                words.append(line_split[0].lower())\n",
    "                labels.append(int(line_split[1]))\n",
    "            i += 1\n",
    "    return words, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_file = abspath(join(dirname(\"__file__\"), \"data/complex_words_training.txt\"))\n",
    "development_file = abspath(join(dirname(\"__file__\"), \"data/complex_words_development.txt\"))\n",
    "test_file = abspath(join(dirname(\"__file__\"), \"data/complex_words_test_unlabeled.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data we may start working on the actual assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 - Evaluation Matrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually in order to evaluate how well a classifcation algorithm works we use three measures - \n",
    "- **Precision**: Measure how 'useful' the results are (how many are hits and how many are miss)\n",
    "- **Recall**: Measure how 'complete' the results are (out off all the actually possible results, how many did we hit)\n",
    "- **Fscore**: Measure the balance between the Precision and Recall\n",
    "\n",
    "We would like to implement functions that compose each of these matrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by calculating the precision - the precision is defined as:\n",
    "<br>$precision = \\frac{tp}{tp + fp}$\n",
    "<br>Where $tp$ stands for true-positive meaning a hit, and $fp$ stands for false-positive meaning the prediction is positive but it's actually a false alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Input: y_pred, a list of length n with the predicted labels,\n",
    "## y_true, a list of length n with the true labels\n",
    "\n",
    "## Calculates the precision of the predicted labels\n",
    "def get_precision(y_pred, y_true):\n",
    "    positive_hits_array = np.array([(1 if(y_pred[i] == y_true[i] == 1) else 0) for i in range(len(y_pred))])\n",
    "    positive_hits = np.count_nonzero(positive_hits_array == 1)\n",
    "    total_positive = np.count_nonzero(np.array(y_pred) == 1)\n",
    "    precision = float(positive_hits) / float(total_positive)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's make sure it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [0, 0, 1, 1]\n",
    "# Here we have one true positive at position 2, and 1 false positive at position 1\n",
    "assert(get_precision(y_pred, y_true) == (1 / (1 + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second measure we will implement is the recall, calculated as follows\n",
    "<br>$recall = \\frac{tp}{tp + fn}$\n",
    "<br>Where $fn$ stands for false-negative, meaning the prediction is negative also it should have been positive(a miss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Calculates the recall of the predicted labels\n",
    "def get_recall(y_pred, y_true):\n",
    "    positive_hits_array = np.array([(1 if(y_pred[i] == y_true[i] == 1) else 0) for i in range(len(y_pred))])\n",
    "    positive_hits = np.count_nonzero(positive_hits_array == 1)\n",
    "    false_negative_array = np.array([(1 if ((not(y_pred[i] == y_true[i] )) and (y_pred[i] == 0) ) else 0) for i in range(len(y_pred))])\n",
    "    false_negative =  np.count_nonzero(false_negative_array)\n",
    "    recall = float(positive_hits) / float(positive_hits + false_negative)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's again make sure this works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [1, 0, 1, 1]\n",
    "# We have one true positive at position 2, and two false negative at positions 0 and 3\n",
    "assert(get_recall(y_pred, y_true) == (1 / (1 + 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the last measure we want to implement is the fscore(also known as f1) which is calculated as:\n",
    "<br>$F = 2 * \\frac{precision*recall}{precision + recall}$\n",
    "<br> F score measure the relation between the precision and recall, the results vary between 0 and 1, but once it's equal 1 that means the relation is exact meaning the classifier is perfect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Calculates the f-score of the predicted labels\n",
    "def get_fscore(y_pred, y_true):\n",
    "    precision = get_precision(y_pred, y_true)\n",
    "    recall = get_recall(y_pred, y_true)\n",
    "    \n",
    "    # in case we might tr and divide by 0 \n",
    "    if(precision + recall) == 0:\n",
    "        return 0\n",
    "    \n",
    "    fscore = 2 * float(precision * recall) / float(precision + recall)\n",
    "    return fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And making sure it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [1, 0, 1, 1]\n",
    "# precision is 1/2, recall is 1/3\n",
    "assert(get_fscore(y_pred, y_true) == 2 * (((1 / 2) * (1 / 3)) / (1 / 2 + 1 / 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be extra catious and make sure we didn't mess up we also want to compare our results to scikit results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [1, 0, 1, 1]\n",
    "precision = get_precision(y_pred, y_true)\n",
    "recall = get_recall(y_pred, y_true)\n",
    "fscore = get_fscore(y_pred, y_true)\n",
    "\n",
    "sk_precision = metrics.precision_score(y_true, y_pred)\n",
    "sk_recall = metrics.recall_score(y_true, y_pred)\n",
    "sk_fscore = metrics.f1_score(y_true, y_pred)\n",
    "\n",
    "assert(precision == sk_precision)\n",
    "assert(recall == sk_recall)\n",
    "assert(fscore == sk_fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our own good we would also like to implement a function that prints out all the information given the two arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_predictions(y_pred, y_true):\n",
    "    print(\"Precision:\", get_precision(y_pred, y_true))\n",
    "    print(\"Recall:\", get_recall(y_pred, y_true))\n",
    "    print(\"Fscore:\", get_fscore(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5\n",
      "Recall: 0.3333333333333333\n",
      "Fscore: 0.4\n"
     ]
    }
   ],
   "source": [
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [1, 0, 1, 1]\n",
    "test_predictions(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after implementing the functions, we can go on and implement actual classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 - Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2.1 - All complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classifier we will implement is a very simple one that classifies all the words as complex no matter what they actually are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Labels every word complex\n",
    "def all_complex(data_file):\n",
    "    words, actual_labels = load_file(data_file)\n",
    "    all_complex_labels = np.ones((len(words),), dtype=int)\n",
    "    precision = get_precision(all_complex_labels, actual_labels)\n",
    "    recall = get_recall(all_complex_labels, actual_labels)\n",
    "    fscore = get_fscore(all_complex_labels, actual_labels)\n",
    "    performance = [precision, recall, fscore]\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we shall test it with each of the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Precision: 0.43275 \n",
      "Training Recall: 1.0 \n",
      "Training Fscore: 0.604083057058105\n"
     ]
    }
   ],
   "source": [
    "ac_tr_precision, ac_tr_recall, ac_tr_fscore = all_complex(training_file)\n",
    "print(\"Training Precision: {} \\nTraining Recall: {} \\nTraining Fscore: {}\".format(ac_tr_precision, ac_tr_recall, ac_tr_fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Precision: 0.418 \n",
      "Dev Recall: 1.0 \n",
      "Dev Fscore: 0.5895627644569816\n"
     ]
    }
   ],
   "source": [
    "ac_dv_precision, ac_dv_recall, ac_dv_fscore = all_complex(development_file)\n",
    "print(\"Dev Precision: {} \\nDev Recall: {} \\nDev Fscore: {}\".format(ac_dv_precision, ac_dv_recall, ac_dv_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can note how the recall of this model is always 1 - that makes sense because although it probably has a lot of false positives - it never \"miss\" in terms of false negtive because it always has an answer - complex. \n",
    "\n",
    "Another thing to note is that the Fscore is not that high - that's because the precision of this model is not that great, it actually mess up in more than half cases, which makes sense (assuming there aren't that many complex words in the dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2.2 - word length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second baseline we will implement is a word length based one, which gives a positive value to a word if it's length goes past a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1.2.2: Word length thresholding\n",
    "def word_length_baseline(data_file, threshold):\n",
    "    words, actual_labels = load_file(data_file)\n",
    "    threshold_labels = [(1 if(len(word) >= threshold) else 0) for word in words]\n",
    "    \n",
    "    precision = get_precision(threshold_labels, actual_labels)\n",
    "    recall = get_recall(threshold_labels, actual_labels)\n",
    "    fscore = get_fscore(threshold_labels, actual_labels)\n",
    "    preformance = [precision, recall, fscore]\n",
    "    return preformance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to run this for both the training dataset and the development dataset for different threshold values in order to get a \"feel\" about how it behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAAJwCAYAAADWXSa8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd72c7aac88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precisions = np.zeros(15)\n",
    "recall = np.zeros(15)\n",
    "fscores = np.zeros(15)\n",
    "for i in range(15):\n",
    "    tprecision, trecall, tfscore = word_length_baseline(training_file, i)\n",
    "    precisions[i] = tprecision\n",
    "    recall[i] = trecall\n",
    "    fscores[i] = tfscore\n",
    "\n",
    "plt.figure(num=1, figsize=(10, 10))\n",
    "pr_plt = plt.subplot(2 ,1, 1, xlabel=\"recall\", ylabel=\"precision\", label=\"pr\")\n",
    "pr_plt.set_title(\"precision/recall\", y=1.08)\n",
    "pr_plt.plot(recall, precisions)\n",
    "\n",
    "fs_plt = plt.subplot(2 ,1, 2, xlabel=\"threshold\", ylabel=\"fscore\", label=\"fs\")\n",
    "fs_plt.set_title(\"fscore measure\", y=1.01)\n",
    "fs_plt.plot(range(15), fscores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we explore those plots we can see that once the recall is big the precision is small and vice versa, that makes a lot of sense considering precision and recall \n",
    "\n",
    "Another thing we can clearly see here is how the Fscore starts low, goes up, and then it starts deteriorating like a slope - this makes sense as well considering how if the length is really short it should behave like our previous result, and then it should better itself until we reach a certaing length from which words just don't really accure (for example, we don't know many reasonable words with a length greater than 20, so although those are for sure complex, the odds of them showing up in our text are pretty small...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after we're done with our exploration we can go about building the \"ideal\" threshold classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Finds the best length threshold by f-score, and uses this threshold to\n",
    "## classify the training and development set\n",
    "def word_length_threshold(training_file, development_file):\n",
    "    best_tfscore = 0.0\n",
    "    best_i = 1\n",
    "    i = 1\n",
    "    while(True):\n",
    "        tprecision, trecall, tfscore = word_length_baseline(training_file, i)\n",
    "        if(tfscore < best_tfscore):\n",
    "            print(\"best threshold is: \", best_i)\n",
    "            break\n",
    "        else:\n",
    "            best_i = i\n",
    "            i += 1\n",
    "            best_tfscore = tfscore\n",
    "            \n",
    "    tprecision, trecall, tfscore = word_length_baseline(training_file, best_i)\n",
    "    dprecision, drecall, dfscore = word_length_baseline(development_file, best_i)\n",
    "    training_performance = [tprecision, trecall, tfscore]\n",
    "    development_performance = [dprecision, drecall, dfscore]\n",
    "    return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold is:  7\n",
      "Training Precision: 0.6007401315789473 \n",
      "Training Recall: 0.8440207972270364 \n",
      "Training Fscore: 0.7018976699495555\n",
      "\n",
      "Dev Precision: 0.6053511705685619 \n",
      "Dev Recall: 0.8660287081339713 \n",
      "Dev Fscore: 0.7125984251968505\n"
     ]
    }
   ],
   "source": [
    "wl_training_performance, wl_development_performance = word_length_threshold(training_file, development_file)\n",
    "wl_tr_precision, wl_tr_recall, wl_tr_fscore = wl_training_performance\n",
    "wl_dv_precision, wl_dv_recall, wl_dv_fscore = wl_development_performance\n",
    "print(\"Training Precision: {} \\nTraining Recall: {} \\nTraining Fscore: {}\\n\".format(wl_tr_precision, wl_tr_recall, wl_tr_fscore))\n",
    "print(\"Dev Precision: {} \\nDev Recall: {} \\nDev Fscore: {}\".format(wl_dv_precision, wl_dv_recall, wl_dv_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the threshold to Fscore graph it somewhat fits what we would expect - words that are long (about 7 characters) are usually considered complexed therefore the classifier was right. If we set the threshold too low we'd get a lot of false positives, so the precision will be high, but we will get a bad recall, meaning our classifier is very insensitive and doesn't preform well. If we take a high threshold, not many words will pass it (considering what we discussed), so we will get a lot of false negatives(which will give us a bad ). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2.3 - Word Frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to implement a method that classifies based on the frequency of a given word in the language, for that we use Google Ngram Counts - so we first have to load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1.2.3: Word frequency thresholding\n",
    "\n",
    "## Loads Google NGram counts\n",
    "def load_ngram_counts(ngram_counts_file): \n",
    "    counts = defaultdict(int) \n",
    "    with gzip.open(ngram_counts_file, 'rt') as f: \n",
    "        for line in f:\n",
    "            token, count = line.strip().split('\\t') \n",
    "            if token[0].islower(): \n",
    "                counts[token] = int(count) \n",
    "    return counts\n",
    "\n",
    "ngram_path = abspath(join(dirname(\"__file__\"), \"data/ngram_counts.txt.gz\"))\n",
    "ngram_counts = load_ngram_counts(ngram_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading it, we need to explore the ngrams and the text to see what is the range we need for our thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximal ('the', 47376829651) ('of', 30684794816) ('and', 22042266408) ('a', 14166008316) ('was', 5475451740)\n",
      "minimal ('zxxiz', 40) ('zvalking', 40) ('zuwiesen', 40) ('zurite', 40) ('zround', 40)\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter(ngram_counts)\n",
    "most_common = counter.most_common(10)\n",
    "least_common = counter.most_common()[:-10-1:-1]\n",
    "print(\"maximal\", most_common[0], most_common[1], most_common[2], most_common[5], most_common[9])\n",
    "print(\"minimal\", least_common[0], least_common[1], least_common[2], least_common[5], least_common[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this might not be the case in our training document, so let's explore it as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max frequencies 47376829651 751979946 591878281 225568038\n",
      "mean 47031121\n",
      "above mean 828\n"
     ]
    }
   ],
   "source": [
    "words, actual_labels = load_file(training_file)\n",
    "counts = []\n",
    "for i in range(0, len(words)):\n",
    "    counts.append(ngram_counts[words[i]])\n",
    "sorted_co = np.sort(np.array(counts))\n",
    "print(\"max frequencies\", sorted_co[3999], sorted_co[3998], sorted_co[3990], sorted_co[3900])\n",
    "print(\"mean %d\" % np.mean(sorted_co))\n",
    "j = 3999\n",
    "mean = np.mean(sorted_co)\n",
    "while j > 0:\n",
    "    if(sorted_co[j] >= mean):\n",
    "        j -= 1\n",
    "    else:\n",
    "        break\n",
    "print(\"above mean\", 3999 - j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see the maximal frequency is $47376829651$ and the minimal is $0-40$ , but the second best is not $30684794816$  like the word'of' so we're guessing 'of' is not part of the train set, and our dataset is not exactly filled with the most frequent words... Now we can use that info for thresholding for the ngrams, but in our training data it's usually around a much lower results, so we need to set a lower bound so we won't keep getting the same prediction all the time, so we went with an upper bout of 225568038 considering there are 100 other frequencies above it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the code that implements this thresholding - if a word is below the threshold frequency it is labeled complexed, and otherwise simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_frequency_baseline(data_file, counts, threshold):\n",
    "    words, actual_labels = load_file(data_file)\n",
    "    threshold_labels = [counts[word] < threshold for word in words]\n",
    "    precision = get_precision(threshold_labels, actual_labels)\n",
    "    recall = get_recall(threshold_labels, actual_labels)\n",
    "    fscore = get_fscore(threshold_labels, actual_labels)\n",
    "    preformance = [precision, recall, fscore]\n",
    "    return preformance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test this method using different values for the threshold - We used a range of 40 random numbers between 40 to 225568038 (after examining the Google Ngrams Count and our file count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precisions = np.zeros(40)\n",
    "recall = np.zeros(40)\n",
    "fscores = np.zeros(40)\n",
    "thresholds = np.sort(np.random.randint(40, 225568038, size=40, dtype=int))\n",
    "print(\"thresholds:\", thresholds)\n",
    "for i in range(40):\n",
    "    tprecision, trecall, tfscore = word_frequency_baseline(training_file, ngram_counts, thresholds[i])\n",
    "    precisions[i] = tprecision\n",
    "    recall[i] = trecall\n",
    "    fscores[i]=tfscore\n",
    "\n",
    "    \n",
    "plt.figure(num=3, figsize=(10, 10))\n",
    "pr_plt = plt.subplot(2 ,1, 1, xlabel=\"recall\", ylabel=\"precision\", label=\"pr\")\n",
    "pr_plt.set_title(\"precision/recall\", y=1.08)\n",
    "pr_plt.plot(recall, precisions)\n",
    "\n",
    "fs_plt = plt.subplot(2 ,1, 2, xlabel=\"threshold\", ylabel=\"fscore\", label=\"fs\")\n",
    "fs_plt.set_title(\"fscore measure\", y=1.01)\n",
    "fs_plt.plot(thresholds, fscores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see (and it makes sense) that the tresholds can vary the recall and precision - a higher recall will be when the threshold is high, and a lower one when it's low and the exact opposite happens with the precision. We do see once we set the threshold lower we get a better fscore - this might be because of the amount of true positives we'd get that way (increases the amount of true positives), if we set it very low it will just act as all positive(which we've seen earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finds the best frequency threshold by f-score, and uses this threshold to\n",
    "## classify the training and development set\n",
    "def word_frequency_threshold(training_file, development_file, counts):\n",
    "    best_tfscore = 0.0\n",
    "    best_threshold = 0\n",
    "    i = 1\n",
    "    thresholds = np.sort(np.random.randint(40, 225568038, size=30, dtype=int))\n",
    "    print(\"thresholds:\", thresholds)\n",
    "    for threshold in thresholds:\n",
    "        tprecision, trecall, tfscore = word_frequency_baseline(training_file, counts, threshold)\n",
    "        if(tfscore > best_tfscore):\n",
    "            best_tfscore = tfscore\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    print(\"best threshold\", best_threshold)\n",
    "            \n",
    "    tprecision, trecall, tfscore = word_frequency_baseline(training_file, counts, best_threshold)\n",
    "    dprecision, drecall, dfscore = word_frequency_baseline(development_file, counts, best_threshold)\n",
    "    \n",
    "    training_performance = [tprecision, trecall, tfscore]\n",
    "    development_performance = [dprecision, drecall, dfscore]\n",
    "    return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wf_training_performance, wf_development_performance = word_frequency_threshold(training_file, development_file, ngram_counts)\n",
    "wf_tr_precision, wf_tr_recall, wf_tr_fscore = wf_training_performance\n",
    "wf_dv_precision, wf_dv_recall, wf_dv_fscore = wf_development_performance\n",
    "print(\"Training Precision: {} \\nTraining Recall: {} \\nTraining Fscore: {}\\n\".format(wf_tr_precision, wf_tr_recall, wf_tr_fscore))\n",
    "print(\"Dev Precision: {} \\nDev Recall: {} \\nDev Fscore: {}\".format(wf_dv_precision, wf_dv_recall, wf_dv_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this preforms worse than the previous classifier - that might be because although this logic (words that are frequent are usually easier) holds for the real world, it might not hold for our text, we did see the mean of the frequencies in this specific training set is around 47031121 , and there are only 828 words above that mean which is not that many, so there just aren't a lot of very high frequency words in our text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3 - Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.3.1 - Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tried different baselines and saw the results from them, that were not that great, it's time to do actual classification.\n",
    "<br>The method we are going to use is called Naive Bayes Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We personally find that explaining this method via an example makes it a bit easier to grasp instead of just overloading people with probability formulas.\n",
    "<br>We shall use our example on the given problem considering it is most fitting, and afterwards we will implement the actual code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our dataset which is composed of two types of labels - 0 for a simple word, and 1 for a complex word.\n",
    "Now lets assume (for the sake of the example, the actual dataset might act differently but we shall see later) someone told us the following - there is a 70% chance of a word being simple, and a 30% chance that a word is complexed.\n",
    "Another thing we are told is that the mean of the word length of the simple words is 4 and the mean of word length of the complex words is 7.\n",
    "<br>Now we got a new word, and it has a length of 3, what will we guess it's label is? Well, an educated guess will be to say there's a big chance it's a simple word..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>__How did we assume that?__ We saw there is a high chance of being a simple word, and then we also saw the length of the word is 3 and usually shorter words are simple - so by likelihood the chances of our word being simple are pretty high.\n",
    "<br>The _Naive Bayes Classifier_ works pretty much just like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>__How exactly does it work?__\n",
    "<br> First thing to note is that the classifier assumes independence among features, this is not always true in real life (actually, it usually isn't...) but it simplifies the model - in our case it will assume there's no relation between the length of a word and it's frequency (although in reallity it's safe to assume a really long word won't be that frequent). \n",
    "Because of how wrong this assumption is, the classifier got the name _naive_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the classifier does is to find the probability of belonging to a class, given a set of features, in our case we can write it down as $P(simple | f_{length}, f_{freq})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have that we can use the Bayse rule to get this probabilty:\n",
    "<br>$P(simple \\vert f_{length}, f_{freq}) = \\frac{P(simple) * P(f_{length}, f_{freq} \\vert simple)}{P(f_{length}, f_{freq})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know $P(simple)=0.7$ and we don't need $P(f_{length}, f_{freq})$ to build a classifier, so all we are left to do is calculate $P(f_{length}, f_{freq} \\vert simple)$ . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply the conditional probability formula we will get:\n",
    "<br>$P(f_{length}, f_{freq} \\vert simple) = P(f_{length} \\vert simple) * P(f_{freq} \\vert simple, f_{length})$\n",
    "<br><br>Or in the general case (not our example) for $n$ features and a label $l$ we get:\n",
    "<br>$P(f_1, f_2, f_3, \\dots , f_n | l) = P(f_1 \\vert l) * P(f_2 \\vert l, f_1) * \\dots * P(f_n \\vert l, f_1, \\dots, f_{n-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can note that if we had n features, this will require a lot of data because we will have to have data for each of these assumption, but we don't actually need it. Why? well, this is where the _naive_ part comes in handy - we assumed all features are independent, so what we actually get is:\n",
    "<br> $P(f_{length}, f_{freq} | simple) = P(f_{length} \\vert simple) * P(f_{freq} \\vert simple)$\n",
    "<br><br>Or in the general case:\n",
    "<br>$P(f_1, f_2, f_3, \\dots , f_n \\vert l) = \\prod{P(f_i|l)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now finally to classify a new vector of features we have to choose the label for it (simple 0 or complex 1), to do so all we have to do is:\n",
    "<br>\n",
    "$Classifier(f_{l'}, f_{f'}) = \\arg\\max_{s\\in{0, 1}}P(f_{l'}, f_{f'} \\vert s)$ \n",
    "<br><br>Or in the general case for $m$ labels and $n$ features:\n",
    "<br>\n",
    "$Classifier(f_{1},\\dots, f_{n}) = \\arg\\max_{s\\in{0,\\dots,m}}P(f_{1},\\dots, f_{n} \\vert s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__implementation__\n",
    "<br>So now that we get the reasoning behind this method, we can go along and implement the classifier.\n",
    "<br>One last thing to note is that we were asked to use the Guassian based classifier, this classifier assumes the features follow a _normal_ distribution (meaning all the features falls on a normal curve)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code results in the classifier (note the code comments for a step by step explenation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1.3.1: Naive Bayes\n",
    "        \n",
    "## Trains a Naive Bayes classifier using length and frequency features\n",
    "def naive_bayes(training_file, development_file, counts):\n",
    "    #import training dataset\n",
    "    t_words, t_labels = load_file(training_file)\n",
    "    t_features = {}\n",
    "    # get length features\n",
    "    t_features[\"length\"] = np.array([len(word) for word in t_words])\n",
    "    # get frequency features\n",
    "    t_features[\"frequency\"] = np.array([counts[word] for word in t_words])\n",
    "    # build features array\n",
    "    X_t_original = np.array([t_features[\"length\"], t_features[\"frequency\"]]).T\n",
    "    \n",
    "    # normalize features\n",
    "    t_mean = np.mean(X_t_original)\n",
    "    t_sd = np.std(X_t_original)\n",
    "    X_t_scaled = (X_t_original - t_mean)/t_sd\n",
    "    \n",
    "    # train the classifier\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_t_scaled, t_labels)\n",
    "    \n",
    "    # extract features for development file\n",
    "    d_words, d_labels = load_file(development_file)\n",
    "    d_features = {}\n",
    "    # get length features\n",
    "    d_features[\"length\"] = np.array([len(word) for word in d_words])\n",
    "    # get frequency features\n",
    "    d_features[\"frequency\"] = np.array([counts[word] for word in d_words])\n",
    "    # build features array\n",
    "    X_d_original = np.array([d_features[\"length\"], d_features[\"frequency\"]]).T\n",
    "    \n",
    "    # normalize development features - note how we use the training mean and sd\n",
    "    X_d_scaled = (X_d_original - t_mean)/t_sd\n",
    "    \n",
    "    dev_pred = clf.predict(X_d_scaled)\n",
    "    train_pred = clf.predict(X_t_scaled)\n",
    "    \n",
    "    tprecision = get_precision(train_pred, t_labels)\n",
    "    trecall = get_recall(train_pred, t_labels)\n",
    "    tfscore = get_fscore(train_pred, t_labels)\n",
    "    \n",
    "    dprecision = get_precision(dev_pred, d_labels)\n",
    "    drecall = get_recall(dev_pred, d_labels)\n",
    "    dfscore = get_fscore(dev_pred, d_labels)\n",
    "    \n",
    "    training_performance = [tprecision, trecall, tfscore]\n",
    "    development_performance = [dprecision, drecall, dfscore]\n",
    "    return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_training_performance, nb_development_performance = naive_bayes(training_file, development_file, ngram_counts)\n",
    "nb_tr_precision, nb_tr_recall, nb_tr_fscore = nb_training_performance\n",
    "nb_dv_precision, nb_dv_recall, nb_dv_fscore = nb_development_performance\n",
    "print(\"Training Precision: {} \\nTraining Recall: {} \\nTraining Fscore: {}\\n\".format(nb_tr_precision, nb_tr_recall, nb_tr_fscore))\n",
    "print(\"Dev Precision: {} \\nDev Recall: {} \\nDev Fscore: {}\".format(nb_dv_precision, nb_dv_recall, nb_dv_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we look at the fscore measure in each of the methods we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(num=5, figsize=(10, 10))\n",
    "types = [\"all complex\", \"word length\",\"word frequency\", \"naive bayse\"]\n",
    "f_scores_tr = [ac_tr_fscore, wl_tr_fscore, wf_tr_fscore, nb_tr_fscore]\n",
    "f_scores_dv = [ac_dv_fscore, wl_dv_fscore, wf_dv_fscore, nb_dv_fscore]\n",
    "bar_colors = ['b', 'g', 'r', 'c']\n",
    "ax_tr = plt.subplot(211)\n",
    "rectangles_tr = plt.bar(range(len(types)), f_scores_tr, width=0.5,\n",
    "                     color=bar_colors)\n",
    "ax_tr.set_xticks(np.linspace(0, len(types) - 1, len(types)))\n",
    "ax_tr.set_xticklabels(types, fontsize=10)\n",
    "ax_tr.set_ylabel('fscore (s)')\n",
    "ax_tr.set_title('Training set Fscore', y=1.08)\n",
    "\n",
    "ax_dv = plt.subplot(212)\n",
    "rectangles_dv = plt.bar(range(len(types)), f_scores_dv, width=0.5,\n",
    "                     color=bar_colors)\n",
    "ax_dv.set_xticks(np.linspace(0, len(types) - 1, len(types)))\n",
    "ax_dv.set_xticklabels(types, fontsize=10)\n",
    "ax_dv.set_ylabel('fscore (s)')\n",
    "ax_dv.set_title('Development set Fscore', y=1.08)\n",
    "\n",
    "\n",
    "def autolabel(ax, rectangles):\n",
    "    \"\"\"attach some text vi autolabel on rectangles.\"\"\"\n",
    "    for rect in rectangles:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width() / 2.,\n",
    "                1.05 * height, '%.4f' % height,\n",
    "                ha='center', va='bottom')\n",
    "        plt.setp(plt.xticks()[1], rotation=30)\n",
    "\n",
    "\n",
    "autolabel(ax_tr, rectangles_tr)\n",
    "autolabel(ax_dv, rectangles_dv)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Naive Bayse does not preform best out of all the classifiers. This might be because of our independence assumption which is really wrong (usually there is a relation between word frequencies and lengths), or because we just didn't choose the right features to work by (as we previously seen, choosing word frequencies as a feature does not preform the best on our dataset). \n",
    "\n",
    "As we can see word length preforms the best, that might be because there is actually a really reasonable correlation between the length of a word and how complexed it is, regardless of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.4 - ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases the way a words is classified is based on it's context, meaning sometimes a word can be classified as a simple word and sometimes as a difficult word, all because of the context it appeared in. \n",
    "\n",
    "We would like to show an example for that - in order to do so, we will find two instances of the same word labeled differently in the training set (the code can easily be adapted to any other set, we chose the training one because it made more sense), then we will explore the context trying to infere why they were tagged differently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we'd like to do is to load the context as well as the words, so we can show how it modifies the classification. In order to do so we need to change the file loading code a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_file_with_context(data_file):\n",
    "    words = []\n",
    "    labels = []   \n",
    "    contexts = []\n",
    "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i > 0:\n",
    "                line_split = line[:-1].split(\"\\t\")\n",
    "                words.append(line_split[0].lower())\n",
    "                labels.append(int(line_split[1]))\n",
    "                contexts.append(line_split[3])\n",
    "            i += 1\n",
    "    return words, labels, contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And using the code one the training data we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words, labels, contexts = load_file_with_context(training_file)\n",
    "print(words[0],labels[0],contexts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what we need to do is find a words with multiple instances that differ in how it's labeled. \n",
    "<br>To do so we will find for each word all it's instances in the words list, and then for words with mulitple labels we will look into the labels to see if they differ, if so we will check the context in which they appear to get a better grasp on why the context matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_context_sensitive_words(words, labels, contexts):\n",
    "    words = np.array(words)\n",
    "    differences = []\n",
    "    for word in words:\n",
    "        ii = np.where(words == word)[0]\n",
    "        if(len(ii) > 1):\n",
    "            diff = [(i, j) for i in ii for j in ii if(not labels[i] == labels[j])]\n",
    "            if(len(diff) > 0):\n",
    "                # We only have to iterate it half way because the other half is a mirror of the first\n",
    "                for i in range(int(len(diff)/2)):\n",
    "                    differences.append((words[diff[i][0]], diff[i][0], diff[i][1]))\n",
    "    return differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ambiguity = find_context_sensitive_words(words, labels, contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can show all these words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for couple in ambiguity:\n",
    "    word, index_a, index_b = couple\n",
    "    print(\"Word:\", word)\n",
    "    print(\"Labeled as {} in index {} and as {} in index {}\".format(labels[index_a], index_a, labels[index_b], index_b))\n",
    "    print(\"First context:\\n\", contexts[index_a], \"\\n\")\n",
    "    print( \"Second context:\\n\", contexts[index_b], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at some of the words one can see how and why they were labeled differently:\n",
    "- If we look at the word _sprouts_ it appears in two contexts - firstly as a name of a place \"Sprouts Farmer\" which makes it a very easy identification, but then as a 'sprinkle of sprouts' which is much more complexed (especially if like us you're not a native English speaker and had to google 'sprouts')\n",
    "- If we look at the word _element_ , seemengly a simple word, it can come as an actual element like in the first context(\"the heaviest element\") but it can also appear in as a slang word - \"in his element\" like in the second context, which complicate things. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Remark___ : We did now want to load the entire notebook as part of our notebook because that will just be overloading on our notebook, but we do want to touch upon required information so we imported the methods we need from the original notebook and implemented methods to fit the requirements of our question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to do is to explore the dataset they used for the classifiers implementation. In order to do that we will first have to load the dataset(note that we used the code they implemented to do so, but we modified it for our need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReutersParser(html_parser.HTMLParser):\n",
    "    \"\"\"Utility class to parse a SGML file and yield documents one at a time.\"\"\"\n",
    "\n",
    "    def __init__(self, encoding='latin-1'):\n",
    "        html_parser.HTMLParser.__init__(self)\n",
    "        self._reset()\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        method = 'start_' + tag\n",
    "        getattr(self, method, lambda x: None)(attrs)\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        method = 'end_' + tag\n",
    "        getattr(self, method, lambda: None)()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.in_title = 0\n",
    "        self.in_body = 0\n",
    "        self.in_topics = 0\n",
    "        self.in_topic_d = 0\n",
    "        self.title = \"\"\n",
    "        self.body = \"\"\n",
    "        self.topics = []\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "    def parse(self, fd):\n",
    "        self.docs = []\n",
    "        for chunk in fd:\n",
    "            self.feed(chunk.decode(self.encoding))\n",
    "            for doc in self.docs:\n",
    "                yield doc\n",
    "            self.docs = []\n",
    "        self.close()\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if self.in_body:\n",
    "            self.body += data\n",
    "        elif self.in_title:\n",
    "            self.title += data\n",
    "        elif self.in_topic_d:\n",
    "            self.topic_d += data\n",
    "\n",
    "    def start_reuters(self, attributes):\n",
    "        pass\n",
    "\n",
    "    def end_reuters(self):\n",
    "        self.body = re.sub(r'\\s+', r' ', self.body)\n",
    "        self.docs.append({'title': self.title,\n",
    "                          'body': self.body,\n",
    "                          'topics': self.topics})\n",
    "        self._reset()\n",
    "\n",
    "    def start_title(self, attributes):\n",
    "        self.in_title = 1\n",
    "\n",
    "    def end_title(self):\n",
    "        self.in_title = 0\n",
    "\n",
    "    def start_body(self, attributes):\n",
    "        self.in_body = 1\n",
    "\n",
    "    def end_body(self):\n",
    "        self.in_body = 0\n",
    "\n",
    "    def start_topics(self, attributes):\n",
    "        self.in_topics = 1\n",
    "\n",
    "    def end_topics(self):\n",
    "        self.in_topics = 0\n",
    "\n",
    "    def start_d(self, attributes):\n",
    "        self.in_topic_d = 1\n",
    "\n",
    "    def end_d(self):\n",
    "        self.in_topic_d = 0\n",
    "        self.topics.append(self.topic_d)\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "\n",
    "def stream_reuters_documents(data_path=None):\n",
    "    DOWNLOAD_URL = ('http://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "                        'reuters21578-mld/reuters21578.tar.gz')\n",
    "    ARCHIVE_FILENAME = 'reuters21578.tar.gz'\n",
    "    data_path = os.path.join(get_data_home(), \"reuters\")\n",
    "    if not os.path.exists(data_path):\n",
    "        \"\"\"Download the dataset.\"\"\"\n",
    "        print(\"downloading dataset (once and for all) into %s\" %\n",
    "              data_path)\n",
    "        os.mkdir(data_path)\n",
    "\n",
    "        def progress(blocknum, bs, size):\n",
    "            total_sz_mb = '%.2f MB' % (size / 1e6)\n",
    "            current_sz_mb = '%.2f MB' % ((blocknum * bs) / 1e6)\n",
    "            if _not_in_sphinx():\n",
    "                sys.stdout.write(\n",
    "                    '\\rdownloaded %s / %s' % (current_sz_mb, total_sz_mb))\n",
    "\n",
    "        archive_path = os.path.join(data_path, ARCHIVE_FILENAME)\n",
    "        urlretrieve(DOWNLOAD_URL, filename=archive_path,\n",
    "                    reporthook=progress)\n",
    "\n",
    "        print(\"untarring Reuters dataset...\")\n",
    "        tarfile.open(archive_path, 'r:gz').extractall(data_path)\n",
    "        print(\"done.\")\n",
    "          \n",
    "    parser = ReutersParser()\n",
    "    doc_counter = 0\n",
    "    docs = {\n",
    "        'title_index': [],\n",
    "        'title': [],\n",
    "        'body': [],\n",
    "        'topic': []\n",
    "    }\n",
    "    for filename in glob(os.path.join(data_path, \"*.sgm\")):\n",
    "        for doc in parser.parse(open(filename, 'rb')):\n",
    "            if(doc['topics']):\n",
    "                for topic in doc['topics']:\n",
    "                    docs['title_index'].append(doc_counter)\n",
    "                    docs['title'].append(doc['title'])\n",
    "                    docs['body'].append(doc['body'])\n",
    "                    docs['topic'].append(topic)\n",
    "            else:\n",
    "                docs['title_index'].append(doc_counter)\n",
    "                docs['title'].append(doc['title'])\n",
    "                docs['body'].append(doc['body'])\n",
    "                docs['topic'].append(None)\n",
    "            doc_counter += 1\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did is to modify the downloader function to return a dictionary of the form 'title','body', 'topic' which we then can feed into the Pandas dataframe module and explore it.\n",
    "Note how we arranged it by topics so it will be easier to handle for our needs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_stream = stream_reuters_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=data_stream)\n",
    "print(\"there are {} topics in the dataset\".format(df.topic.nunique()))\n",
    "print(\"and {} unique titles\".format(df.title.nunique()))\n",
    "print(\"there are {} documents in the dataset\".format(df.title_index.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each of the topics we can get the count, mean, max index, std, and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df.groupby('topic').describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring the dataset in terms of the documents it contains and the topics they are about, we want to explore the dataset content in terms of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_vocab_statistics(dataframe):    \n",
    "    print(\"statistics about the characters in each of the document's body:\\n\")\n",
    "    print(dataframe['body_length_chars'].describe(), \"\\n\")\n",
    "    \n",
    "    print(\"statistics about the words in each of the document's body:\\n\")    \n",
    "    print(dataframe['body_length_words'].describe(), \"\\n\")\n",
    "    \n",
    "    print(\"total amount of words in all the documents(body): {}\\n\".format(dataframe['body_length_words'].sum()))\n",
    "    print(\"total amount of characters in all the documents(body): {}\\n\".format(dataframe['body_length_chars'].sum()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we first want to modify the dataframe to have all the required fields for the statistics\n",
    "mod = df.drop(['topic'], axis=1)\n",
    "mod = mod.drop_duplicates(subset=['title_index'], keep='first')\n",
    "mod['body_length_chars'] = mod['body'].map(lambda x: len(x))\n",
    "mod['body_length_words'] = mod['body'].map(lambda x: len(word_tokenize(x)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "report_vocab_statistics(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reporting and gathering all the information regarding the dataset, it is time for us to start analyzing the code and what's going on in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main principle the author of the code is trying to show is the idea of 'out of core' learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What is out of core learning?__ \n",
    "<br>Sometimes, especially when dealing with text documents, the dataset we get to work with is much greater than the actual RAM we have on a decent computer, so we wouldn't want to load all of it in one go (that will just result in our computer slowing down until it can't go on...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of out of core learning is this - we will load a certain amount of data each time and learn from it (batches), so generally speaking, most of our learning is done from things we don't hold in our memory anymore (therfore out of core)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, Scikit-learn implemented an API called \"partial_fit\" - a classifier that implements a partial fit works as follows: \n",
    "<br>The classifier will see a batch and then will incremently update whatever it is learning based on that batch, that way whenever a new batch arrives it learns what it needs from it and \"throws it away\" instead of holding it in memory or trying to learn everything in one go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, as we've seen just now the dataset we work with has over $2000000$ words, that is a very large dataset to work with in one batch (for example, in the last assignment we only had $978726$ and that took a while to process as well...), \n",
    "so to try and infere information from it, let alone actual learning, is a nearly impossible task if you use a regular computer. What we should do instead is 'chunk' the dataset (in the notebook they used a 1000 documents each time, and considering there are $21578$ documents, it means $\\frac{2854659}{21578} = 132.3$ (approx) words per document, so we only work with $132300$ words each time which is much more reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formal name for the learning process we just discussed is called _online setting_ - lets farther explain it (we'll focus on classification, but there are also online settings for regression problems).\n",
    "<br>On each round we receive an instance $x_t$ and extend a prediction using our current hypothesis $w_t$(we estimate weights). \n",
    "We then receive the true target $y_t$ and suffer an instantaneous loss based on the discrepancy between $y_t$ and our prediction.\n",
    "<br>Our goal is to make the cumulative loss that we suffer small.\n",
    "Finally, we update the hypothesis according to the previous hypothesis and the current example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more formally - we can describe this using math as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given $x_t \\in R^n$ , we predict $sign(w_t * x_t)$\n",
    "- We get the true target and suffer loss - $\\max({\\epsilon - y_{t}w_{t}*x_{t}})$\n",
    "- We update the hypothesis - $w_{t+1} = g(w_t, z_t)$\n",
    "\n",
    "Usually in classification $z_t$ is set to be the distance between our results and the true prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code they implemented test and compare a few classifiers that implement a partial fit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ___SGD___ - SGD stands for Stochastic Gradient Descent. This methood is really similar to the Gradient Desecent method we discussed in class, only this time it is stochastics, meaning the samples are selected randomly. Each sample at a time the gradient of the loss is estimated and the model is updated with a learning rate. Just like regular gradient descent we optimize $w$ by minimizing the loss function. In the begining we choose an initial guess for $w$ and a learning rate $\\eta$, we then randomly shuffle the examples in the training set, and then for each sample we update $w$ to be  $w = w - \\eta \\nabla Q_i(w)$ , we keep doing so until the algorithm converge. \n",
    "\n",
    "-  ___Perceptron___ - this is an algorithm for supervised learning of a binary classifier (a classifier which determines if an inputed represented by a vector of numbers fits a class or doesn't). the algorithm is based on a linear-predictor function, which is a function of the form $g(x) = wx + b$ where $x$ is our input, $w$ is a set of weights, and $b$ is the bias. what the perceptron does if to return $1$ if $g(x) > 0$ and $0$ otherwise. The algorithm learns the set of weights(tunes the weights) based on the training data, and then once it has them and it gets a vector x from the test data it feeds it to the function $g$ with the learned weights.\n",
    "\n",
    "-  ___NB Multinomial___ - Naive Bayes Multinomial Classifier is a Naive Bayes Classifier (see Part 1 of the assignment for  explenation) for multinomial models, that means that the feature vectors represent the frequencies with which certain events have been generated by a multinomial with probabilities $p_1, \\dots, p_n$ such that $p_i$ is the probability that event $i$ occures. A feature vector in this model is therefor a histogram with $x_i$ counting the number of times event i was observed in a particular instance. In document classification this  classifier is usually used with events representing the occurrence of a word in a single document (aka bag-of-words). \n",
    "\n",
    "-  ___Passive-Aggressive___ - this type of classifier uses a measure called _hinge-loss_ in it's training process. The hinge loss is defined as \n",
    "<br>$\\ell(w; (x,y)) = \\max(0, \\delta(w,(x,y))-\\epsilon)$\n",
    "<br> where $\\delta(w,(x,y))$ is the distance between $y$ and $xw$ : $\\delta(w,(x,y)) = |y_t-(x_{t}w_{t}|$\n",
    "<br> and $\\epsilon$ is the insensitivity parameter.\n",
    "<br> The algorithm goal is to make the hinge loss as small as possible at each round.\n",
    "<br> On each round of the algorithm we update the weights based on -\n",
    "<br> $w_{t+1} = \\arg\\min_{w \\in R^n} \\frac{||w-w_t||^2}{2} \\\\ s.t \\\\ \\ell(w; (x_t,y_t)) = 0$\n",
    "<br> Which is basically a projection of $w_t$ on the space of vectors that has a 0 loss.\n",
    "<br> This algorithm is _passive_ when the hinge loss is 0, because then we get $w_{t+1} = w_t$, but it's _aggressive_ when that's not the case, because when it's not 0 it aggressively forces $\\ell(w_{t+1}; (x_t,y_t)) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After explaining each of the Algorithms we only have one thing to address - the _hashing-vectorized_.\n",
    "<br>The method converts a collection of documents into a matrix of token occurences, and it uses scipy sparse matrix (which means that, considering this probably is a very sparse matrix, all the values that are assigned 0 are not kept in memory). The idea in this implementation is to use hashing to find the token string name to feature integer index mapping.\n",
    "<br>We need to use this method here for a few reasons:\n",
    "- It is very low in memory, we don't need to store an entire dictionary in memory. \n",
    "- There is no state computed during fit (in partial fit). \n",
    "- It holds no state besides the constructor parameters. \n",
    "\n",
    "As we note in the code, they used this method with $n_{features} = 2^{18}$, that is because if they don't do so, there might be collisions (different tokens mapped to same place)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we'll be looking into the task of classifying emails and SMS messages as 'spam' and 'ham' ('ham' being the opposite of spam - good, meaningful messages). We'll be looking and answering questions about pre-written code. The first part of the code loads the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress(i, end_val, bar_length=50):\n",
    "    '''\n",
    "    Print a progress bar of the form: Percent: [#####      ]\n",
    "    i is the current progress value expected in a range [0..end_val]\n",
    "    bar_length is the width of the progress bar on the screen.\n",
    "    '''\n",
    "    percent = float(i) / end_val\n",
    "    hashes = '#' * int(round(percent * bar_length))\n",
    "    spaces = ' ' * (bar_length - len(hashes))\n",
    "    sys.stdout.write(\"\\rPercent: [{0}] {1}%\".format(hashes + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "NEWLINE = '\\n'\n",
    "HAM = 'ham'\n",
    "SPAM = 'spam'\n",
    "\n",
    "SOURCES = [\n",
    "    ('data/spam',        SPAM),\n",
    "    ('data/easy_ham',    HAM),\n",
    "    ('data/hard_ham',    HAM),\n",
    "    ('data/beck-s',      HAM),\n",
    "    ('data/farmer-d',    HAM),\n",
    "    ('data/kaminski-v',  HAM),\n",
    "    ('data/kitchen-l',   HAM),\n",
    "    ('data/lokay-m',     HAM),\n",
    "    ('data/williams-w3', HAM),\n",
    "    ('data/BG',          SPAM),\n",
    "    ('data/GP',          SPAM),\n",
    "    ('data/SH',          SPAM)\n",
    "]\n",
    "\n",
    "SKIP_FILES = {'cmds'}\n",
    "\n",
    "\n",
    "def read_files(path):\n",
    "    '''\n",
    "    Generator of pairs (filename, filecontent)\n",
    "    for all files below path whose name is not in SKIP_FILES.\n",
    "    The content of the file is of the form:\n",
    "        header....\n",
    "        <emptyline>\n",
    "        body...\n",
    "    This skips the headers and returns body only.\n",
    "    '''\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = open(file_path, encoding=\"latin-1\")\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield file_path, content\n",
    "\n",
    "\n",
    "def build_data_frame(l, path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for i, (file_name, text) in enumerate(read_files(path)):\n",
    "        if ((i + l) % 100 == 0):\n",
    "            progress(i + l, 58910, 50)\n",
    "        rows.append({'text': text, 'class': classification})\n",
    "        index.append(file_name)\n",
    "   \n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame, len(rows)\n",
    "\n",
    "def load_data():\n",
    "    data = DataFrame({'text': [], 'class': []})\n",
    "    l = 0\n",
    "    for path, classification in SOURCES:\n",
    "        data_frame, nrows = build_data_frame(l, path, classification)\n",
    "        data = data.append(data_frame)\n",
    "        l += nrows\n",
    "    data = data.reindex(np.random.permutation(data.index))\n",
    "    return data\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a fancy code, but the imporant thing about it is the data structure: a pandas dataframe in size (D,2) - D being the number of **documents** in the collection. For each document, there are two columns: text (the actual text of the document) and class (classification as ham/spam).\n",
    "\n",
    "This is the training code (no need to run it yet, just understand how it works):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "        ('classifier',         MultinomialNB())\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def train(data = None, n_folds = 6):\n",
    "    if data is None:\n",
    "        print(\"Loading data...\")\n",
    "        data = load_data()\n",
    "        print(\"Data loaded\")\n",
    "    k_fold = KFold(n_splits = n_folds)\n",
    "    pipeline = build_pipeline()\n",
    "    scores = []\n",
    "    confusion = numpy.array([[0, 0], [0, 0]])\n",
    "    print(\"Training with %d folds\" % n_folds)\n",
    "    for i, (train_indices, test_indices) in enumerate(k_fold.split(data)):\n",
    "        train_text = data.iloc[train_indices]['text'].values\n",
    "        train_y = data.iloc[train_indices]['class'].values.astype(str)\n",
    "\n",
    "        test_text = data.iloc[test_indices]['text'].values\n",
    "        test_y = data.iloc[test_indices]['class'].values.astype(str)\n",
    "\n",
    "        print(\"Training for fold %d\" % i)\n",
    "        pipeline.fit(train_text, train_y)\n",
    "        print(\"Testing for fold %d\" % i)\n",
    "        predictions = pipeline.predict(test_text)\n",
    "\n",
    "        confusion += confusion_matrix(test_y, predictions)\n",
    "        score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "        scores.append(score)\n",
    "        print(\"Score for %d: %2.2f\" % (i, score))\n",
    "        print(\"Confusion matrix for %d: \" % i)\n",
    "        print(confusion)\n",
    "\n",
    "    print('Total emails classified:', len(data))\n",
    "    print('Score:', sum(scores)/len(scores))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick overview of how it works:\n",
    "The code uses a technique called k-fold (cross validation). Instead of simply splitting the corpus to a training and test set, it splits the dataset into k equal parts. The code than runs k times, each time selecting a different part of the corpus as the test set (and the rest k-1 parts as training). So this way, we can use the entire corpus as both training and test set. The disadvantage to this is, of course, performance (the whole learning/predicting process runs k times). \n",
    "In this case, we can assume why this technique was used: our corpus is made of several different sources, so simply splitting the corpus would have likely resulted in a test set that isn't similar to the training set.\n",
    "\n",
    "Another new technique introduced is the pipeline: it is used to create a sequence of several transformers (and a final estimator), and package them all as one object. It does not provide new functionality on it's own, but rather serves as a convinient method to pack a workflow of several transformers.\n",
    "\n",
    "The first element in the pipeline is the CountVectorizer. It's basic function is simple - it recieves a list of documents, and creates a matrix of documents X token counts. So if a token j appears 4 times in a document i, the matrix value in cell (i,j) will be 4. A token can be an n-gram as well, and we're able to choose an n-gram range, so the Vectorizer is able to mix tokens of different sizes in one matrix. In this example, our n-range is 1-2, meaning we check unigrams and bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create the CountVectorizer, and use it to transform our data into a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 2))\n",
    "data_x, data_y = data['text'], data['class']\n",
    "X = cv.fit_transform(data_x, data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're using the entire corpus, with no training/test split, because the k-fold technique ensures that the entire corpus will eventually be used as both training and test data.\n",
    "\n",
    "First we'll calculate the number of unigrams and bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_uni_bi (fnames):\n",
    "    cnt = Counter([len(feature.split()) for feature in fnames])\n",
    "    return {\"unigrams\": cnt.get(1), \"bigrams\": cnt.get(2)}\n",
    "\n",
    "fnames = cv.get_feature_names()\n",
    "count_uni_bi (fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both unigrams and bigrams are used as features (and they're the only features), we could use *get_feature_names* to get the list of all unigrams and bigrams. From here it's just a matter of separating them and counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to get the most frequent unigrams and bigrams. We could work directly with our matrix, summing every column and printing the top 50 biggest numbers of each n-gram. However, because of the matrix size, we found this to be problematic performance-wise. \n",
    "Instead, we would like to recreate the tokens and count ourselves. Luckily, scikit provides easy access to the very same preprocessing methods they use, so we can assure our tokenization methods are identical. We've created a function that splits the data into unigrams and bigrams, and we use the Counter \"most_common\" method on both.\n",
    "Not that the bigrams split is also done within a document (meaning the last word of document and the first word of the next document will **not** create a bigram) - this is consistent with the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_uni_bi (data, analyzer):\n",
    "    uni, bi = [], []\n",
    "    for doc in data:\n",
    "        tokens = analyzer(doc)\n",
    "        for t in tokens:\n",
    "            if len(t.split()) == 1:\n",
    "                uni.append(t)\n",
    "            else:\n",
    "                bi.append(t)\n",
    "    return [uni,bi]\n",
    "\n",
    "analyzer = cv.build_analyzer()\n",
    "uni_bi_list = split_uni_bi(data_x, analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"unigrams:\")\n",
    "Counter(uni_bi_list[0]).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"bigrams:\")\n",
    "Counter(uni_bi_list[1]).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these words look unlikely (they mostly look like HTML documents), but we've verified with the original matrix too. This is a spam dataset, after all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next task, we'll need the same split but for each class. So we'll filter the dataset per class and use the same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ham = data.loc[data['class'] == 'ham']\n",
    "spam = data.loc[data['class'] == 'spam']\n",
    "ham_x, spam_x = ham['text'], spam['text']\n",
    "ham_uni_bi_list = split_uni_bi(ham_x, analyzer)\n",
    "spam_uni_bi_list = split_uni_bi(spam_x, analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"ham unigrams:\")\n",
    "Counter(ham_uni_bi_list[0]).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"ham bigrams:\")\n",
    "Counter(ham_uni_bi_list[1]).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"spam unigrams:\")\n",
    "Counter(spam_uni_bi_list[0]).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"spam bigrams:\")\n",
    "Counter(spam_uni_bi_list[1]).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to find the top 20 features for each class. Scikit offers a solution for this case - SelectFromModel. However, we ran into two problems when trying to use it:\n",
    "-SelectFromModel does not support sparse matrices (and our matrix is way too big to not be sparse)\n",
    "-More importantly - for binary classification tasks, SelectFromModel can only returns one list of top features (and not a feature for each class).\n",
    "\n",
    "We've had to dig deeper, and when we checked the source code, we discovered that SelectFromModel basically just returns the features with the highest coefficient (more specifically, highest absolute value of coefficient). So we've decided to work with the coefficient ourselves. So first, let's get our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = data_x[:49092]\n",
    "train_y = data_y[:49092]\n",
    "X = cv.fit_transform(train_text)\n",
    "mnb = MultinomialNB().fit (X, train_y)\n",
    "coefs = mnb.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this time we do split to train and testing data, but we only split once in the 'normal' method, without using cross-validation.\n",
    "\n",
    "We have our coefficient list, but we still run into the same problem - we only have a general list for the classifier, and not for each category. At first we thought that each side end of the list represents a different class (so the top 20 features will be class A, and the bottom 20 class B). But upon further inspection, this turned out to be wrong. We looked at the \"feature count\" attribute of each feature - which represents the number of samples encountered for each (class, feature) during fitting.\n",
    "The bottom 20 looked pretty good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "args = np.argsort(coefs)\n",
    "top_20, bottom_20 = args[:20], args[-20:]\n",
    "bottom = [feature_names[i] for i in bottom_20]\n",
    "print (\"bottom 20:\")\n",
    "for i in range(20):\n",
    "    print (bottom[i], mnb.feature_count_[0][bottom_20[i]], mnb.feature_count_[1][bottom_20[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the top 20, on the other hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top = [feature_names[i] for i in top_20]\n",
    "print (\"top 20:\")\n",
    "for i in range(20):\n",
    "    print (top[i], mnb.feature_count_[0][top_20[i]], mnb.feature_count_[1][top_20[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all those features have extremely small feature counts - which means they are extremely rare, and not valuable features. \n",
    "This explains why Scikit used the top **absolute values** of coefficients - all our coefficients are negative numbers, and here we see that the bottom negative numbers are the valueable ones - hence, the one who have the highest absolute value.\n",
    "Also, inspecting Scikit's code and documentation, nowhere does it implies there's any use to the worst coefficients (for example, SelectFromModel throws them away beneath a certain threshold). \n",
    "\n",
    "So, the coefficient list on it's own doesn't solve our problem - we had to dig even deeper. We discovered a curious thing: when we checked coeff for other classification tasks, with more than one class, coeff actually returned a separate list for every class. Only in binary tasks, coeff returns only one list. So we looked into Naive Bayes code to discover what are those coefficient values, and this is what we found:\n",
    "\n",
    "def _get_coef(self):\n",
    "    return (self.feature_log_prob_[1:]\n",
    "            if len(self.classes_) == 2 else self.feature_log_prob_)\n",
    "            \n",
    "(this is a quote from Scikit's code, so we can't run it here).\n",
    "\n",
    "We disover that the coefficient is nothing more than the log probability of every feature. And also, it seems that for some reason, for binary tasks, Scikit decides specifically to only return the coefficients for the first class. But luckily, we can still access the original log probabilities, and get both lists. We just have to check which class is 0, and which is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we just have to access the bottom 20 (or top 20 absolute values) features of every list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ham_coeffs = mnb.feature_log_prob_[0]\n",
    "spam_coeffs = mnb.feature_log_prob_[1]\n",
    "\n",
    "def print_bottom_names (coeffs, feature_names,n, class_num):\n",
    "    args = np.argsort(coeffs)\n",
    "    bottom_co = args[-n:]\n",
    "    bottom = [feature_names[i] for i in bottom_co]\n",
    "    for i in range(n):\n",
    "        print (bottom[i], mnb.feature_count_[class_num][bottom_co[i]])\n",
    "        \n",
    "print (\"top features for ham:\")\n",
    "print_bottom_names(ham_coeffs, feature_names, 20, 0)\n",
    "\n",
    "print()\n",
    "print (\"top features for spam:\")\n",
    "print_bottom_names(spam_coeffs, feature_names, 20, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our results know makes perfect sense - we see that \"ham\" words are generally pretty normal (\"be\", \"with\" etc) while \"spam\" words are generally more nonsense. Each word also have very high feature count for its class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we want to add another feature to our model - the length of the text. But we should consider that the new feature is usually on a completely different scale to our normal features. The length of most documents is thousands of characters, While our usual features (number of appearences for a certain word/bigram in a document) is much lower. Is this a problem?\n",
    "\n",
    "Let's return to the way Naive Bayes classifier works, as explained in part 1:\n",
    "- We assume the features are independent\n",
    "- We calculate the probability of belonging to a specific class:\n",
    "<br>$P(f_1, f_2, f_3, \\dots , f_n \\vert l) = \\prod{P(f_i|l)}$\n",
    "\n",
    "As we can see, the probability for each feature is calculated separately and independently, and eventually we multiply all those probabilities. Because of that, the scale of the feature doesn't make a difference. Handling of each feature is independent, and by the time we multiply them, they're all converted to probabilities (which are of course scaled between 0-1).\n",
    "\n",
    "What about Logistic Regression? \n",
    "<br>Well, first we have to understand how it works. Logistic Regression is similar to Linear Regression (like we implemented in homework1, question 2). The main difference is that while we now wish to use it for binary classification tasks (while linear regression predicted the y value, but not a binary value). So instead, we use the Sigmoid Function to normalize our predicted values between 0 and 1.\n",
    "\n",
    "(add sigmoid function here)\n",
    "\n",
    "Logistic regression, on it's own, does not require scaling. The reason is that after repeated iterations, the optimal coefficients for each feature are chosen, and those coefficients take into account the scale of the feature. That's not to say scaling is useless. Many optimization algorithms (such as gradient descent) converges faster on normalized values, meaning that scaling will lead to better performance. But it isn't neccesary.\n",
    "\n",
    "So when is scaling neccesary?\n",
    "<br>If we're using a Logistic Regression model **with regularization**. Certain penalty functions are very sensitive to scaling, meaning that using the same feature but in different scale (for example, using meters or kilometers) will result in drastically different penalties. The Logistic Regression used in the model uses the solver 'lbfgs', which according to Scikit documentation, only works with the L2 penalty function:\n",
    "\n",
    "(add L2 penalty function)\n",
    "\n",
    "This function is sensitive to scaling. So in order to use this length feature in the Logistic Regression model, we'll need to scale it - but not because of the Logistic Regression itself, but because of the regularization method it uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's add the length feature to our model. We will build our own Pipeline, and we'll also use FeatureUnion. FeatureUnion concatenates lists of features from different transformers. Meaning we can \"prepeare\" several feature sets separately (running each one through different transformers), and use the FeatureUnion to combine them to a single feature list.\n",
    "\n",
    "The FeatureUnion and the Pipeline accept objects called transformers. Transformers in Scikit are objects who inherit from general class TransformerMixin, and implement two methods: fit and transform. In our case, we will build a very simple transformer that returns the length of each string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LengthTransformer(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return np.array([len(doc) for doc in X]).reshape(-1,1)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our fit method does nothing (as there's no need to fit the data in any way). Our transform method, at it's core, returns the length of every item in the training set. It also prepears the datat for the scaler - by converting it into a numpy array, and reshaping it (as the scaler does not accept 1D arrays).\n",
    "Now here's our FeatureUnion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "union = FeatureUnion([\n",
    "                (\"count\", CountVectorizer(ngram_range=(1, 2))), \n",
    "                (\"length\", Pipeline([(\"lt\", LengthTransformer()), (\"scaler\", StandardScaler(with_mean=False))]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge two feature sets - the \"count' feature set is the CountVectorizer from eariler, while the \"length\" feature set is our newly written length transformer. We normalize the length, for reasons we explained in the previous question.\n",
    "\n",
    "So here's our complete pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_pipeline2():\n",
    "    pipeline = Pipeline([\n",
    "        ('features',   FeatureUnion([\n",
    "                (\"count\", CountVectorizer(ngram_range=(1, 2))), \n",
    "                (\"length\", Pipeline([(\"lt\", LengthTransformer()), (\"scaler\", StandardScaler(with_mean=False))]))])), \n",
    "        ('classifier',         LogisticRegression(solver='lbfgs'))\n",
    "    ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's run the original code, with our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train2(data = None, n_folds = 4):\n",
    "    if data is None:\n",
    "        print(\"Loading data...\")\n",
    "        data = load_data()\n",
    "        print(\"Data loaded\")\n",
    "    k_fold = KFold(n_splits = n_folds)\n",
    "    pipeline = build_pipeline2()\n",
    "    scores = []\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    print(\"Training with %d folds\" % n_folds)\n",
    "    for i, (train_indices, test_indices) in enumerate(k_fold.split(data)):\n",
    "        train_text = data.iloc[train_indices]['text'].values\n",
    "        train_y = data.iloc[train_indices]['class'].values.astype(str)\n",
    "        test_text = data.iloc[test_indices]['text'].values\n",
    "        test_y = data.iloc[test_indices]['class'].values.astype(str)\n",
    "        \n",
    "        print(\"Training for fold %d\" % i)\n",
    "        pipeline.fit(train_text, train_y)\n",
    "        print(\"Testing for fold %d\" % i)\n",
    "        predictions = pipeline.predict(test_text)\n",
    "        \n",
    "        confusion += confusion_matrix(test_y, predictions)\n",
    "        score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "        scores.append(score)\n",
    "        \n",
    "        print(\"Score for %d: %2.2f\" % (i, score))\n",
    "        print(\"Confusion matrix for %d: \" % i)\n",
    "        print(confusion)\n",
    "\n",
    "    print('Total emails classified:', len(data))\n",
    "    print('Score:', sum(scores)/len(scores))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion)\n",
    "    return pipeline\n",
    "    confusion = confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "    print(\"Score for %d: %2.2f\" % (i, score))\n",
    "    print(\"Confusion matrix for %d: \" % i)\n",
    "    print(confusion)\n",
    "    print('Total emails classified:', len(test_text))\n",
    "    return pipeline\n",
    "\n",
    "pipeline = train2(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will create a NER (Named Entity Recognition) model that recognizes names of persons, organizations and other entities in text. We will use the CoNLL 2002 dataset. As we build the code, we will use the Spanish version of the dataset. At the end of every segment, we'll test our functions on the Dutch dataset and compare the differences. \n",
    "First, we will import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "etr = conll2002.chunked_sents('esp.train') # In Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our data, as presented in the assignment, comes in the form of **chunked_sents**. This is a tree data structure, that represent the data like this:\n",
    "\n",
    "-A word is represented as a pair (word, POS tagging).\n",
    "-Each sentence is a tree. The root of the tree is always \"S\".\n",
    "-The words of the sentence are the children of the root, depending on their NER tag:\n",
    "For NER tag O, the word itself (represented by a pair) will be a direct children of the root. \n",
    "For other tags, the children will be another subtree. His root will be the category (for example, \"LOCATION\"), and his children will be all the words within the LOCATION tag.\n",
    "\n",
    "This representation is equivalent to a list, and we can easily convert between the two. It does have one advantage over lists - the tree structure \"forces\" legal tag sequences by its nature, and it's impossible to represent an illegal sequence using a tree. Nevertheless, for our questions we will use the list form, for several reasons:\n",
    "\n",
    "-The chunked_sents represnatation doesn't directly include the word tags we need.\n",
    "-Scikit provides a much wider toolkit for lists rather than trees, including the models we were asked to use (such as DictVectorizer).\n",
    "-Working with a list is much more simpler and convinient.\n",
    "We'll refer to the question of illegal sequences later on.\n",
    "\n",
    "So here is our lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sents = list(conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(conll2002.iob_sents('esp.testa'))\n",
    "d_train_sents = list(conll2002.iob_sents('ned.train'))\n",
    "d_test_sents = list(conll2002.iob_sents('ned.testa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand how our dataset is represented. For example, let's look at an example of the first element (and hence, first sentence) in the first dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see:\n",
    "\n",
    "- A word is represented by a tuple of three elements: the word itself, it's POS (Part of Speech) tagging, and it's correct NER tagging.\n",
    "- A sentence is represented by a list of words\n",
    "- Each element in the dataset is a sentence.\n",
    "\n",
    "Now that we understand our data, it's time to extract features. We'll be looking for word level features, and in this step we'll be looking at each word separately. Those are the features we've chosen to extract for every word:\n",
    "\n",
    "Form (The actual word), POS tagging, is number, does it contain a number, does it begin with a capital letter, is it all capital letters, is it a punctuation char, the first one, two and three letters of the word, the last one, two and three letters in of the word. \n",
    "Here's the code for feature extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hasNumbers(str):\n",
    "    return any(c.isdigit() for c in str)\n",
    "\n",
    "def get_word_features (word):\n",
    "    w = word[0]\n",
    "    features = {\n",
    "     \"form\": w,\n",
    "     \"pos\": word[1],\n",
    "     \"is_number\": w.isdigit(),\n",
    "     \"contains_number\": hasNumbers(w),\n",
    "     \"beginCapital\": w[0].isupper(),\n",
    "     \"allCaps\": w.isupper(),\n",
    "     \"isPunc\": w in string.punctuation,\n",
    "     \"firstLetter\": w[0],\n",
    "     \"first2Letters\": w[0:2],\n",
    "     \"first3Letters\": w[0:3],\n",
    "     \"lastLetter\": w[-1],\n",
    "     \"last2Letters\": w[-2:],\n",
    "     \"last3Letters\": w[-3:]\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's an example on the word 'Melbourne':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_word_features(train_sents[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have our features, it's time to train our model. We will be using Scikit's DictVectorizer data structure to keep our data, and it's logistic regression implementation for the training. Those methods requires two seperate lists of identical size, where every element represents one word in the corpus. The first list (X) keeps the list of features for every word, and the second list (y) has the NER tagging of the word - which is the answer our model will be trying to guess.\n",
    "\n",
    "Here is the code that creates those lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_corpus_features (corpus):\n",
    "#gets a corpus, returns a list of features for every word\n",
    "    X = []\n",
    "    for sent in corpus:\n",
    "        X += [get_word_features(w) for w in sent]\n",
    "    return X\n",
    "\n",
    "def get_y (corpus):\n",
    "    y = []\n",
    "    for sent in corpus:\n",
    "        y += [w[2] for w in sent]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in addition to the obvious effect of getting features/NER taggings, those functions also transform our data from a list of **sentences** to a list of **words** - so our output is a big list of all the words, not divided to sentences anymore.\n",
    "\n",
    "Now we'll create our DictVectorizer and train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_sents, v, features):\n",
    "    y = get_y (train_sents)\n",
    "    X = v.fit_transform(features)\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X, y)\n",
    "    return clf\n",
    "\n",
    "v = DictVectorizer(sparse=True)\n",
    "features = get_corpus_features(train_sents)\n",
    "clf = train(train_sents, v, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **DictVecotizer** structure and it's fit_transform method, transforms the features list to a matrix. Each row in the matrix represents a word, and each column represents a feature. Each cell M(i,j) includes the corresponding numerical value for feature j in word i.\n",
    "\n",
    "But what if the features are non-numerical? Boolean data easily transforms to 1 (True) or 0 (False), but strings are a bit more complicated to encode. DictVectorizer's solution is to create a separate boolean feature for every string it encounters. For example, for the word \"Melbourne\", rather than the \"firstLetter\" feature, it will create a new boolean feature: \"firstLetter=M\", that will be True for every word that begins in M, and False otherwise. This is similar to a One-Hot Encoding vector.\n",
    "\n",
    "This is a good solution, but it means that for each possible value of a string feature (for example, each word, prefix or suffix of length 1,2,3), a specific feature will be created. In other words, our matrix is getting really big. For the first sentence alone (11 words), 75 features are created. For the entire dataset (264715 words), the matrix will be huge - in fact, too huge for the computer's memory (or at least, *my* computer's memory) to handle.\n",
    "\n",
    "That's why we use a **sparse** version of the matrix. How do this work? The crucial fact is that while we have a lot of data, mthe huge majority of it (typically above 99%) is zeros. For example, the word \"Melbourne\" will recieve 1 in the \"firstLetter=M\" feature, and 0 in every other \"firstLetter\" feature. The same applies for every single string feature (some of them, such as 3 letter combinations, have a lot of possible values) - that's a lot of zeros.\n",
    "So the sparse data structre take advantage of that fact. Rather than actually save all those zeros, it only keeps the non-zero values and their positions, and assumes zero everywhere else. \n",
    "\n",
    "So now we have our sparse matrix X, and our target values y, it's time to create our classifier (clf) using Scikit's logistic regression implementation. It's time to test this model on the our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict (clf, v, test_features):\n",
    "    X2 = v.transform(test_features)\n",
    "    return clf.predict(X2)\n",
    "\n",
    "test_features = get_corpus_features(test_sents)\n",
    "y_predict = predict (clf, v, test_features)\n",
    "y_true = get_y (test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is fairly straight forward, but it's interesting to note the *transform* method used. We create a sparse feature matrix for the test set, but it will not be in the same size as the original matrix for the train data. However, the *predict* method demands that the train and test matrixes will have the same number of features. \n",
    "The transform method transforms the test_features matrix to the same size as the train_features - by adding empty columns for features encountered in the training data, but not the test data. However, this also means there's some information loss - as features that were encountered in the test data but not the train data are \"silently ignored\".\n",
    "\n",
    "So now we have our predictions, it's time to evaluate how well we did. We can do this by several different methods. The easiest of which will be to calculate the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy (x,y):\n",
    "    correct = sum([1 if x[i]==y[i] else 0 for i in range(len(x))])\n",
    "    return correct / len(x)\n",
    "\n",
    "accuracy(y_predict, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So almost 93%, not too bad but we will try to do better. We also have more tools to analyze our errors. For a start, we can simply print all the errors and manually look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_errors (x,y,test_sents):\n",
    "    features = get_corpus_features(test_sents)\n",
    "    errors = []\n",
    "    for i in range(len(x)):\n",
    "        if x[i] != y[i]:\n",
    "            errors.append((y[i], x[i], features[i].get(\"form\")))\n",
    "    return sorted(errors)\n",
    "        \n",
    "errors = get_errors(y_predict, y_true, test_sents)\n",
    "for i in range(20):\n",
    "    print('correct = %-8s guess = %-8s word = %-30s' % (errors[i][0], errors[i][1], errors[i][2]))\n",
    "    \n",
    "#we only print the first 20 errors as examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the confusion matrix:\n",
    "<br>(We'll also print the NER tags in order, because the Scikit confusion matrix does not include labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(set(y_true))\n",
    "print(metrics.confusion_matrix(y_true, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit also offers an handy tool called a classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_true, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we are doing a very good job with recognizing O's, but we struggle with other tags. In general, B tags seem to have better score than I tags, while MISC category seems to be the most difficult to classify. It also means that our 93% accuracy score is misleading - the score is relatively high because we are good in recognizing O's (which is the majority of the dataset), but when it comes to the classification of other tags, our accuracy is much lower.\n",
    "\n",
    "Let's check our model on the Dutch dataset as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_features = get_corpus_features(d_train_sents)\n",
    "d_clf = train(d_train_sents, v, d_features)\n",
    "d_test_features = get_corpus_features(d_test_sents)\n",
    "d_y_predict = predict (d_clf, v, d_test_features)\n",
    "d_y_true = get_y (d_test_sents)\n",
    "print (\"accuracy:\", accuracy (d_y_predict,d_y_true))\n",
    "print (metrics.classification_report(d_y_true, d_y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems our Dutch model is doing slightly better than the Spanish one, but it's still prone to similar mistakes - once again we recognize very high success rates among O's (which are the big majority of the dataset), I tags are more difficult to predict than B tags, and \"Misc\" category is still problematic. \n",
    "One noticeable difference is that the Dutch model seems to struggle with Location names (0.29 fscore on I-LOC - even lower than I-MISC), which the Spanish model did better on. A possible explanation is insufficient training data - the Dutch dataset only had 64 I-LOC tags, which is much lower than other tags. \n",
    "The support column refers to the test dataset and not the training dataset, but a check on the training dataset confirms that I-LOC tags are indeed relatively rare.\n",
    "\n",
    "So now, we will try to imrove our overall model by using more than just word-specific features - we'll be looking at features of the previous and following word. Here's our updated *get_word_features2* method, which now receives three arguments (the previous and next word, in addition to the current one) and extracts all features for all 3 words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_features2 (word, prev, next):\n",
    "#also includes information about next and previous word\n",
    "    w = word[0]\n",
    "    p = prev[0]\n",
    "    n = next[0]\n",
    "    features = {\n",
    "     \"form\": w,\n",
    "     \"pos\": word[1],\n",
    "     \"is_number\": w.isdigit(),\n",
    "     \"contains_number\": hasNumbers(w),\n",
    "     \"beginCapital\": w[0].isupper(),\n",
    "     \"allCaps\": w.isupper(),\n",
    "     \"isPunc\": w in string.punctuation,\n",
    "     \"firstLetter\": w[0],\n",
    "     \"first2Letters\": w[0:2],\n",
    "     \"first3Letters\": w[0:3],\n",
    "     \"lastLetter\": w[-1],\n",
    "     \"last2Letters\": w[-2:],\n",
    "     \"last3Letters\": w[-3:],\n",
    "     \"p_form\": p,\n",
    "     \"p_pos\": prev[1],\n",
    "     \"p_is_number\": p.isdigit(),\n",
    "     \"p_contains_number\": hasNumbers(p),\n",
    "     \"p_beginCapital\": p[0].isupper(),\n",
    "     \"p_allCaps\": p.isupper(),\n",
    "     \"p_isPunc\": p in string.punctuation,\n",
    "     \"p_firstLetter\": p[0],\n",
    "     \"p_first2Letters\": p[0:2],\n",
    "     \"p_first3Letters\": p[0:3],\n",
    "     \"p_lastLetter\": p[-1],\n",
    "     \"p_last2Letters\": p[-2:],\n",
    "     \"p_last3Letters\": p[-3:],\n",
    "     \"n_form\": n,\n",
    "     \"n_pos\": next[1],\n",
    "     \"n_is_number\": n.isdigit(),\n",
    "     \"n_contains_number\": hasNumbers(n),\n",
    "     \"n_beginCapital\": n[0].isupper(),\n",
    "     \"n_allCaps\": n.isupper(),\n",
    "     \"n_ispunc\": n in string.punctuation,\n",
    "     \"n_firstLetter\": n[0],\n",
    "     \"n_first2Letters\": n[0:2],\n",
    "     \"n_first3Letters\": n[0:3],\n",
    "     \"n_lastLetter\": n[-1],\n",
    "     \"n_last2Letters\": n[-2:],\n",
    "     \"n_last3Letters\": n[-3:]\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def get_corpus_features2 (corpus):\n",
    "#gets a corpus, returns a list of features for every word\n",
    "    flat = [w for sent in corpus for w in sent]\n",
    "    pad = [(\"*\",\"*\",\"*\")]\n",
    "    flat = pad + flat + pad\n",
    "    X = []\n",
    "    for i in range(1, len(flat) - 1):\n",
    "        X.append(get_word_features2(flat[i], flat[i - 1], flat[i + 1]))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now let's train on the new data, and try to predict and check our new accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features2 = get_corpus_features2(train_sents)\n",
    "clf2 = train(train_sents, v, features2)\n",
    "test_features2 = get_corpus_features2(test_sents)\n",
    "y_predict2 = predict (clf2, v, test_features2)\n",
    "y_true2 = get_y (test_sents)\n",
    "print(\"accuracy:\", accuracy (y_predict2, y_true2))\n",
    "print(metrics.classification_report(y_true2, y_predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the new score is around 95% - an improvement on the previous attempt. Let's check on the Dutch dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_features2 = get_corpus_features2(d_train_sents)\n",
    "d_clf2 = train(d_train_sents, v, d_features2)\n",
    "d_test_features2 = get_corpus_features2(d_test_sents)\n",
    "d_y_predict2 = predict (d_clf2, v, d_test_features2)\n",
    "d_y_true2 = get_y (d_test_sents)\n",
    "print(\"accuracy:\", accuracy (d_y_predict2, d_y_true2))\n",
    "print(metrics.classification_report(d_y_true2, d_y_predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a similar improvement in the Dutch model as well - who is now up to almost 97% accuracy score. We see some improvement in all categories, including the problematic ones. I-LOC for example is up from 0.29 to 0.41, which is an improvement but still not a good score by any means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.1.3 - Finding Illegal Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the tagging method we used was greedy tagging. We did not check the logic of the tagging, and in particular we didn't check if tag sequences were legal or not. Let's write a function that will find all illegal tag sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_illegal_sequences (guess):\n",
    "    OIX, IXIY, BXIY = 0, 0, 0\n",
    "    for i in range(len(guess) - 1):\n",
    "        curr, next = guess[i], guess[i + 1]\n",
    "        if curr[0] == \"O\" and next[0] == \"I\":\n",
    "            OIX += 1\n",
    "        elif curr[0] == \"I\" and next[0] == \"I\" and curr[1:] != next[1:]:\n",
    "            IXIY+=1\n",
    "        elif curr[0] == \"B\" and next[0] ==\"I\" and curr[1:] != next[1:]:\n",
    "            BXIY += 1\n",
    "    return {\"O-IX\": OIX, \"IX-IY\": IXIY, \"BX-IY\": BXIY}\n",
    "\n",
    "print(\"Spanish:\", find_illegal_sequences(y_predict2))\n",
    "print(\"Dutch:\", find_illegal_sequences(d_y_predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Spanish dataset, all three illegal sequences seem to happen in similar frequency. Our test set includes 52923 words (and hence 52922 sequences of 2), and overall we guessed 646 illegal sequences - that's about 1.2% of our guesses. \n",
    "In the Dutch dataset, illegal sequences are rarer, and only take about 0.5% of our guesses (which is consistent with our results so far, in which our model did better in Dutch than Spanish).\n",
    "\n",
    "How can we use this information?\n",
    "<br>If we were to change our predicting model to prevent it from predicting illegal sequences, we could theoretically improve our accuracy by **up to** 1.2%. Of course, this will be a difficult process. Even if we know that a sequence of 2 tags is illegal, we still face two problems:\n",
    "\n",
    "- We need to determine which of the two tags is wrong (or maybe both are wrong)\n",
    "- We need to correct it to the right answer.\n",
    "\n",
    "If we were to implement such an algorithm, a possible method of doing so would be to use the *predcit_proba* method, which for every word, returns the probability of each tag (a distribution). Using this method, a possible rough algorithm would be:\n",
    "\n",
    "- Predict normally\n",
    "\n",
    "- Look for illegal sequences - similar to *find_illegal_sequences* above, but rather than just counting, for every illegal sequence we find we will perform the following steps:\n",
    "\n",
    "- Determine which of the two tags is more likely to be wrong. We can do it by getting max probability in each word's distribution, and choosing the lower value of the two. \n",
    "For example: let w1,w2 be two words, and t1,t2 the tags with the highest probability. If p(t1)>p(t2), we will conclude that ws is more likely to be wrong.\n",
    "\n",
    "- Look for the second highest probability in w2's distribution - let's call it t2'. \n",
    "\n",
    "- Check if the sequence (t1, t2') is a legal sequence. If it is, change w2 tag to t2' and move to the next sequence.\n",
    "\n",
    "- If it's illegal, we'll try the next most likely tag - it might be the next option on w2's distribution, or the second option in w1's distribution. \n",
    "\n",
    "- Repeat until you get a legal sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we would like to further improve our results, by using word embeddings. Word embeddings are the results of mapping from words to numerical vectors, based on words context. \n",
    "The idea behind them is simple: words that often occur together in similar contexts, are likely to have similiar meanings. We can assume that words like 'cat' and 'dog' will frequently appear in similar context, while 'cat' and 'tractor' are less likely to. So we go through a lot of text, and for every word in our vocabulary, we collect data of all the words it appears frequently with. After collecting our data, we can perform various mathematical manipulations to turn this data into a vector for every word, in such a way that words that frequently appear together will have similar vectors. We will use those vector to try and improve our NER tagging.\n",
    "\n",
    "So first, let's import our word embeddings database (in Spanish):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordvectors_file_vec = 'data\\wiki.es.vec'\n",
    "wordvecs = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a lot of cool things with this, such as finding the most similiar words to a given word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordvecs.most_similar_cosmul(positive=['rey','mujer'],negative=['hombre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the one feature that interests us is the wordvecs feature, which returns a numerical vector of size 300 that represents a word. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordvecs.word_vec(\"hombre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it's time to extract our features. For each word we will have 900 features - it's vector, the vector of the previous and the next word. Note: this code returns only the word embeddings, and not the features we've used in the previous questions.\n",
    "\n",
    "Our first naive implementation failed, because we ran into a new problem: some of the words in our training sents were simply missing in the word embeddings dataset, which has caused it to throw an exception. We have to decide, what do we do in that case? \n",
    "Our first idea was to give it a set value, such as a vector of zeros. But we've decided against it, because we feared to create a wrong similarity between all the missing words (they would all have the exact same word embedding vector, so might be interpeted as the same word). This is why we've decided instead to generate random values for each missing word. It's not a perfect solution, and has some fairly noticeable downsides:\n",
    "\n",
    "1) Accidental similarity might occur (a randomly generated vector might be similar to an unrelated word)\n",
    "<br>2) Several occurences of the same word will be assigned different vectors.\n",
    "\n",
    "But we still think it's preferable to giving the exact same vector to all missing words.\n",
    "So here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_emb_features (word, prev, next, wordvecs):\n",
    "    w = word[0]\n",
    "    p = prev[0]\n",
    "    n = next[0]\n",
    "    try: \n",
    "        wordvec = wordvecs.word_vec(w)\n",
    "    except KeyError:\n",
    "        wordvec = np.random.rand(300) * 2 - 1\n",
    "    try: \n",
    "        p_wordvec = wordvecs.word_vec(p)\n",
    "    except KeyError:\n",
    "        p_wordvec = np.random.rand(300) * 2 - 1\n",
    "    try: \n",
    "        n_wordvec = wordvecs.word_vec(n)\n",
    "    except KeyError:\n",
    "        n_wordvec = np.random.rand(300) * 2 - 1\n",
    "    return np.concatenate((wordvec, p_wordvec, n_wordvec))\n",
    "\n",
    "def emb_corpus_features (corpus, wordvec):\n",
    "#gets a corpus, returns a list of features for every word\n",
    "    flat = [w for sent in corpus for w in sent]\n",
    "    pad = [(\"*\", \"*\", \"*\")]\n",
    "    flat = pad + flat + pad\n",
    "    X = []\n",
    "    for i in range(1, len(flat) - 1):\n",
    "        X.append(word_emb_features(flat[i], flat[i - 1], flat[i + 1], wordvec))\n",
    "    return X\n",
    "\n",
    "emb_features = emb_corpus_features(train_sents, wordvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have our features, and the next step in our workflow is running them into the DictVectorizer. But here we ran into a technical problem: <br>the DictVectorizer (as his name implies...) is expecting to recieve a dictionary, while our word embeddings are in the form of a list of vectors.\n",
    "\n",
    "Of course, converting a vector into a dictionary is logically very easy. But when we tried it, we've ran into serious performance issues. Our dataset is huge - 264715 words with 900 features for each - and converting this into a dictionary simply takes a lot of time. Also logically, it's pretty silly to convert our matrix into a dictionary, only for the DictVectorizer to convert it back into a matrix.\n",
    "\n",
    "So instead we asked ourselves - why did we need the DictVectorizer in the first place?\n",
    "\n",
    "Well, the DictVectorizer has solved 3 problems for us:\n",
    "\n",
    "1) Convert our original features, who were in the form of a dictionary, into a matrix\n",
    "<br>2) Convert string features into numerical one-hot encodings\n",
    "<br>3) Created a sparse matrix for better performance and taking less memory\n",
    "\n",
    "Interestingly enough, all 3 problems aren't relevant anymore!\n",
    "\n",
    "1) Our data is already in the form of vectors list, with all vectors in the same size - which is pretty much identical to a matrix\n",
    "<br>2) Word embeddings are numerical features, so such conversion isn't necessary\n",
    "<br>3) As we recall, sparse matrices were only effective because most of our data was made of zeros. However, unlike one-hot vectors who are mostly zeros by definition, word embeddings are made of many values. Zero is one of those possible values, but it does not carry any special meaning and does not appear more frequently than others.\n",
    "\n",
    "Common confusion about point 3: earlier we said we needed sparse matrix for performance reasons - because my computer simply couldn't handle the non-sparse matrix. In a quick look, our situation is now much worse - instead of using a 13 (or 39) features, we now have 900 of them! But remember point 2 - because word embeddings are numerical features, conversion into one-hot encoding is unneccasary. So our new matrix is actually much **smaller** - it's size is 264715 X 900, as opposed to the one in the previous question - which size was a fairly insane 264715 X 115042!\n",
    "\n",
    "So basically, our DictVectorizer is now redundant! We'll just skip it and send our data straight to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_y = get_y (train_sents)\n",
    "emb_clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(emb_features, emb_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now it's time to evaluate our new exciting method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_test_features = emb_corpus_features(test_sents, wordvec)\n",
    "emb_y_predict = emb_clf.predict(emb_test_features)\n",
    "emb_y_true = get_y (test_sents)\n",
    "print (\"accuracy:\", accuracy (emb_y_predict, emb_y_true))\n",
    "print (metrics.classification_report(emb_y_true, emb_y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is, well, fairly disappointing. How can it be that we've used much more features, who are more meaningful, and still got much worse results? Something has to be wrong here...\n",
    "Well, upon serious examination we've found the problem. Apparently we were a bit too hasty with our treatment of missing words in the WE dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_missing_percentage (corpus, wordvec):\n",
    "    count = 0\n",
    "    flat = [w for sent in corpus for w in sent]\n",
    "    for w in flat:\n",
    "        w2 = w[0]\n",
    "        try: \n",
    "            wordvec.word_vec(w2)\n",
    "        except KeyError:\n",
    "            count += 1\n",
    "    return(count / len(flat)) * 100\n",
    "\n",
    "get_missing_percentage(train_sents, wordvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that a whooping 17% of our dataset is missing word embeddings! Which means that 17% of our predictions were based on random, meaningless numbers. Given that information, a 86% accuracy is actually fairly impressive.\n",
    "\n",
    "Of course, the actual situation is a bit more complicated. Because we're also looking at previous or next words, it's possible that for an unknown word, we'll still have correct information on previous and next words, hopefully allowing us to guess by context. This is probably how we could achieve 86% accuracy despite only having word embeddings for 83% of our dataset. \n",
    "But the point remains - a large bit of our training data is just wrong!\n",
    "\n",
    "So we will try to simply skip the missing words. This will make our training matrix smaller - but since the \"data\" we're skipping is actually just random numbers, it's not a great loss. \n",
    "We do have one significant loss though: because we're skipping some words, we now can't calculate information about previous and next words. What if it's missing? We'll have to either generate fake data (which is what we're trying to avoid in the first place), or find the closest non-missing word (which might be several words away, and might be irrelevant for the context of our word).\n",
    "\n",
    "So we'll only use word embeddings for the current word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emb_corpus_features_skip (corpus, wordvec):\n",
    "#gets a corpus, returns a list of features for every word\n",
    "    flat = [w for sent in corpus for w in sent]\n",
    "    X = []\n",
    "    for i in range(1, len(flat) - 1):\n",
    "        w = flat[i][0]\n",
    "        try: \n",
    "            v = wordvec.word_vec(w)\n",
    "            X.append([v, flat[i][2]])\n",
    "        except KeyError:\n",
    "            pass    \n",
    "    return X\n",
    "\n",
    "emb_skip = emb_corpus_features_skip(train_sents, wordvec)\n",
    "emb_features_skip = [x[0] for x in emb_skip]\n",
    "emb_y_skip = [x[1] for x in emb_skip]\n",
    "emb_skip_clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(emb_features_skip, emb_y_skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our coding approach is a bit different now - we  have to \"remember\" which words we skipped and also skip the relevant y value. So instead of getting features and y values separately, we're now generating them together, and returing a list of (X, y) pairs (with X being a vector of size 300).\n",
    "\n",
    "Let's test this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_test_skip = emb_corpus_features_skip (test_sents, wordvec)\n",
    "emb_test_features_skip = [x[0] for x in emb_test_skip]\n",
    "emb_true_y_skip = [x[1] for x in emb_test_skip]\n",
    "emb_y_predict_skip = emb_skip_clf.predict(emb_test_features_skip)\n",
    "\n",
    "print (\"accuracy:\", accuracy (emb_y_predict_skip, emb_true_y_skip))\n",
    "print (metrics.classification_report(emb_true_y_skip, emb_y_predict_skip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at first glance, our accuracy is almost 98% now - which is the best we've had so far - but our classification matrix reveal a much more worrying truth. It seems that we we skipped most of the non-O words, which results in a mostly empty classification report. A quick check confirms this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def counter_percentage (c, len):\n",
    "    return [(i, c[i] / len * 100.0) for i in c]\n",
    "\n",
    "c_skip = Counter(emb_true_y_skip)\n",
    "counter_percentage(c_skip, len(emb_true_y_skip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = Counter(emb_y_true)\n",
    "counter_percentage(c, len(emb_y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So almost 98% of our data is now O's, while before skipping, the O percentage was about 86%. Logically, this makes sense - O represents \"normal\" words in the language, while the rest of the values refer to names (of people, locations, organizations, etc). Names are by nature a lot less common and more specific, so it's hardly surprising that names like \"Melbourne\" are more likely to be missing in the WE dataset, than normal Spanish words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
