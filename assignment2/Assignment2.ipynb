{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the solution to HW2, written by Yaniv Bin and Tair Hakman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first would like to import all the required modules in order for our code to run properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath, dirname, join\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import gzip, string, nltk\n",
    "from nltk.corpus import conll2002\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go ahead and solve the assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first task is identifying complex words. In order to do so we will use the data provided. \n",
    "first we need to load the data using the provided functions. \n",
    "<br>(For a better flow we modified the skeleton code within the notebook, but we added the edit into the skeleton file as well) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_file(data_file):\n",
    "    words = []\n",
    "    labels = []   \n",
    "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i > 0:\n",
    "                line_split = line[:-1].split(\"\\t\")\n",
    "                words.append(line_split[0].lower())\n",
    "                labels.append(int(line_split[1]))\n",
    "            i += 1\n",
    "    return words, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = abspath(join(dirname(\"__file__\"), \"data/complex_words_training.txt\"))\n",
    "development_file = abspath(join(dirname(\"__file__\"), \"data/complex_words_development.txt\"))\n",
    "test_file = abspath(join(dirname(\"__file__\"), \"data/complex_words_test_unlabeled.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data we may start working on the actual assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 - Evaluation Matrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually in order to evaluate how well a classifcation algorithm works we use three mesures - \n",
    "- Precision: Mesure how 'useful' the results are (how many are hits and how many are miss)\n",
    "- Recall: Mesure how 'complete' the results are (out off all the actually possible results, how many did we hit)\n",
    "- Fscore: Mesure the balance between the Precision and Recall\n",
    "\n",
    "We would like to implement functions that compose each of these matrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by calculating the precision - the precision is defined as:\n",
    "<br>$precision = \\frac{tp}{tp + fp}$\n",
    "<br>Where $tp$ stands for true-positive meaning a hit, and $fp$ stands for false-positive meaning the prediction is positive but it's actually a false alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input: y_pred, a list of length n with the predicted labels,\n",
    "## y_true, a list of length n with the true labels\n",
    "\n",
    "## Calculates the precision of the predicted labels\n",
    "def get_precision(y_pred, y_true):\n",
    "    positive_hits_array = np.array([(1 if(y_pred[i] == y_true[i] == 1) else 0) for i in range(len(y_pred))])\n",
    "    positive_hits = np.count_nonzero(positive_hits_array == 1)\n",
    "    total_positive = np.count_nonzero(np.array(y_pred) == 1)\n",
    "    precision = float(positive_hits) / float(total_positive)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [0, 0, 1, 1]\n",
    "get_precision(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second mesure we will implement is the recall, calculated as follows\n",
    "<br>$recall = \\frac{tp}{tp + fn}$\n",
    "<br>Where $fn$ stands for false-negative, meaning the prediction is negative also it should have been positive(a miss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Calculates the recall of the predicted labels\n",
    "def get_recall(y_pred, y_true):\n",
    "    positive_hits_array = np.array([(1 if(y_pred[i] == y_true[i] == 1) else 0) for i in range(len(y_pred))])\n",
    "    positive_hits = np.count_nonzero(positive_hits_array == 1)\n",
    "    total_positive = np.count_nonzero(np.array(y_true) == 1)\n",
    "    recall = float(positive_hits) / float(total_positive)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [1, 0, 1, 1]\n",
    "get_recall(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the last measure we want to implement is the fscore(also known as f1) which is calculated as:\n",
    "<br>$F = 2 * \\frac{precision*recall}{precision + recall}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Calculates the f-score of the predicted labels\n",
    "def get_fscore(y_pred, y_true):\n",
    "    precision = get_precision(y_pred, y_true)\n",
    "    recall = get_recall(y_pred, y_true)\n",
    "    fscore = 2 * float(precision * recall) / float(precision + recall)\n",
    "    return fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [1, 0, 1, 1]\n",
    "get_fscore(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our own good we would also like to implement a function that prints out all the information given the two arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_predictions(y_pred, y_true):\n",
    "    print(\"Precision:\", get_precision(y_pred, y_true))\n",
    "    print(\"Recall:\", get_recall(y_pred, y_true))\n",
    "    print(\"Fscore:\", get_fscore(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5\n",
      "Recall: 0.3333333333333333\n",
      "Fscore: 0.4\n"
     ]
    }
   ],
   "source": [
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [1, 0, 1, 1]\n",
    "test_predictions(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after implementing the functions, we can go on and implement actual classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 - Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2.1 - All complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classifier we will implement is a very simple one that classifies all the words as complex no matter what they actually are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Labels every word complex\n",
    "def all_complex(data_file):\n",
    "    words, actual_labels = load_file(data_file)\n",
    "    all_complex_labels = np.ones((len(words),), dtype=int)\n",
    "    precision = get_precision(all_complex_labels, actual_labels)\n",
    "    recall = get_recall(all_complex_labels, actual_labels)\n",
    "    fscore = get_fscore(all_complex_labels, actual_labels)\n",
    "    performance = [precision, recall, fscore]\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we shall test it with each of the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Precision: 0.43275 \n",
      "Training Recall: 1.0 \n",
      "Training Fscore: 0.604083057058105\n"
     ]
    }
   ],
   "source": [
    "tr_precision, tr_recall, tr_fscore = all_complex(training_file)\n",
    "print(\"Training Precision: {} \\nTraining Recall: {} \\nTraining Fscore: {}\".format(tr_precision, tr_recall, tr_fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Precision: 0.418 \n",
      "Dev Recall: 1.0 \n",
      "Dev Fscore: 0.5895627644569816\n"
     ]
    }
   ],
   "source": [
    "dv_precision, dv_recall, dv_fscore = all_complex(development_file)\n",
    "print(\"Dev Precision: {} \\nDev Recall: {} \\nDev Fscore: {}\".format(dv_precision, dv_recall, dv_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can note how the recall of this model is always 1 - that makes sense because although it probably has a lot of false positives - it never \"miss\" in terms of false negtive because it's always positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2.2 - word length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second baseline we will implement is a word length based one, which gives a positive value to a word if it's length goes past a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.2.2: Word length thresholding\n",
    "def word_length_baseline(data_file, threshold):\n",
    "    words, actual_labels = load_file(data_file)\n",
    "    threshold_labels = [(1 if(len(word) >= threshold) else 0) for word in words]\n",
    "    \n",
    "    precision = get_precision(threshold_labels, actual_labels)\n",
    "    recall = get_recall(threshold_labels, actual_labels)\n",
    "    fscore = get_fscore(threshold_labels, actual_labels)\n",
    "    preformance = [precision, recall, fscore]\n",
    "    return preformance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to run this for both the training dataset and the development dataset for different threshold values, and plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold - 0 the fscore is 0.604083057058105\n",
      "For threshold - 1 the fscore is 0.604083057058105\n",
      "For threshold - 2 the fscore is 0.604083057058105\n",
      "For threshold - 3 the fscore is 0.604083057058105\n",
      "For threshold - 4 the fscore is 0.6142322097378278\n",
      "For threshold - 5 the fscore is 0.6442307692307692\n",
      "For threshold - 6 the fscore is 0.680399235506477\n",
      "For threshold - 7 the fscore is 0.7018976699495555\n",
      "For threshold - 8 the fscore is 0.6919592298980747\n",
      "For threshold - 9 the fscore is 0.6276346604215458\n",
      "For threshold - 10 the fscore is 0.4877663772691396\n",
      "For threshold - 11 the fscore is 0.33770937075599816\n",
      "For threshold - 12 the fscore is 0.21656686626746505\n",
      "For threshold - 13 the fscore is 0.12539851222104145\n",
      "For threshold - 14 the fscore is 0.07362637362637363\n",
      "[1.         1.         1.         1.         0.99480069 0.96764876\n",
      " 0.9254766  0.8440208  0.70595032 0.5418833  0.35701906 0.21548238\n",
      " 0.12536106 0.06816869 0.03870595]\n",
      "[0.43275    0.43275    0.43275    0.43275    0.44427245 0.48284808\n",
      " 0.53794493 0.60074013 0.67851194 0.74562798 0.76961395 0.78033473\n",
      " 0.79487179 0.78145695 0.75280899]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nISIitVJYiIhIrf4/sVQtIf+5QYwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3c0c5322e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precisions = np.zeros(15)\n",
    "recall = np.zeros(15)\n",
    "for i in range(15):\n",
    "    tprecision, trecall, tfscore = word_length_baseline(training_file, i)\n",
    "    precisions[i] = tprecision\n",
    "    recall[i] = trecall\n",
    "    print(\"For threshold - {} the fscore is {}\".format(i, tfscore))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(recall, precisions)\n",
    "plt.xlabel(\"recall\")\n",
    "plt.ylabel(\"precision\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold - 0 the fscore is 0.5895627644569816\n",
      "For threshold - 1 the fscore is 0.5895627644569816\n",
      "For threshold - 2 the fscore is 0.5895627644569816\n",
      "For threshold - 3 the fscore is 0.5895627644569816\n",
      "For threshold - 4 the fscore is 0.5971428571428571\n",
      "For threshold - 5 the fscore is 0.6313364055299538\n",
      "For threshold - 6 the fscore is 0.6791489361702128\n",
      "For threshold - 7 the fscore is 0.7125984251968505\n",
      "For threshold - 8 the fscore is 0.7065592635212887\n",
      "For threshold - 9 the fscore is 0.624484181568088\n",
      "For threshold - 10 the fscore is 0.4694214876033058\n",
      "For threshold - 11 the fscore is 0.3500931098696462\n",
      "For threshold - 12 the fscore is 0.2139917695473251\n",
      "For threshold - 13 the fscore is 0.11013215859030837\n",
      "For threshold - 14 the fscore is 0.05949656750572082\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRESkUQoLERFplMJCREQa9f8BXS7HV7/Vy+gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3c0c48e630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precisions = np.zeros(15)\n",
    "recall = np.zeros(15)\n",
    "for i in range(15):\n",
    "    dprecision, drecall, dfscore = word_length_baseline(development_file, i)\n",
    "    precisions[i] = dprecision\n",
    "    recall[i] = drecall\n",
    "    print(\"For threshold - {} the fscore is {}\".format(i, dfscore))\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(recall, precisions)\n",
    "plt.xlabel(\"recall\")\n",
    "plt.ylabel(\"precision\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that once the recall is big the precision is small and vice versa, that's because the two complement eachother. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go about building the \"ideal\" threshold classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Finds the best length threshold by f-score, and uses this threshold to\n",
    "## classify the training and development set\n",
    "def word_length_threshold(training_file, development_file):\n",
    "    best_tfscore = 0.0\n",
    "    best_i = 1\n",
    "    i = 1\n",
    "    while(True):\n",
    "        tprecision, trecall, tfscore = word_length_baseline(training_file, i)\n",
    "        if(tfscore < best_tfscore):\n",
    "            break\n",
    "        else:\n",
    "            best_i = i\n",
    "            i += 1\n",
    "            best_tfscore = tfscore\n",
    "            \n",
    "    tprecision, trecall, tfscore = word_length_baseline(training_file, best_i)\n",
    "    dprecision, drecall, dfscore = word_length_baseline(development_file, best_i)\n",
    "    training_performance = [tprecision, trecall, tfscore]\n",
    "    development_performance = [dprecision, drecall, dfscore]\n",
    "    return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Precision: 0.6007401315789473 \n",
      "Training Recall: 0.8440207972270364 \n",
      "Training Fscore: 0.7018976699495555\n",
      "Dev Precision: 0.6053511705685619 \n",
      "Dev Recall: 0.8660287081339713 \n",
      "Dev Fscore: 0.7125984251968505\n"
     ]
    }
   ],
   "source": [
    "training_performance, development_performance = word_length_threshold(training_file, development_file)\n",
    "tr_precision, tr_recall, tr_fscore = training_performance\n",
    "dv_precision, dv_recall, dv_fscore = development_performance\n",
    "print(\"Training Precision: {} \\nTraining Recall: {} \\nTraining Fscore: {}\".format(tr_precision, tr_recall, tr_fscore))\n",
    "print(\"Dev Precision: {} \\nDev Recall: {} \\nDev Fscore: {}\".format(dv_precision, dv_recall, dv_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2.3 - Word Frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will create a NER (Named Entity Recognition) model that recognizes names of persons, organizations and other entities in text. We will use the CoNLL 2002 dataset. As we build the code, we will use the Spanish version of the dataset. At the end of every segment, we'll test our functions on the Dutch dataset and compare the differences. \n",
    "First, we will import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = list(conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(conll2002.iob_sents('esp.testa'))\n",
    "d_train_sents = list(conll2002.iob_sents('ned.train'))\n",
    "d_test_sents = list(conll2002.iob_sents('ned.testa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand how our dataset is represented. Each of the two datasets is imported as a list. Each element in the list is a **sentence**. Let's look at an example of the first element (and hence, first sentence) in the first dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Melbourne', 'NP', 'B-LOC'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('Australia', 'NP', 'B-LOC'),\n",
       " (')', 'Fpt', 'O'),\n",
       " (',', 'Fc', 'O'),\n",
       " ('25', 'Z', 'O'),\n",
       " ('may', 'NC', 'O'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('EFE', 'NC', 'B-ORG'),\n",
       " (')', 'Fpt', 'O'),\n",
       " ('.', 'Fp', 'O')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the sentence represented by a list of word. A word is **also** represented by a list, that contains three elements: the word itself, it's POS (Part of Speech) tagging, and it's correct NER tagging. \n",
    "\n",
    "Now that we understand our data, it's time to extract features. We'll be looking for word level features, and in this step we'll be looking at each word separately. Those are the features we've chosen to extract for every word:\n",
    "\n",
    "Form (The actual word), POS tagging, is number, does it contain a number, does it begin with a capital letter, is it all capital letters, is it a punctuation char, the first one, two and three letters of the word, the last one, two and three letters in of the word. \n",
    "Here's the code for feature extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasNumbers(str):\n",
    "    return any(c.isdigit() for c in str)\n",
    "\n",
    "def get_word_features (word):\n",
    "    w = word[0]\n",
    "    features = {\n",
    "     \"form\": w,\n",
    "     \"pos\": word[1],\n",
    "     \"is_number\": w.isdigit(),\n",
    "     \"contains_number\": hasNumbers(w),\n",
    "     \"beginCapital\": w[0].isupper(),\n",
    "     \"allCaps\": w.isupper(),\n",
    "     \"isPunc\": w in string.punctuation,\n",
    "     \"firstLetter\": w[0],\n",
    "     \"first2Letters\": w[0:2],\n",
    "     \"first3Letters\": w[0:3],\n",
    "     \"lastLetter\": w[-1],\n",
    "     \"last2Letters\": w[-2:],\n",
    "     \"last3Letters\": w[-3:]\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's an example on the word 'Melbourne':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'allCaps': False,\n",
       " 'beginCapital': True,\n",
       " 'contains_number': False,\n",
       " 'first2Letters': 'Me',\n",
       " 'first3Letters': 'Mel',\n",
       " 'firstLetter': 'M',\n",
       " 'form': 'Melbourne',\n",
       " 'isPunc': False,\n",
       " 'is_number': False,\n",
       " 'last2Letters': 'ne',\n",
       " 'last3Letters': 'rne',\n",
       " 'lastLetter': 'e',\n",
       " 'pos': 'NP'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_features(train_sents[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have our features, it's time to train our model. We will be using Scikit's DictVectorizer data structure to keep our data, and it's logistic regression implementation for the training. Those methods requires two seperate lists of identical size, where every element represents one word in the corpus. The first list (X) keeps the list of features for every word, and the second list (y) has the NER tagging of the word - which is the answer our model will be trying to guess.\n",
    "\n",
    "Here is the code that creates those lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_features (corpus):\n",
    "#gets a corpus, returns a list of features for every word\n",
    "    X=[]\n",
    "    for sent in corpus:\n",
    "        X+=[get_word_features(w) for w in sent]\n",
    "    return X\n",
    "\n",
    "def get_y (corpus):\n",
    "    y=[]\n",
    "    for sent in corpus:\n",
    "        y+=[w[2] for w in sent]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in addition to the obvious effect of getting features/NER taggings, those functions also transform our data from a list of **sentences** to a list of **words** - so our output is a big list of all the words, not divided to sentences anymore.\n",
    "\n",
    "Now we'll create our DictVectorizer and train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_sents, v, features):\n",
    "    y = get_y (train_sents)\n",
    "    X = v.fit_transform(features)\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X, y)\n",
    "    return clf\n",
    "\n",
    "v = DictVectorizer(sparse=True)\n",
    "features = get_corpus_features(train_sents)\n",
    "clf = train(train_sents, v, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **DictVecotizer** structure and it's fit_transform method, transforms the features list to a matrix. Each row in the matrix represents a word, and each column represents a feature. Each cell M(i,j) includes the corresponding numerical value for feature j in word i.\n",
    "But what if the features are non-numerical? Boolean data easily transforms to 1 (True) or 0 (False), but strings are a bit more complicated to encode. DictVectorizer's solution is to create a separate boolean feature for every string it encounters. For example, for the word \"Melbourne\", rather than the \"firstLetter\" feature, it will create a new boolean feature: \"firstLetter=M\", that will be True for every word that begins in M, and False otherwise.\n",
    "\n",
    "This is a good solution, but it means that each word, prefix or suffix of length 1,2,3, and other string features - a specific feature will be created. In othe words, our matrix is getting really big. For the first sentence alone (11 words), 75 features are created. For the entire dataset (264715 words), the matrix will be huge - in fact, too huge for the computer's memory (or at least, *my* computer's memory) to handle.\n",
    "\n",
    "That's why we use a **sparse** version of the matrix. The sparse data structure takes advantage of the fact that most of the matrix (typically above 99%) is zeros - the word \"Melbourne\" for example will have 0 in every prefix feature other than \"firstLetter=M\", \"first2Letters=Me\" and \"first3Letters=Mel\" (same applies for suffix). So the sparse data structre, rather than actually save all those zeros, only keeps the non-zero values and cells, and assumes zero everywhere else.\n",
    "\n",
    "So now we have our sparse matrix X, and our target values y, it's time to create our classifier (clf) using Scikit's logistic regression implementation. It's time to test this model on the our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict (clf, v, test_features):\n",
    "    X2 = v.transform(test_features)\n",
    "    return clf.predict(X2)\n",
    "\n",
    "test_features = get_corpus_features(test_sents)\n",
    "y_predict = predict (clf, v, test_features)\n",
    "y_true = get_y (test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is fairly straight forward, but it's interesting to note the *transform* method used. We create a sparse feature matrix for the test set, but it will not be in the same size as the original matrix for the train data. However, the *predict* method demands that the train and test matrixes will have the same number of features. \n",
    "The transform method transforms the test_features matrix to the same size as the train_features - by adding empty columns for features encountered in the training data, but not the test data. However, this also means there's some information loss - as features that were encountered in the test data but not the train data are \"silently ignored\".\n",
    "\n",
    "So now we have our predictions, it's time to evaluate how well we did. We can do this by several different methods. The easiest of which will be to calculate the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9295391417720084"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy (x,y):\n",
    "    correct = sum([1 if x[i]==y[i] else 0 for i in range(len(x))])\n",
    "    return correct / len(x)\n",
    "\n",
    "accuracy(y_predict, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So almost 93%, not too bad but we will try to do better. We also have more tools to analyze our errors. For a start, we can simply print all the errors and manually look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct=B-LOC    guess=B-MISC   word=K-2                           \n",
      "correct=B-LOC    guess=B-MISC   word=K-2                           \n",
      "correct=B-LOC    guess=B-MISC   word=K-2                           \n",
      "correct=B-LOC    guess=B-MISC   word=K-2                           \n",
      "correct=B-LOC    guess=B-MISC   word=MADRID                        \n",
      "correct=B-LOC    guess=B-MISC   word=Regin                        \n",
      "correct=B-LOC    guess=B-MISC   word=SEVILLA                       \n",
      "correct=B-LOC    guess=B-MISC   word=SEVILLA                       \n",
      "correct=B-LOC    guess=B-MISC   word=Tbet                         \n",
      "correct=B-LOC    guess=B-MISC   word=Yacuiba-Ro                   \n",
      "correct=B-LOC    guess=B-ORG    word=ACEUCHAL                      \n",
      "correct=B-LOC    guess=B-ORG    word=Aceuchal                      \n",
      "correct=B-LOC    guess=B-ORG    word=Auditorio                     \n",
      "correct=B-LOC    guess=B-ORG    word=Austria                       \n",
      "correct=B-LOC    guess=B-ORG    word=Autoridad                     \n",
      "correct=B-LOC    guess=B-ORG    word=Autoridad                     \n",
      "correct=B-LOC    guess=B-ORG    word=BILBAO                        \n",
      "correct=B-LOC    guess=B-ORG    word=Bolivia-Brasil                \n",
      "correct=B-LOC    guess=B-ORG    word=CHILE                         \n",
      "correct=B-LOC    guess=B-ORG    word=Canad                        \n"
     ]
    }
   ],
   "source": [
    "def get_errors (x,y,test_sents):\n",
    "    features = get_corpus_features(test_sents)\n",
    "    errors=[]\n",
    "    for i in range(len(x)):\n",
    "        if x[i]!=y[i]:\n",
    "            errors.append((y[i], x[i], features[i].get(\"form\")))\n",
    "    return sorted(errors)\n",
    "        \n",
    "errors = get_errors (y_predict,y_true,test_sents)\n",
    "for i in range(20):\n",
    "    print('correct=%-8s guess=%-8s word=%-30s' % (errors[i][0],errors[i][1],errors[i][2]))\n",
    "    \n",
    "#we only print the first 20 errors as examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the confusion matrix:\n",
    "(We'll also print the NER tags in order, because the Scikit confusion matrix does not include labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-PER', 'B-ORG', 'I-LOC', 'B-PER', 'B-MISC', 'B-LOC', 'I-MISC', 'O', 'I-ORG'}\n",
      "[[  779    10    77    30     3     5    30    15    35]\n",
      " [   17   214    42     8     0    32    26    14    92]\n",
      " [  186    45  1201    67     1    12    52    53    83]\n",
      " [  107     5    49   739    20    11    25   233    33]\n",
      " [   24     5     9    30   131     5    37    18    78]\n",
      " [   51    33    34    21    12   126    55    29   293]\n",
      " [  159    53   121    41    18    68   382    78   446]\n",
      " [   83     5    36   142    12    12    25   477    67]\n",
      " [    7    35    72    17     1    35    35     9 45145]]\n"
     ]
    }
   ],
   "source": [
    "print (set(y_true))\n",
    "print (metrics.confusion_matrix(y_true, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit also offers an handy tool called a classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.55      0.79      0.65       984\n",
      "     B-MISC       0.53      0.48      0.50       445\n",
      "      B-ORG       0.73      0.71      0.72      1700\n",
      "      B-PER       0.67      0.60      0.64      1222\n",
      "      I-LOC       0.66      0.39      0.49       337\n",
      "     I-MISC       0.41      0.19      0.26       654\n",
      "      I-ORG       0.57      0.28      0.38      1366\n",
      "      I-PER       0.52      0.56      0.53       859\n",
      "          O       0.98      1.00      0.99     45356\n",
      "\n",
      "avg / total       0.92      0.93      0.92     52923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we are doing a very good job with recognizing O's, but we struggle with other tags. In general, B tags seem to have better score than I tags, while MISC category seems to be the most difficult to classify. It also means that our 93% accuracy score is misleading - the score is relatively high because we are good in recognizing O's (which is the majority of the dataset), but when it comes to the classification of other tags, our accuracy is much lower.\n",
    "\n",
    "Let's check our model on the Dutch dataset as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9508053174834824\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.65      0.60      0.62       479\n",
      "     B-MISC       0.74      0.60      0.66       748\n",
      "      B-ORG       0.72      0.45      0.55       686\n",
      "      B-PER       0.55      0.70      0.62       703\n",
      "      I-LOC       0.37      0.23      0.29        64\n",
      "     I-MISC       0.42      0.23      0.30       215\n",
      "      I-ORG       0.69      0.37      0.48       396\n",
      "      I-PER       0.38      0.45      0.41       423\n",
      "          O       0.98      1.00      0.99     33973\n",
      "\n",
      "avg / total       0.95      0.95      0.95     37687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_features = get_corpus_features(d_train_sents)\n",
    "d_clf = train(d_train_sents, v, d_features)\n",
    "d_test_features = get_corpus_features(d_test_sents)\n",
    "d_y_predict = predict (d_clf, v, d_test_features)\n",
    "d_y_true = get_y (d_test_sents)\n",
    "print (\"accuracy:\", accuracy (d_y_predict,d_y_true))\n",
    "print (metrics.classification_report(d_y_true, d_y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems our Dutch model is doing slightly better than the Spanish one, but it's still prone to similar mistakes - once again we recognize very high success rates among O's (which are the big majority of the dataset), I tags are more difficult to predict than B tags, and \"Misc\" category is still problematic. \n",
    "One noticeable difference is that the Dutch model seems to struggle with Location names (0.29 fscore on I-LOC - even lower than I-MISC), which the Spanish model did better on. A possible explanation is insufficient training data - the Dutch dataset only had 64 I-LOC tags, which is much lower than other tags. \n",
    "The support column refers to the test dataset and not the training dataset, but a check on the training dataset confirms that I-LOC tags are indeed relatively rare.\n",
    "\n",
    "So now, we will try to imrove our overall model by using more than just word-specific features - we'll be looking at features of the previous and following word. Here's our updated *get_word_features2* method, which now receives three arguments (the previous and next word, in addition to the current one) and extracts all features for all 3 words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_features2 (word, prev, next):\n",
    "#also includes information about next and previous word\n",
    "    w = word[0]\n",
    "    p = prev[0]\n",
    "    n = next[0]\n",
    "    features = {\n",
    "     \"form\": w,\n",
    "     \"pos\": word[1],\n",
    "     \"is_number\": w.isdigit(),\n",
    "     \"contains_number\": hasNumbers(w),\n",
    "     \"beginCapital\": w[0].isupper(),\n",
    "     \"allCaps\": w.isupper(),\n",
    "     \"isPunc\": w in string.punctuation,\n",
    "     \"firstLetter\": w[0],\n",
    "     \"first2Letters\": w[0:2],\n",
    "     \"first3Letters\": w[0:3],\n",
    "     \"lastLetter\": w[-1],\n",
    "     \"last2Letters\": w[-2:],\n",
    "     \"last3Letters\": w[-3:],\n",
    "     \"p_form\": p,\n",
    "     \"p_pos\": prev[1],\n",
    "     \"p_is_number\": p.isdigit(),\n",
    "     \"p_contains_number\": hasNumbers(p),\n",
    "     \"p_beginCapital\": p[0].isupper(),\n",
    "     \"p_allCaps\": p.isupper(),\n",
    "     \"p_isPunc\": p in string.punctuation,\n",
    "     \"p_firstLetter\": p[0],\n",
    "     \"p_first2Letters\": p[0:2],\n",
    "     \"p_first3Letters\": p[0:3],\n",
    "     \"p_lastLetter\": p[-1],\n",
    "     \"p_last2Letters\": p[-2:],\n",
    "     \"p_last3Letters\": p[-3:],\n",
    "     \"n_form\": n,\n",
    "     \"n_pos\": next[1],\n",
    "     \"n_is_number\": n.isdigit(),\n",
    "     \"n_contains_number\": hasNumbers(n),\n",
    "     \"n_beginCapital\": n[0].isupper(),\n",
    "     \"n_allCaps\": n.isupper(),\n",
    "     \"n_ispunc\": n in string.punctuation,\n",
    "     \"n_firstLetter\": n[0],\n",
    "     \"n_first2Letters\": n[0:2],\n",
    "     \"n_first3Letters\": n[0:3],\n",
    "     \"n_lastLetter\": n[-1],\n",
    "     \"n_last2Letters\": n[-2:],\n",
    "     \"n_last3Letters\": n[-3:]\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def get_corpus_features2 (corpus):\n",
    "#gets a corpus, returns a list of features for every word\n",
    "    flat = [w for sent in corpus for w in sent]\n",
    "    pad = [(\"*\",\"*\",\"*\")]\n",
    "    flat = pad + flat + pad\n",
    "    X=[]\n",
    "    for i in range(1, len(flat)-1):\n",
    "        X.append(get_word_features2(flat[i],flat[i-1],flat[i+1]))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now let's train on the new data, and try to predict and check our new accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9528560361279594\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.59      0.79      0.68       984\n",
      "     B-MISC       0.53      0.50      0.52       445\n",
      "      B-ORG       0.80      0.74      0.77      1700\n",
      "      B-PER       0.86      0.78      0.82      1222\n",
      "      I-LOC       0.65      0.62      0.63       337\n",
      "     I-MISC       0.57      0.41      0.48       654\n",
      "      I-ORG       0.72      0.60      0.66      1366\n",
      "      I-PER       0.85      0.88      0.86       859\n",
      "          O       0.99      1.00      0.99     45356\n",
      "\n",
      "avg / total       0.95      0.95      0.95     52923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features2 = get_corpus_features2(train_sents)\n",
    "clf2 = train(train_sents, v, features2)\n",
    "test_features2 = get_corpus_features2(test_sents)\n",
    "y_predict2 = predict (clf2, v, test_features2)\n",
    "y_true2 = get_y (test_sents)\n",
    "print (\"accuracy:\", accuracy (y_predict2,y_true2))\n",
    "print (metrics.classification_report(y_true2, y_predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the new score is around 95% - an improvement on the previous attempt. Let's check on the Dutch dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.968742537214424\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.77      0.78      0.77       479\n",
      "     B-MISC       0.79      0.71      0.75       748\n",
      "      B-ORG       0.83      0.62      0.71       686\n",
      "      B-PER       0.68      0.82      0.74       703\n",
      "      I-LOC       0.68      0.30      0.41        64\n",
      "     I-MISC       0.56      0.44      0.49       215\n",
      "      I-ORG       0.80      0.53      0.64       396\n",
      "      I-PER       0.74      0.89      0.81       423\n",
      "          O       0.99      1.00      0.99     33973\n",
      "\n",
      "avg / total       0.97      0.97      0.97     37687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_features2 = get_corpus_features2(d_train_sents)\n",
    "d_clf2 = train(d_train_sents, v, d_features2)\n",
    "d_test_features2 = get_corpus_features2(d_test_sents)\n",
    "d_y_predict2 = predict (d_clf2, v, d_test_features2)\n",
    "d_y_true2 = get_y (d_test_sents)\n",
    "print (\"accuracy:\", accuracy (d_y_predict2, d_y_true2))\n",
    "print (metrics.classification_report(d_y_true2, d_y_predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a similar improvement in the Dutch model as well - who is now up to almost 97% accuracy score. We see some improvement in all categories, including the problematic ones. I-LOC for example is up from 0.29 to 0.41, which is an improvement but still not a good score by any means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.1.3 - Finding Illegal Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the tagging method we used was greedy tagging. We did not check the logic of the tagging, and in particular we didn't check if tag sequences were legal or not. Let's write a function that will find all illegal tag sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish: {'O-IX': 210, 'IX-IY': 232, 'BX-IY': 204}\n",
      "Dutch: {'O-IX': 51, 'IX-IY': 47, 'BX-IY': 127}\n"
     ]
    }
   ],
   "source": [
    "def find_illegal_sequences (guess):\n",
    "    OIX, IXIY, BXIY = 0,0,0\n",
    "    for i in range(len(guess)-1):\n",
    "        curr, next = guess[i], guess[i+1]\n",
    "        if curr[0]==\"O\" and next[0]==\"I\":\n",
    "            OIX+=1\n",
    "        elif curr[0]==\"I\" and next[0]==\"I\" and curr[1:] != next[1:]:\n",
    "            IXIY+=1\n",
    "        elif curr[0]==\"B\" and next[0]==\"I\" and curr[1:] != next[1:]:\n",
    "            BXIY+=1\n",
    "    return {\"O-IX\": OIX, \"IX-IY\": IXIY, \"BX-IY\": BXIY}\n",
    "\n",
    "print (\"Spanish:\", find_illegal_sequences(y_predict2))\n",
    "print (\"Dutch:\", find_illegal_sequences(d_y_predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Spanish dataset, all three illegal sequences seem to happen in similar frequency. Our test set includes 52923 words (and hence 52922 sequences of 2), and overall we guessed 646 illegal sequences - that's about 1.2% of our guesses. \n",
    "In the Dutch dataset, illegal sequences are rarer, and only take about 0.5% of our guesses (which is consistent with our results so far, in which our model did better in Dutch than Spanish).\n",
    "\n",
    "How can we use this information?\n",
    "If we were to change our predicting model to prevent it from predicting illegal sequences, we could theoretically improve our accuracy by **up to** 1.2%. Of course, this will be a difficult process. Even if we know that a sequence of 2 tags is illegal, we still face two problems:\n",
    "\n",
    "-We need to determine which of the two tags is wrong (or maybe both are wrong)\n",
    "-We need to correct it to the right answer.\n",
    "\n",
    "If we were to implement such an algorithm, a possible method of doing so would be to use the *predcit_proba* method, which for every word, returns the probability of each tag (a distribution). Using this method, a possible rough algorithm would be:\n",
    "\n",
    "-Predict normally\n",
    "\n",
    "-Look for illegal sequences - similar to *find_illegal_sequences* above, but rather than just counting, for every illegal sequence we find we will perform the following steps:\n",
    "\n",
    "-Determine which of the two tags is more likely to be wrong. We can do it by getting max probability in each word's distribution, and choosing the lower value of the two. \n",
    "For example: let w1,w2 be two words, and t1,t2 the tags with the highest probability. If p(t1)>p(t2), we will conclude that ws is more likely to be wrong.\n",
    "\n",
    "-Look for the second highest probability in w2's distribution - let's call it t2'. \n",
    "\n",
    "-Check if the sequence (t1, t2') is a legal sequence. If it is, change w2 tag to t2' and move to the next sequence.\n",
    "\n",
    "-If it's illegal, we'll try the next most likely tag - it might be the next option on w2's distribution, or the second option in w1's distribution. \n",
    "\n",
    "-Repeat until you get a legal sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
